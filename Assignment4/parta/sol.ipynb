{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vivitsu329\\Documents\\environments\\nlpenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torchtext.vocab import GloVe\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import SubsetRandomSampler,DataLoader\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EMBED_DIM = 300 \n",
    "ENC_BIDIRECTIONAL = True\n",
    "ENC_BIDIRECTIONAL_FACTOR = 2 if ENC_BIDIRECTIONAL else 1\n",
    "ENC_HIDDEN_DIM = 256\n",
    "DEC_HIDDEN_DIM  = 256\n",
    "ENC_OUTPUT_DIM =  DEC_HIDDEN_DIM\n",
    "DEC_EMBED_DIM = 300\n",
    "NUM_EPOCHS = 300\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device_cpu = torch.device('cpu')\n",
    "device_fast = torch.device('cpu')\n",
    "\n",
    "if torch.has_mps:\n",
    "    device_fast = torch.device('mps')\n",
    "elif torch.has_cuda:\n",
    "    device_fast = torch.device('cuda')\n",
    "\n",
    "\n",
    "output_index_to_word = {}\n",
    "output_vocab = {}\n",
    "\n",
    "for i in range(0,10):\n",
    "    output_vocab[str(i)] = i\n",
    "    output_index_to_word[i] = str(i)\n",
    "\n",
    "output_vocab['-'] = 10\n",
    "output_index_to_word[10] = '-'\n",
    "output_vocab['<sos>'] = 11\n",
    "output_index_to_word[11] = '<sos>'\n",
    "output_vocab['<eos>'] = 12\n",
    "output_index_to_word[12] = '<eos>'\n",
    "\n",
    "glove = GloVe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_training_data(filename='./Assignment4aDataset.txt',glove=glove):\n",
    "    \n",
    "    f = open(filename,'r')\n",
    "    dataset = []\n",
    "    for line in f.readlines():       \n",
    "         \n",
    "        nl_date , out_date = line.split(',')\n",
    "        nl_date = nl_date.replace(\"\\'\",\"\").strip()\n",
    "        out_date = out_date.replace(\"\\'\",\"\").strip()\n",
    "\n",
    "        if \"/\" in nl_date:\n",
    "            #number_list = list(nl_date)\n",
    "            #nl_date = \" \".join(number_list)\n",
    "            split_on_slash = nl_date.split(\"/\")\n",
    "            nl_date = \" / \".join(split_on_slash)\n",
    "\n",
    "        nl_date = nl_date.lower()\n",
    "\n",
    "        embeddings = []\n",
    "        for word in nl_date.split(' '):\n",
    "            if word == '':\n",
    "                continue\n",
    "            embeddings.append(glove[word])\n",
    "            #embeddings.append(torch.tensor(fasttext_model.get_word_vector(word)))\n",
    "        current_inp_length = len(embeddings)\n",
    "        embeddings = torch.stack(embeddings)\n",
    "\n",
    "        target = []\n",
    "        target.append(output_vocab['<sos>'])\n",
    "\n",
    "        for character in list(out_date):\n",
    "            target.append(output_vocab[character])\n",
    "        \n",
    "        target.append(output_vocab['<eos>'])\n",
    "\n",
    "        dataset.append({'in' : embeddings,'in_length' : current_inp_length,'out' : target})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def collate_function(batch_data):\n",
    "    inputs = [b['in'] for b in batch_data]\n",
    "    in_lengths = [b['in_length'] for b in batch_data]\n",
    "    out = torch.tensor([b['out'] for b in batch_data])\n",
    "    inputs = pad_sequence(inputs,batch_first=True)\n",
    "    return {'src': inputs, 'src_length' : in_lengths, 'trg' : out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(get_training_data())\n",
    "\n",
    "train_idx,valid_idx = train_test_split(np.arange(len(train_dataset)), \n",
    "    test_size=0.2,\n",
    "    shuffle= True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_dataloader = DataLoader(train_dataset,BATCH_SIZE,sampler=train_sampler,collate_fn=collate_function)\n",
    "valid_dataloader = DataLoader(train_dataset,BATCH_SIZE,sampler=valid_sampler,collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim = EMBED_DIM,enc_hidden_dim = ENC_HIDDEN_DIM,enc_output_dim = ENC_OUTPUT_DIM,NUM_LAYERS=1,enc_bidirectional=ENC_BIDIRECTIONAL,dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        #self.embedding_layer = nn.Embedding(vocab_size,EMBED_DIM)\n",
    "        self.rnn = nn.GRU(embed_dim,enc_hidden_dim, num_layers = NUM_LAYERS ,batch_first= True ,bidirectional=enc_bidirectional)\n",
    "\n",
    "\n",
    "        # ENCODER_OUTPUT_DIM = DECODER_HIDDEN_SIZE\n",
    "        self.fc = nn.Linear(2*enc_hidden_dim,enc_output_dim) \n",
    "\n",
    "        self.fc_out = nn.Linear(enc_output_dim,1)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inp,inp_len):\n",
    "        \n",
    "        #embedded_input = self.embedding_layer(inp)\n",
    "        embedded_input = inp   # [batch_size, input_seq_length, embed_dim ]\n",
    "        \n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(embedded_input,inp_len,batch_first=True,enforce_sorted=False)\n",
    "        packed_output , hidden = self.rnn(packed_embedding)  # hidden = [D*num_layers, batch_size , hidden_dim ]\n",
    "        outputs, _  = nn.utils.rnn.pad_packed_sequence(packed_output,batch_first=True)  # [batch_size, inp_seq_length, hidden_dim]\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)))  # [batch_size, decoder_hidden_size]\n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear(enc_hidden_dim+dec_hidden_dim,dec_hidden_dim)\n",
    "        self.v = nn.Linear(dec_hidden_dim,1)\n",
    "\n",
    "    def forward(self,hidden,encoder_outputs, encoder_length_mask):\n",
    "        \n",
    "        # encoder_outputs = [batch_size,seq_length, enc_hidden_dim][2*ENCODER_HIDDEN_DIM or ENCODER_HIDDEN_DIM]\n",
    "        # hidden = [batch_size,  dec_hidden_dim]\n",
    "        # encoder_length_mask = [batch_size, seq_length]\n",
    "\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        h = hidden.unsqueeze(1).repeat(1,src_len,1)  # h = [batch_size,seq_length,dec_hidden_dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((h,encoder_outputs),dim=2)))  #[batch_size,seq_length,dec_hidden_dim]\n",
    "        attention_scores = self.v(energy).squeeze(2)  # attention_scores = [batch_size , seq_length ]\n",
    "        attention_scores = attention_scores.masked_fill(encoder_length_mask==1, -1e10)   # Fill padding tokens with a lower value\n",
    "        attention_scores = F.softmax(attention_scores,dim=1)\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src_lengths,max_src_length):\n",
    "\n",
    "        src_mask = torch.zeros((len(src_lengths),max_src_length),dtype=torch.int64)\n",
    "        for i in range(len(src_lengths)):\n",
    "\n",
    "            src_mask[i,src_lengths[i]:] = 1\n",
    "        return src_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc = Encoder(30,30,15,10)\n",
    "\n",
    "inp = torch.randn((3,20,30))\n",
    "inp_len = [20 for i in range(3)]\n",
    "outputs, hidden = enc(inp,inp_len)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,enc_hidden_dim,dec_hidden_dim,dec_output_dim,emb_dim):\n",
    "        \n",
    "        # enc_hidden_dim = 2*ENCODER_HIDDEN_DIM or ENCODER_HIDDEN_DIM\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention = Attention(enc_hidden_dim,dec_hidden_dim)\n",
    "        self.embedding_layer = nn.Embedding(vocab_size,emb_dim)\n",
    "        self.rnn = nn.GRU(enc_hidden_dim + emb_dim,dec_hidden_dim,batch_first = True)\n",
    "\n",
    "        self.fc_out = nn.Linear(enc_hidden_dim + emb_dim + dec_hidden_dim , dec_output_dim)\n",
    "        \n",
    "        #self.fc_tilde = nn.Linear(enc_hidden_dim + emb_dim + dec_hidden_dim , dec_hidden_dim)\n",
    "        #self.max_out_layer = nn.MaxPool1d(kernel_size=2)\n",
    "        #self.W0 = nn.Linear(dec_hidden_dim>>1,emb_dim)\n",
    "        #self.prob_out = nn.Linear(emb_dim,vocab_size)\n",
    "        #self.prob_out.weight = self.embedding_layer.weight\n",
    "        \n",
    "\n",
    "    def forward(self,input,hidden,encoder_outputs,encoder_length_mask):\n",
    "            # encoder outputs =  batch_size , seq_len , encoder_output_dim\n",
    "            # hidden = batch_size , hidden_dim\n",
    "            # input = batch_size\n",
    "            \n",
    "            input = input.unsqueeze(0) # [1,batch_size]\n",
    "            embedded = self.embedding_layer(input) # [1,batch_size,embed_dim]\n",
    "\n",
    "            embedded = embedded.permute(1,0,2) #[ batch_size, seq_length=1, embed_dim ]\n",
    "\n",
    "            attention_vector = self.attention(hidden,encoder_outputs,encoder_length_mask) # [ batch_size , seq_length ]\n",
    "            attention_vector = attention_vector.unsqueeze(1) # [batch_size , 1 , seq_length ]\n",
    "\n",
    "            weighted = torch.bmm(attention_vector,encoder_outputs) # [ batch_size, 1, encoder_output_dim]\n",
    "            #weighted = weighted.permute(1,0,2) #[1 , batch_size , encoder_output_dim]\n",
    "\n",
    "\n",
    "            rnn_input = torch.cat((embedded,weighted),dim=2) #[batch_size, seq_length=1, encoder + decoder]\n",
    "\n",
    "            out,h = self.rnn(rnn_input,hidden.unsqueeze(0)) # consider only a single layer (1.) so unsqueeze(0)\n",
    "\n",
    "            # out = [batch_size, seq_length = 1, decoder_hidden_out (bidirectional)]\n",
    "            # hidden = [D*num_layers,batch_size, decoder_hidden_out]\n",
    "\n",
    "\n",
    "            embedded = embedded.squeeze(1)  # [batch_size,embed_dim]\n",
    "            out = out.squeeze(1)    #[batch_size, decoder_hidden_out] # Have to change if the number of layers is changed to more than 1\n",
    "            weighted = weighted.squeeze(1)  # [batch_size,encoder_output_dim] \n",
    "            prediction = self.fc_out(torch.cat([embedded,out,weighted],dim=1)) #[batch_size, decoder_output_dim]\n",
    "            \n",
    "            \n",
    "            #prediction = F.softmax(self.fc_out(torch.cat([embedded,out,weighted],dim=1)),dim=1) #[batch_size, decoder_output_dim]\n",
    "            #t_tilde =  self.fc_tilde(torch.cat([embedded,out,weighted],dim=1)) # [batch_size, decoder_hidden_dim]\n",
    "            #t_tilde = self.max_out_layer(t_tilde.unsqueeze(1)) #[batch_size,1,decoder_hidden_dim/2]\n",
    "            #t_tilde = t_tilde.squeeze(1)\n",
    "\n",
    "            #inter_step  = self.W0(t_tilde) # [ batch_size , dec_embeddim]\n",
    "            #prediction = self.prob_out(inter_step) #[  batch_size , vocab_size]\n",
    "            #prediction = F.softmax(prediction,dim=1)\n",
    "            return prediction, h.squeeze(0), attention_vector # Reduce the number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                input_embed_dim = EMBED_DIM,\n",
    "                encoder_hidden_dim = ENC_HIDDEN_DIM,\n",
    "                encoder_hidden_output = ENC_OUTPUT_DIM,\n",
    "                enc_num_layers = 1,\n",
    "                enc_bidirectional = ENC_BIDIRECTIONAL,\n",
    "\n",
    "                dec_vocab_size = len(output_vocab),\n",
    "                dec_embed_dim = DEC_EMBED_DIM,\n",
    "                dec_hidden_dim  =DEC_HIDDEN_DIM,\n",
    "                device_train = device_cpu\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_embed_dim,encoder_hidden_dim,encoder_hidden_output,enc_num_layers,enc_bidirectional=enc_bidirectional)\n",
    "        enc_bidirectional_factor = 2 if enc_bidirectional else 1\n",
    "        self.decoder = Decoder(dec_vocab_size,enc_bidirectional_factor*encoder_hidden_dim,dec_hidden_dim=dec_hidden_dim,dec_output_dim=dec_vocab_size,emb_dim=dec_embed_dim)\n",
    "        self.device_train = device_train\n",
    "\n",
    "\n",
    "    def create_mask(self, src_lengths,max_src_length):\n",
    "\n",
    "        src_mask = torch.zeros((len(src_lengths),max_src_length),dtype=torch.int64)\n",
    "        for i in range(len(src_lengths)):\n",
    "\n",
    "            src_mask[i,src_lengths[i]:] = 1\n",
    "        return src_mask\n",
    "        \n",
    "    def forward(self,source,source_len,target,teacher_forcing_ratio = 0.0):\n",
    "        #   source = [batch_size, max_src_len]\n",
    "        #   source_len = [length of sentence in the batch]\n",
    "        #   target = [batch_size,traget_length]\n",
    "        #   teacher_forcing_ratio = probability to use teacher forcinbg\n",
    "\n",
    "        batch_size = source.shape[0]\n",
    "        target_length = target.shape[1]\n",
    "        target_vocab_size = self.decoder.vocab_size\n",
    "        outputs= torch.zeros(batch_size,target_length,target_vocab_size).to(self.device_train)\n",
    "        encoder_outputs , hidden = self.encoder(source,source_len)\n",
    "\n",
    "        inp = target[:,0]        \n",
    "        enc_mask = self.create_mask(source_len,int(encoder_outputs.shape[1]))\n",
    "        enc_mask = enc_mask.to(self.device_train)\n",
    "        for t in range(1,target_length):\n",
    "            decoder_output, hidden, attention_vector =  self.decoder(inp,hidden,encoder_outputs,enc_mask)\n",
    "\n",
    "            outputs[:,t,:] = decoder_output # batch_size, vocab_size\n",
    "            teacher_force = random.random() < teacher_forcing_ratio \n",
    "\n",
    "            top1 = decoder_output.argmax(1)\n",
    "\n",
    "            inp = target[:,t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weights(model : TranslationModel):\n",
    "    \n",
    "    for name,param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data,mean=0,std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data,0)\n",
    "    \n",
    "    \n",
    "    nn.init.xavier_uniform_(model.encoder.fc_out.weight)\n",
    "    nn.init.normal_(model.decoder.attention.attn.weight, mean=0, std=0.001)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = TranslationModel()\n",
    "apply_weights(t)\n",
    "batch_data = next(iter(train_dataloader))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=output_vocab['<eos>'])\n",
    "optimizer = optim.Adam(t.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "model_output = t(batch_data['src'],batch_data['src_length'],batch_data['trg'])\n",
    "\n",
    "\n",
    "model_out_reshaped = model_output[1:].view(-1,model_output.shape[-1])\n",
    "reshaped_target = batch_data['trg'][1:].view(-1)\n",
    "loss_value = criterion(model_out_reshaped,reshaped_target)\n",
    "loss_value.backward()\n",
    "nn.utils.clip_grad_norm_(t.parameters(),5)\n",
    "optimizer.step()\n",
    "print(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import  datetime\n",
    "\n",
    "import os\n",
    "\n",
    "def train_model(model,num_epochs,train_loader,valid_loader,optimizer,criterion,checkpoint_name='translation_model.pth',device_train = device_cpu):\n",
    "    \n",
    "    current_datetime = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    tensorboard_name =  checkpoint_name + current_datetime\n",
    "    writer = SummaryWriter(os.getcwd() +'\\\\runs\\\\' + tensorboard_name)\n",
    "\n",
    "    best_validation_loss = 1000.0\n",
    "    model = model.to(device_train)\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        training_loss = 0.0\n",
    "        model.train()\n",
    "        for i,batch in enumerate(train_loader):\n",
    "\n",
    "            source, source_length, target = batch['src'], batch['src_length'], batch['trg']\n",
    "\n",
    "            source = source.to(device_train)\n",
    "            target = target.to(device_train)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # model_output = [batch_size, output_seq_length,vocab_size]\n",
    "            model_output  = model(source,source_length,target)\n",
    "\n",
    "\n",
    "            source = source.to(device_cpu)\n",
    "            target = target.to(device_cpu)\n",
    "            model_output = model_output.to(device_cpu)\n",
    "            \n",
    "            model_out_reshaped = model_output[1:].view(-1,model_output.shape[-1]) \n",
    "            #print(model_out_reshaped.shape)\n",
    "            reshaped_target = target[1:].view(-1)\n",
    "            #print(reshaped_target.shape)\n",
    "\n",
    "            loss_value = criterion(model_out_reshaped,reshaped_target)\n",
    "            loss_value.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(),5)\n",
    "\n",
    "            optimizer.step()\n",
    "            training_loss += loss_value.item()\n",
    "        \n",
    "        print(\"Epoch \" + str(e) + \" Training Loss Value = \" + str(training_loss/len(train_loader)))\n",
    "        writer.add_scalars('Train/Loss vs Epoch',{'train' :training_loss/len(train_loader)},e)\n",
    "\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, batch in enumerate(valid_loader):\n",
    "                source, source_length, target = batch['src'], batch['src_length'], batch['trg']\n",
    "\n",
    "                source = source.to(device_train)\n",
    "                target = target.to(device_train)\n",
    "\n",
    "                model_output  = model(source,source_length,target,0)\n",
    "\n",
    "\n",
    "                source = source.to(device_cpu)\n",
    "                target = target.to(device_cpu)\n",
    "                model_output = model_output.to(device_cpu)\n",
    "                \n",
    "                model_out_reshaped = model_output[1:].view(-1,model_output.shape[-1])\n",
    "                reshaped_target = target[1:].view(-1)\n",
    "                loss_value = criterion(model_out_reshaped,reshaped_target)\n",
    "\n",
    "                validation_loss += loss_value.item()\n",
    "        averaged_validation_loss = validation_loss/len(valid_loader)\n",
    "        writer.add_scalars('Train/Loss vs Epoch',{'valid' :averaged_validation_loss},e)\n",
    "\n",
    "        print(\"Epoch \" + str(e) + \" Validation Loss Value = \" + str(averaged_validation_loss))\n",
    "        print(\"\")\n",
    "        if (averaged_validation_loss <= best_validation_loss):\n",
    "            best_validation_loss = averaged_validation_loss\n",
    "            torch.save(model.state_dict(),checkpoint_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss Value = 1.163891459941864\n",
      "Epoch 0 Validation Loss Value = 0.5932923158009847\n",
      "\n",
      "Epoch 1 Training Loss Value = 0.45339826226234436\n",
      "Epoch 1 Validation Loss Value = 0.35730191071828205\n",
      "\n",
      "Epoch 2 Training Loss Value = 0.3102092615365982\n",
      "Epoch 2 Validation Loss Value = 0.2731743710381644\n",
      "\n",
      "Epoch 3 Training Loss Value = 0.25460091143846514\n",
      "Epoch 3 Validation Loss Value = 0.2462291663128232\n",
      "\n",
      "Epoch 4 Training Loss Value = 0.23632875949144364\n",
      "Epoch 4 Validation Loss Value = 0.23963613216839139\n",
      "\n",
      "Epoch 5 Training Loss Value = 0.27409595507383344\n",
      "Epoch 5 Validation Loss Value = 0.2315670999269637\n",
      "\n",
      "Epoch 6 Training Loss Value = 0.22873130881786347\n",
      "Epoch 6 Validation Loss Value = 0.22683260388790616\n",
      "\n",
      "Epoch 7 Training Loss Value = 0.22682017707824706\n",
      "Epoch 7 Validation Loss Value = 0.2327646166086197\n",
      "\n",
      "Epoch 8 Training Loss Value = 0.2267318130135536\n",
      "Epoch 8 Validation Loss Value = 0.225244991363041\n",
      "\n",
      "Epoch 9 Training Loss Value = 0.22402387470006943\n",
      "Epoch 9 Validation Loss Value = 0.2243365002056909\n",
      "\n",
      "Epoch 10 Training Loss Value = 0.22558936250209807\n",
      "Epoch 10 Validation Loss Value = 0.22433994426613763\n",
      "\n",
      "Epoch 11 Training Loss Value = 0.2234400913119316\n",
      "Epoch 11 Validation Loss Value = 0.22507983304205395\n",
      "\n",
      "Epoch 12 Training Loss Value = 0.22683608347177506\n",
      "Epoch 12 Validation Loss Value = 0.22686643070644802\n",
      "\n",
      "Epoch 13 Training Loss Value = 0.2233030356168747\n",
      "Epoch 13 Validation Loss Value = 0.22373271674390824\n",
      "\n",
      "Epoch 14 Training Loss Value = 0.22267759007215499\n",
      "Epoch 14 Validation Loss Value = 0.2258774713864402\n",
      "\n",
      "Epoch 15 Training Loss Value = 0.22361926656961442\n",
      "Epoch 15 Validation Loss Value = 0.22412962123515115\n",
      "\n",
      "Epoch 16 Training Loss Value = 0.2235048635005951\n",
      "Epoch 16 Validation Loss Value = 0.2250488575488802\n",
      "\n",
      "Epoch 17 Training Loss Value = 0.22642813074588775\n",
      "Epoch 17 Validation Loss Value = 0.22380445873926555\n",
      "\n",
      "Epoch 18 Training Loss Value = 0.22211402934789656\n",
      "Epoch 18 Validation Loss Value = 0.22292590377822755\n",
      "\n",
      "Epoch 19 Training Loss Value = 0.22261916947364807\n",
      "Epoch 19 Validation Loss Value = 0.22308496492249624\n",
      "\n",
      "Epoch 20 Training Loss Value = 0.2219785475730896\n",
      "Epoch 20 Validation Loss Value = 0.2231141979259158\n",
      "\n",
      "Epoch 21 Training Loss Value = 0.22162844842672347\n",
      "Epoch 21 Validation Loss Value = 0.22497144554342544\n",
      "\n",
      "Epoch 22 Training Loss Value = 0.22533301216363907\n",
      "Epoch 22 Validation Loss Value = 0.22396978735923767\n",
      "\n",
      "Epoch 23 Training Loss Value = 0.22182473665475846\n",
      "Epoch 23 Validation Loss Value = 0.2269723221423134\n",
      "\n",
      "Epoch 24 Training Loss Value = 0.22227880042791368\n",
      "Epoch 24 Validation Loss Value = 0.22365986662251608\n",
      "\n",
      "Epoch 25 Training Loss Value = 0.22082265019416808\n",
      "Epoch 25 Validation Loss Value = 0.22628149532136463\n",
      "\n",
      "Epoch 26 Training Loss Value = 0.2218006562590599\n",
      "Epoch 26 Validation Loss Value = 0.2235356585847007\n",
      "\n",
      "Epoch 27 Training Loss Value = 0.230875850379467\n",
      "Epoch 27 Validation Loss Value = 0.22439640240063743\n",
      "\n",
      "Epoch 28 Training Loss Value = 0.22099772453308106\n",
      "Epoch 28 Validation Loss Value = 0.22353901158249567\n",
      "\n",
      "Epoch 29 Training Loss Value = 0.22027305620908738\n",
      "Epoch 29 Validation Loss Value = 0.22278230057822335\n",
      "\n",
      "Epoch 30 Training Loss Value = 0.22003414344787597\n",
      "Epoch 30 Validation Loss Value = 0.22400078934336465\n",
      "\n",
      "Epoch 31 Training Loss Value = 0.22025696682929993\n",
      "Epoch 31 Validation Loss Value = 0.2228526281458991\n",
      "\n",
      "Epoch 32 Training Loss Value = 0.22039578008651733\n",
      "Epoch 32 Validation Loss Value = 0.224127159232185\n",
      "\n",
      "Epoch 33 Training Loss Value = 0.2200276472568512\n",
      "Epoch 33 Validation Loss Value = 0.2240178639453555\n",
      "\n",
      "Epoch 34 Training Loss Value = 0.22196795427799224\n",
      "Epoch 34 Validation Loss Value = 0.23533792699140216\n",
      "\n",
      "Epoch 35 Training Loss Value = 0.22226448613405228\n",
      "Epoch 35 Validation Loss Value = 0.22614303798902602\n",
      "\n",
      "Epoch 36 Training Loss Value = 0.2200836550593376\n",
      "Epoch 36 Validation Loss Value = 0.23029236400884295\n",
      "\n",
      "Epoch 37 Training Loss Value = 0.22173547488451004\n",
      "Epoch 37 Validation Loss Value = 0.22394309062806386\n",
      "\n",
      "Epoch 38 Training Loss Value = 0.21940281254053115\n",
      "Epoch 38 Validation Loss Value = 0.22372823527881078\n",
      "\n",
      "Epoch 39 Training Loss Value = 0.21904044884443283\n",
      "Epoch 39 Validation Loss Value = 0.22368602975020332\n",
      "\n",
      "Epoch 40 Training Loss Value = 0.21904244804382325\n",
      "Epoch 40 Validation Loss Value = 0.22446611168838682\n",
      "\n",
      "Epoch 41 Training Loss Value = 0.21973900836706162\n",
      "Epoch 41 Validation Loss Value = 0.22806663243543535\n",
      "\n",
      "Epoch 42 Training Loss Value = 0.2222504006624222\n",
      "Epoch 42 Validation Loss Value = 0.22526705525224172\n",
      "\n",
      "Epoch 43 Training Loss Value = 0.21879346024990082\n",
      "Epoch 43 Validation Loss Value = 0.22543402536520882\n",
      "\n",
      "Epoch 44 Training Loss Value = 0.2187795102596283\n",
      "Epoch 44 Validation Loss Value = 0.2284431748446964\n",
      "\n",
      "Epoch 45 Training Loss Value = 0.21992403668165206\n",
      "Epoch 45 Validation Loss Value = 0.2279129825414173\n",
      "\n",
      "Epoch 46 Training Loss Value = 0.21823007619380952\n",
      "Epoch 46 Validation Loss Value = 0.22539958263200427\n",
      "\n",
      "Epoch 47 Training Loss Value = 0.21774346375465392\n",
      "Epoch 47 Validation Loss Value = 0.22507985220068977\n",
      "\n",
      "Epoch 48 Training Loss Value = 0.21768017166852952\n",
      "Epoch 48 Validation Loss Value = 0.22656274977184476\n",
      "\n",
      "Epoch 49 Training Loss Value = 0.22065845429897307\n",
      "Epoch 49 Validation Loss Value = 0.2261717908439182\n",
      "\n",
      "Epoch 50 Training Loss Value = 0.21773751968145372\n",
      "Epoch 50 Validation Loss Value = 0.2270435622287175\n",
      "\n",
      "Epoch 51 Training Loss Value = 0.21679729336500167\n",
      "Epoch 51 Validation Loss Value = 0.22728241909117924\n",
      "\n",
      "Epoch 52 Training Loss Value = 0.21678946441411973\n",
      "Epoch 52 Validation Loss Value = 0.22650890927466136\n",
      "\n",
      "Epoch 53 Training Loss Value = 0.2166284214258194\n",
      "Epoch 53 Validation Loss Value = 0.22705637273334323\n",
      "\n",
      "Epoch 54 Training Loss Value = 0.21621649599075318\n",
      "Epoch 54 Validation Loss Value = 0.22709055292227912\n",
      "\n",
      "Epoch 55 Training Loss Value = 0.2191092785000801\n",
      "Epoch 55 Validation Loss Value = 0.22802459058307467\n",
      "\n",
      "Epoch 56 Training Loss Value = 0.2168004054427147\n",
      "Epoch 56 Validation Loss Value = 0.2279415111693125\n",
      "\n",
      "Epoch 57 Training Loss Value = 0.2157588706612587\n",
      "Epoch 57 Validation Loss Value = 0.2280134158948111\n",
      "\n",
      "Epoch 58 Training Loss Value = 0.21545172959566117\n",
      "Epoch 58 Validation Loss Value = 0.22835934375013625\n",
      "\n",
      "Epoch 59 Training Loss Value = 0.21560834842920304\n",
      "Epoch 59 Validation Loss Value = 0.22953641722126614\n",
      "\n",
      "Epoch 60 Training Loss Value = 0.21596241211891173\n",
      "Epoch 60 Validation Loss Value = 0.2295882607263232\n",
      "\n",
      "Epoch 61 Training Loss Value = 0.22333210855722427\n",
      "Epoch 61 Validation Loss Value = 0.22874377880777633\n",
      "\n",
      "Epoch 62 Training Loss Value = 0.21579602146148683\n",
      "Epoch 62 Validation Loss Value = 0.22935362585953303\n",
      "\n",
      "Epoch 63 Training Loss Value = 0.2147362518310547\n",
      "Epoch 63 Validation Loss Value = 0.23038992593212734\n",
      "\n",
      "Epoch 64 Training Loss Value = 0.21455486059188844\n",
      "Epoch 64 Validation Loss Value = 0.2297941358789565\n",
      "\n",
      "Epoch 65 Training Loss Value = 0.2145548951625824\n",
      "Epoch 65 Validation Loss Value = 0.22985267804728615\n",
      "\n",
      "Epoch 66 Training Loss Value = 0.21442005735635758\n",
      "Epoch 66 Validation Loss Value = 0.23038660604802388\n",
      "\n",
      "Epoch 67 Training Loss Value = 0.2144200124144554\n",
      "Epoch 67 Validation Loss Value = 0.23065308209449525\n",
      "\n",
      "Epoch 68 Training Loss Value = 0.21496375000476836\n",
      "Epoch 68 Validation Loss Value = 0.23022103711726175\n",
      "\n",
      "Epoch 69 Training Loss Value = 0.2207693462371826\n",
      "Epoch 69 Validation Loss Value = 0.23276009564361874\n",
      "\n",
      "Epoch 70 Training Loss Value = 0.21683268225193023\n",
      "Epoch 70 Validation Loss Value = 0.22978765032594167\n",
      "\n",
      "Epoch 71 Training Loss Value = 0.21493154561519623\n",
      "Epoch 71 Validation Loss Value = 0.23073599664937883\n",
      "\n",
      "Epoch 72 Training Loss Value = 0.21443500638008117\n",
      "Epoch 72 Validation Loss Value = 0.2313382230580799\n",
      "\n",
      "Epoch 73 Training Loss Value = 0.21422055178880692\n",
      "Epoch 73 Validation Loss Value = 0.23137892829993414\n",
      "\n",
      "Epoch 74 Training Loss Value = 0.21411046850681306\n",
      "Epoch 74 Validation Loss Value = 0.23155110956184446\n",
      "\n",
      "Epoch 75 Training Loss Value = 0.21411842185258864\n",
      "Epoch 75 Validation Loss Value = 0.23202887295730532\n",
      "\n",
      "Epoch 76 Training Loss Value = 0.21690596568584442\n",
      "Epoch 76 Validation Loss Value = 0.23206997816524808\n",
      "\n",
      "Epoch 77 Training Loss Value = 0.21985623639822005\n",
      "Epoch 77 Validation Loss Value = 0.23300630256297097\n",
      "\n",
      "Epoch 78 Training Loss Value = 0.21525310015678406\n",
      "Epoch 78 Validation Loss Value = 0.2319826218816969\n",
      "\n",
      "Epoch 79 Training Loss Value = 0.2144774003624916\n",
      "Epoch 79 Validation Loss Value = 0.23165342211723328\n",
      "\n",
      "Epoch 80 Training Loss Value = 0.21427088528871535\n",
      "Epoch 80 Validation Loss Value = 0.23138633985368032\n",
      "\n",
      "Epoch 81 Training Loss Value = 0.21417823666334151\n",
      "Epoch 81 Validation Loss Value = 0.2330140835709042\n",
      "\n",
      "Epoch 82 Training Loss Value = 0.2141148833632469\n",
      "Epoch 82 Validation Loss Value = 0.233067159141813\n",
      "\n",
      "Epoch 83 Training Loss Value = 0.2140994262099266\n",
      "Epoch 83 Validation Loss Value = 0.23286148692880357\n",
      "\n",
      "Epoch 84 Training Loss Value = 0.21414437115192414\n",
      "Epoch 84 Validation Loss Value = 0.23286941316392687\n",
      "\n",
      "Epoch 85 Training Loss Value = 0.2141483971476555\n",
      "Epoch 85 Validation Loss Value = 0.2336273046713027\n",
      "\n",
      "Epoch 86 Training Loss Value = 0.22069242066144942\n",
      "Epoch 86 Validation Loss Value = 0.2348033468874674\n",
      "\n",
      "Epoch 87 Training Loss Value = 0.22315661334991455\n",
      "Epoch 87 Validation Loss Value = 0.23110201481788878\n",
      "\n",
      "Epoch 88 Training Loss Value = 0.215112539768219\n",
      "Epoch 88 Validation Loss Value = 0.23165212926410494\n",
      "\n",
      "Epoch 89 Training Loss Value = 0.21439124703407286\n",
      "Epoch 89 Validation Loss Value = 0.23167387857323601\n",
      "\n",
      "Epoch 90 Training Loss Value = 0.21413544648885727\n",
      "Epoch 90 Validation Loss Value = 0.23161099732868254\n",
      "\n",
      "Epoch 91 Training Loss Value = 0.2140576692223549\n",
      "Epoch 91 Validation Loss Value = 0.23183718512928675\n",
      "\n",
      "Epoch 92 Training Loss Value = 0.2140329893231392\n",
      "Epoch 92 Validation Loss Value = 0.2320491342790543\n",
      "\n",
      "Epoch 93 Training Loss Value = 0.21403608578443528\n",
      "Epoch 93 Validation Loss Value = 0.23235849705007341\n",
      "\n",
      "Epoch 94 Training Loss Value = 0.21403654158115387\n",
      "Epoch 94 Validation Loss Value = 0.23245721369508712\n",
      "\n",
      "Epoch 95 Training Loss Value = 0.21400348365306854\n",
      "Epoch 95 Validation Loss Value = 0.23297111595433856\n",
      "\n",
      "Epoch 96 Training Loss Value = 0.21413470649719238\n",
      "Epoch 96 Validation Loss Value = 0.23420850483198014\n",
      "\n",
      "Epoch 97 Training Loss Value = 0.22014334905147553\n",
      "Epoch 97 Validation Loss Value = 0.23559796360750046\n",
      "\n",
      "Epoch 98 Training Loss Value = 0.21863309121131896\n",
      "Epoch 98 Validation Loss Value = 0.23076316927160537\n",
      "\n",
      "Epoch 99 Training Loss Value = 0.21579596495628356\n",
      "Epoch 99 Validation Loss Value = 0.23231565904995752\n",
      "\n",
      "Epoch 100 Training Loss Value = 0.21471531826257706\n",
      "Epoch 100 Validation Loss Value = 0.23182209332784018\n",
      "\n",
      "Epoch 101 Training Loss Value = 0.21424694216251372\n",
      "Epoch 101 Validation Loss Value = 0.2326977179637031\n",
      "\n",
      "Epoch 102 Training Loss Value = 0.2140781751871109\n",
      "Epoch 102 Validation Loss Value = 0.23399916125668418\n",
      "\n",
      "Epoch 103 Training Loss Value = 0.21402297329902648\n",
      "Epoch 103 Validation Loss Value = 0.23324797290658195\n",
      "\n",
      "Epoch 104 Training Loss Value = 0.21401162433624268\n",
      "Epoch 104 Validation Loss Value = 0.234284285042021\n",
      "\n",
      "Epoch 105 Training Loss Value = 0.2140220514535904\n",
      "Epoch 105 Validation Loss Value = 0.234193112642046\n",
      "\n",
      "Epoch 106 Training Loss Value = 0.21403170120716095\n",
      "Epoch 106 Validation Loss Value = 0.23504504206634702\n",
      "\n",
      "Epoch 107 Training Loss Value = 0.21414447116851806\n",
      "Epoch 107 Validation Loss Value = 0.23484213815795052\n",
      "\n",
      "Epoch 108 Training Loss Value = 0.21414474618434906\n",
      "Epoch 108 Validation Loss Value = 0.23403597942420415\n",
      "\n",
      "Epoch 109 Training Loss Value = 0.22281667304039002\n",
      "Epoch 109 Validation Loss Value = 0.2330706034387861\n",
      "\n",
      "Epoch 110 Training Loss Value = 0.2167411050796509\n",
      "Epoch 110 Validation Loss Value = 0.23293040929332612\n",
      "\n",
      "Epoch 111 Training Loss Value = 0.21461533123254775\n",
      "Epoch 111 Validation Loss Value = 0.23436971008777618\n",
      "\n",
      "Epoch 112 Training Loss Value = 0.21439735120534897\n",
      "Epoch 112 Validation Loss Value = 0.23419713571904197\n",
      "\n",
      "Epoch 113 Training Loss Value = 0.21408015197515487\n",
      "Epoch 113 Validation Loss Value = 0.2349059446936562\n",
      "\n",
      "Epoch 114 Training Loss Value = 0.21403804904222487\n",
      "Epoch 114 Validation Loss Value = 0.235425709022416\n",
      "\n",
      "Epoch 115 Training Loss Value = 0.21398634493350982\n",
      "Epoch 115 Validation Loss Value = 0.23539839637657953\n",
      "\n",
      "Epoch 116 Training Loss Value = 0.21399081319570543\n",
      "Epoch 116 Validation Loss Value = 0.2356251019334036\n",
      "\n",
      "Epoch 117 Training Loss Value = 0.2140025771856308\n",
      "Epoch 117 Validation Loss Value = 0.23569146910357097\n",
      "\n",
      "Epoch 118 Training Loss Value = 0.2139886507987976\n",
      "Epoch 118 Validation Loss Value = 0.23595230801711006\n",
      "\n",
      "Epoch 119 Training Loss Value = 0.21398820519447326\n",
      "Epoch 119 Validation Loss Value = 0.2361456531853903\n",
      "\n",
      "Epoch 120 Training Loss Value = 0.21468557012081146\n",
      "Epoch 120 Validation Loss Value = 0.2361960888855041\n",
      "\n",
      "Epoch 121 Training Loss Value = 0.22240169978141786\n",
      "Epoch 121 Validation Loss Value = 0.23344675132206508\n",
      "\n",
      "Epoch 122 Training Loss Value = 0.21672395795583724\n",
      "Epoch 122 Validation Loss Value = 0.23425608542230394\n",
      "\n",
      "Epoch 123 Training Loss Value = 0.21512448233366013\n",
      "Epoch 123 Validation Loss Value = 0.23470429248279995\n",
      "\n",
      "Epoch 124 Training Loss Value = 0.21455635219812394\n",
      "Epoch 124 Validation Loss Value = 0.23393351028835963\n",
      "\n",
      "Epoch 125 Training Loss Value = 0.21417533659934998\n",
      "Epoch 125 Validation Loss Value = 0.23376315737527514\n",
      "\n",
      "Epoch 126 Training Loss Value = 0.2140711579322815\n",
      "Epoch 126 Validation Loss Value = 0.23438688521347348\n",
      "\n",
      "Epoch 127 Training Loss Value = 0.21400744777917863\n",
      "Epoch 127 Validation Loss Value = 0.23505812339366428\n",
      "\n",
      "Epoch 128 Training Loss Value = 0.21397935169935225\n",
      "Epoch 128 Validation Loss Value = 0.23570159101296986\n",
      "\n",
      "Epoch 129 Training Loss Value = 0.21399413692951202\n",
      "Epoch 129 Validation Loss Value = 0.23577859004338583\n",
      "\n",
      "Epoch 130 Training Loss Value = 0.21395771312713624\n",
      "Epoch 130 Validation Loss Value = 0.2354870280103078\n",
      "\n",
      "Epoch 131 Training Loss Value = 0.21403934770822525\n",
      "Epoch 131 Validation Loss Value = 0.23584489855501387\n",
      "\n",
      "Epoch 132 Training Loss Value = 0.21420988631248475\n",
      "Epoch 132 Validation Loss Value = 0.23500753418793754\n",
      "\n",
      "Epoch 133 Training Loss Value = 0.2237069684267044\n",
      "Epoch 133 Validation Loss Value = 0.23352950456596555\n",
      "\n",
      "Epoch 134 Training Loss Value = 0.21705818784236908\n",
      "Epoch 134 Validation Loss Value = 0.23354632868653252\n",
      "\n",
      "Epoch 135 Training Loss Value = 0.21513218224048614\n",
      "Epoch 135 Validation Loss Value = 0.23418373011407398\n",
      "\n",
      "Epoch 136 Training Loss Value = 0.21447269529104232\n",
      "Epoch 136 Validation Loss Value = 0.23410170466180832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = TranslationModel(device_train=device_fast)\n",
    "apply_weights(t)\n",
    "train_model(t,150,train_dataloader,valid_dataloader,optim.Adam(t.parameters(),lr=0.001),nn.CrossEntropyLoss(),device_train= device_fast,checkpoint_name='translation_model_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TranslationModel()\n",
    "t.load_state_dict(torch.load('./translation_model.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, max_len = 10,glove=glove):\n",
    "\n",
    "    model.eval()\n",
    "    nl_date = sentence.replace(\"\\'\",\"\").strip()\n",
    "    \n",
    "    nl_date = nl_date.lower()\n",
    "\n",
    "    if \"/\" in nl_date:\n",
    "        #number_list = list(nl_date)\n",
    "        #nl_date = \" \".join(number_list)\n",
    "        split_on_slash = nl_date.split(\"/\")\n",
    "        nl_date = \" / \".join(split_on_slash)\n",
    "\n",
    "        \n",
    "    embeddings = []\n",
    "    for word in nl_date.split(' '):\n",
    "        embeddings.append(glove[word])\n",
    "        #embeddings.append(torch.tensor(fasttext_model.get_word_vector(word)))\n",
    "\n",
    "\n",
    "    current_inp_length = len(embeddings)\n",
    "    inp_embeddings = torch.stack(embeddings)\n",
    "    inp_embeddings = inp_embeddings.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        encoder_outputs, hidden = model.encoder(inp_embeddings, [current_inp_length])\n",
    "        mask = model.create_mask([current_inp_length],max([current_inp_length]))\n",
    "        attentions = torch.zeros(max_len, 1, inp_embeddings.shape[1])\n",
    "        trg_indexes = []\n",
    "        for i in range(max_len):\n",
    "\n",
    "            if i==0:\n",
    "                trg_tensor = torch.LongTensor([output_vocab['<sos>']])\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                # input,hidden,encoder_outputs,encoder_length_mask\n",
    "                output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
    "\n",
    "            attentions[i] = attention\n",
    "            \n",
    "            pred_token = output.argmax(1).item()\n",
    "        \n",
    "            trg_indexes.append(pred_token)\n",
    "            trg_tensor = torch.LongTensor([pred_token])\n",
    "\n",
    "            if pred_token == output_vocab['<eos>']:\n",
    "                break\n",
    "    \n",
    "    trg_tokens = [output_index_to_word[i] for i in trg_indexes]\n",
    "    return trg_tokens, attentions\n",
    "    #print(trg_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '0', '1', '9', '-', '0', '4', '-', '2', '1']\n"
     ]
    }
   ],
   "source": [
    "output_tokens , attention = translate_sentence('21/04/2019',t.to(device_cpu))\n",
    "print(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmetrics(model,sentences,ground_truths):\n",
    "    exact_match_count = 0\n",
    "    per_word_matches = [0 for i in range(10)]\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        trg_tokens, attention_weights = translate_sentence(sentences[i],model)\n",
    "        print(trg_tokens)\n",
    "        ground_truth_tokens = list(ground_truths[i])\n",
    "\n",
    "        exact_match = True\n",
    "        for i in range(len(ground_truth_tokens)):\n",
    "\n",
    "            if trg_tokens[i] == ground_truth_tokens[i]:\n",
    "                per_word_matches[i]+=1\n",
    "            else:\n",
    "                exact_match = False\n",
    "        \n",
    "        if exact_match:\n",
    "            exact_match_count+=1\n",
    "\n",
    "    \n",
    "    number = len(sentences)\n",
    "    per_output_accuracy = [ 0.0 for i in range(10)]\n",
    "    exact_match_accuracy = ((1.0*exact_match_count)/number)*100\n",
    "\n",
    "    for i in range(len(per_word_matches)):\n",
    "\n",
    "        per_output_accuracy[i] = ((1.0*per_word_matches[i]) / number) * 100\n",
    "\n",
    "    return exact_match_accuracy,per_output_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '9', '9', '8', '-', '0', '1', '-', '0', '4']\n",
      "['2', '0', '0', '9', '-', '0', '5', '-', '2', '0']\n",
      "['2', '0', '1', '2', '-', '0', '9', '-', '2', '5']\n",
      "['1', '0', '5', '6', '-', '0', '1', '-', '0', '1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(75.0, [75.0, 100.0, 100.0, 100.0, 100.0, 100.0, 75.0, 100.0, 100.0, 100.0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = ['1998-01-04','2009-05-20','2012-09-25','2056-05-01']\n",
    "sentences = ['4 jan 1998','May 20 2009','September 25 2012','05/01/56']\n",
    "\n",
    "getmetrics(t,sentences,ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(sentence, translation, attention):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(attention, cmap='viridis')\n",
    "\n",
    "    nl_date = sentence.replace(\"\\'\",\"\").strip()\n",
    "    nl_date = nl_date.lower()\n",
    "\n",
    "    if \"/\" in nl_date:\n",
    "        #number_list = list(nl_date)\n",
    "        #nl_date = \" \".join(number_list)\n",
    "        split_on_slash = nl_date.split(\"/\")\n",
    "        nl_date = \" / \".join(split_on_slash)\n",
    "\n",
    "    src = nl_date.split(\" \")\n",
    "    trg = list(translation)\n",
    "\n",
    "    ax.set_xticklabels(src, minor=False, rotation='vertical')\n",
    "    ax.set_yticklabels(trg, minor=False)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    # and the x-ticks on top\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(np.arange(attention.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(attention.shape[0]) + 0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.colorbar(heatmap)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivitsu329\\AppData\\Local\\Temp\\ipykernel_8536\\2566052727.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(src, minor=False, rotation='vertical')\n",
      "C:\\Users\\Vivitsu329\\AppData\\Local\\Temp\\ipykernel_8536\\2566052727.py:19: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(trg, minor=False)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_tokens , attention = translate_sentence('2016 20 Sunday May',t.to(device_cpu))\n",
    "plot_heatmap('2016 May 20 Sunday',''.join(output_tokens),attention.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(100,64, num_layers= 2 ,batch_first= True ,bidirectional=True)\n",
    "inp = torch.randn((2,8,100))\n",
    "outputs,hidden = rnn(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inq = torch.cat([hidden[-i , : , :] for i in range(4)],dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn(inp)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(10,30)\n",
    "\n",
    "embedding_layer(torch.tensor([[10,2,3],[4,5,6]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('nlpenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db937d70f0b42a9864491ccec092d177c951bc6bfc86971ae63a9e250c306d60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
