{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torchtext.vocab import GloVe\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import SubsetRandomSampler,DataLoader\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "\n",
    "EMBED_DIM = 300 \n",
    "ENC_BIDIRECTIONAL = True\n",
    "ENC_BIDIRECTIONAL_FACTOR = 2 if ENC_BIDIRECTIONAL else 1\n",
    "ENC_HIDDEN_DIM = 256\n",
    "DEC_HIDDEN_DIM  = 256\n",
    "ENC_OUTPUT_DIM =  DEC_HIDDEN_DIM\n",
    "DEC_EMBED_DIM = 300\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device_cpu = torch.device('cpu')\n",
    "device_fast = torch.device('cpu')\n",
    "\n",
    "if torch.has_mps:\n",
    "    device_fast = torch.device('mps')\n",
    "elif torch.has_cuda:\n",
    "    device_fast = torch.device('cuda')\n",
    "\n",
    "\n",
    "output_index_to_word = {}\n",
    "output_vocab = {}\n",
    "for i in range(0,10):\n",
    "    output_vocab[str(i)] = i\n",
    "    output_index_to_word[i] = str(i)\n",
    "output_vocab['-'] = 10\n",
    "output_index_to_word[10] = '-'\n",
    "output_vocab['<sos>'] = 11\n",
    "output_index_to_word[11] = '<sos>'\n",
    "output_vocab['<eos>'] = 12\n",
    "output_index_to_word[12] = '<eos>'\n",
    "\n",
    "glove = GloVe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_training_data(filename='./Assignment4aDataset.txt',glove=glove):\n",
    "    f = open(filename,'r')\n",
    "    dataset = []\n",
    "    for line in f.readlines():       \n",
    "         \n",
    "        nl_date , out_date = line.split(',')\n",
    "        nl_date = nl_date.replace(\"\\'\",\"\").strip()\n",
    "        out_date = out_date.replace(\"\\'\",\"\").strip()\n",
    "\n",
    "        split_on_slash = nl_date.split(\"/\")\n",
    "        nl_date = \" / \".join(split_on_slash)\n",
    "\n",
    "        embeddings = []\n",
    "        for word in nl_date.split(' '):\n",
    "            embeddings.append(glove[word])\n",
    "        \n",
    "        current_inp_length = len(embeddings)\n",
    "        embeddings = torch.stack(embeddings)\n",
    "\n",
    "        target = []\n",
    "        target.append(output_vocab['<sos>'])\n",
    "\n",
    "        for character in list(out_date):\n",
    "            target.append(output_vocab[character])\n",
    "        \n",
    "        target.append(output_vocab['<eos>'])\n",
    "\n",
    "        dataset.append({'in' : embeddings,'in_length' : current_inp_length,'out' : target})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def collate_function(batch_data):\n",
    "    inputs = [b['in'] for b in batch_data]\n",
    "    in_lengths = [b['in_length'] for b in batch_data]\n",
    "    out = torch.tensor([b['out'] for b in batch_data])\n",
    "    inputs = pad_sequence(inputs,batch_first=True)\n",
    "    return {'src': inputs, 'src_length' : in_lengths, 'trg' : out}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(get_training_data())\n",
    "\n",
    "train_idx,valid_idx = train_test_split(np.arange(len(train_dataset)), \n",
    "    test_size=0.2,\n",
    "    shuffle= True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_dataloader = DataLoader(train_dataset,BATCH_SIZE,sampler=train_sampler,collate_fn=collate_function)\n",
    "valid_dataloader = DataLoader(train_dataset,BATCH_SIZE,sampler=valid_sampler,collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim = EMBED_DIM,enc_hidden_dim = ENC_HIDDEN_DIM,enc_output_dim = ENC_OUTPUT_DIM,NUM_LAYERS=1,enc_bidirectional=ENC_BIDIRECTIONAL,dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        #self.embedding_layer = nn.Embedding(vocab_size,EMBED_DIM)\n",
    "        self.rnn = nn.GRU(embed_dim,enc_hidden_dim, num_layers = NUM_LAYERS ,batch_first= True ,bidirectional=enc_bidirectional)\n",
    "\n",
    "\n",
    "        # ENCODER_OUTPUT_DIM = DECODER_HIDDEN_SIZE\n",
    "        self.fc = nn.Linear(2*enc_hidden_dim,enc_output_dim) \n",
    "\n",
    "        self.fc_out = nn.Linear(enc_output_dim,1)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inp,inp_len):\n",
    "        \n",
    "        #embedded_input = self.embedding_layer(inp)\n",
    "        embedded_input = inp   # [batch_size, input_seq_length, embed_dim ]\n",
    "        \n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(embedded_input,inp_len,batch_first=True,enforce_sorted=False)\n",
    "        packed_output , hidden = self.rnn(packed_embedding)  # hidden = [D*num_layers, batch_size , hidden_dim ]\n",
    "        outputs, _  = nn.utils.rnn.pad_packed_sequence(packed_output,batch_first=True)  # [batch_size, inp_seq_length, hidden_dim]\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)))  # [batch_size, decoder_hidden_size]\n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear(enc_hidden_dim+dec_hidden_dim,dec_hidden_dim)\n",
    "        self.v = nn.Linear(dec_hidden_dim,1)\n",
    "\n",
    "    def forward(self,hidden,encoder_outputs, encoder_length_mask):\n",
    "        \n",
    "        # encoder_outputs = [batch_size,seq_length, enc_hidden_dim][2*ENCODER_HIDDEN_DIM or ENCODER_HIDDEN_DIM]\n",
    "        # hidden = [batch_size,  dec_hidden_dim]\n",
    "        # encoder_length_mask = [batch_size, seq_length]\n",
    "\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        h = hidden.unsqueeze(1).repeat(1,src_len,1)  # h = [batch_size,seq_length,dec_hidden_dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((h,encoder_outputs),dim=2)))  #[batch_size,seq_length,dec_hidden_dim]\n",
    "        attention_scores = self.v(energy).squeeze(2)  # attention_scores = [batch_size , seq_length ]\n",
    "        attention_scores = attention_scores.masked_fill(encoder_length_mask==1, -1e10)   # Fill padding tokens with a lower value\n",
    "        attention_scores = F.softmax(attention_scores,dim=1)\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src_lengths,max_src_length):\n",
    "\n",
    "        src_mask = torch.zeros((len(src_lengths),max_src_length),dtype=torch.int64)\n",
    "        for i in range(len(src_lengths)):\n",
    "\n",
    "            src_mask[i,src_lengths[i]:] = 1\n",
    "        return src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc = Encoder(30,30,15,10)\n",
    "\n",
    "inp = torch.randn((3,20,30))\n",
    "inp_len = [20 for i in range(3)]\n",
    "outputs, hidden = enc(inp,inp_len)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,enc_hidden_dim,dec_hidden_dim,dec_output_dim,emb_dim):\n",
    "        \n",
    "        # enc_hidden_dim = 2*ENCODER_HIDDEN_DIM or ENCODER_HIDDEN_DIM\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention = Attention(enc_hidden_dim,dec_hidden_dim)\n",
    "        self.embedding_layer = nn.Embedding(vocab_size,emb_dim)\n",
    "        self.rnn = nn.GRU(enc_hidden_dim + emb_dim,dec_hidden_dim,batch_first = True)\n",
    "\n",
    "        self.fc_out = nn.Linear(enc_hidden_dim + emb_dim + dec_hidden_dim , dec_output_dim)\n",
    "        \n",
    "        #self.fc_tilde = nn.Linear(enc_hidden_dim + emb_dim + dec_hidden_dim , dec_hidden_dim)\n",
    "        #self.max_out_layer = nn.MaxPool1d(kernel_size=2)\n",
    "        #self.W0 = nn.Linear(dec_hidden_dim>>1,emb_dim)\n",
    "        #self.prob_out = nn.Linear(emb_dim,vocab_size)\n",
    "        #self.prob_out.weight = self.embedding_layer.weight\n",
    "        \n",
    "\n",
    "    def forward(self,input,hidden,encoder_outputs,encoder_length_mask):\n",
    "            # encoder outputs =  batch_size , seq_len , encoder_output_dim\n",
    "            # hidden = batch_size , hidden_dim\n",
    "            # input = batch_size\n",
    "            \n",
    "            input = input.unsqueeze(0) # [1,batch_size]\n",
    "            embedded = self.embedding_layer(input) # [1,batch_size,embed_dim]\n",
    "\n",
    "            embedded = embedded.permute(1,0,2) #[ batch_size, seq_length=1, embed_dim ]\n",
    "\n",
    "            attention_vector = self.attention(hidden,encoder_outputs,encoder_length_mask) # [ batch_size , seq_length ]\n",
    "            attention_vector = attention_vector.unsqueeze(1) # [batch_size , 1 , seq_length ]\n",
    "\n",
    "            weighted = torch.bmm(attention_vector,encoder_outputs) # [ batch_size, 1, encoder_output_dim]\n",
    "            #weighted = weighted.permute(1,0,2) #[1 , batch_size , encoder_output_dim]\n",
    "\n",
    "\n",
    "            rnn_input = torch.cat((embedded,weighted),dim=2) #[batch_size, seq_length=1, encoder + decoder]\n",
    "\n",
    "            out,h = self.rnn(rnn_input,hidden.unsqueeze(0)) # consider only a single layer (1.) so unsqueeze(0)\n",
    "\n",
    "            # out = [batch_size, seq_length = 1, decoder_hidden_out (bidirectional)]\n",
    "            # hidden = [D*num_layers,batch_size, decoder_hidden_out]\n",
    "\n",
    "\n",
    "            embedded = embedded.squeeze(1)  # [batch_size,embed_dim]\n",
    "            out = out.squeeze(1)    #[batch_size, decoder_hidden_out] # Have to change if the number of layers is changed to more than 1\n",
    "            weighted = weighted.squeeze(1)  # [batch_size,encoder_output_dim] \n",
    "            prediction = self.fc_out(torch.cat([embedded,out,weighted],dim=1)) #[batch_size, decoder_output_dim]\n",
    "            #prediction = F.softmax(self.fc_out(torch.cat([embedded,out,weighted],dim=1)),dim=1) #[batch_size, decoder_output_dim]\n",
    "            #t_tilde =  self.fc_tilde(torch.cat([embedded,out,weighted],dim=1)) # [batch_size, decoder_hidden_dim]\n",
    "            #t_tilde = self.max_out_layer(t_tilde.unsqueeze(1)) #[batch_size,1,decoder_hidden_dim/2]\n",
    "            #t_tilde = t_tilde.squeeze(1)\n",
    "\n",
    "            #inter_step  = self.W0(t_tilde) # [ batch_size , dec_embeddim]\n",
    "            #prediction = self.prob_out(inter_step) #[  batch_size , vocab_size]\n",
    "            #prediction = F.softmax(prediction,dim=1)\n",
    "            return prediction, h.squeeze(0), attention_vector # Reduce the number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_out_layer = nn.MaxPool1d(2)\n",
    "output = torch.randn((3,1,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder()\n",
    "dec = Decoder(13,2*ENC_HIDDEN_DIM,DEC_HIDDEN_DIM,13,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "encoder_mask  = create_mask(batch_data['src_length'],max(batch_data['src_length']))\n",
    "enc_outputs , hidden = enc(batch_data['src'],batch_data['src_length'])\n",
    "\n",
    "predictions , hidden = dec(torch.randint(13,(BATCH_SIZE,)),hidden,enc_outputs,encoder_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                input_embed_dim = EMBED_DIM,\n",
    "                encoder_hidden_dim = ENC_HIDDEN_DIM,\n",
    "                encoder_hidden_output = ENC_OUTPUT_DIM,\n",
    "                enc_num_layers = 1,\n",
    "                enc_bidirectional = ENC_BIDIRECTIONAL,\n",
    "\n",
    "                dec_vocab_size = len(output_vocab),\n",
    "                dec_embed_dim = DEC_EMBED_DIM,\n",
    "                dec_hidden_dim  =DEC_HIDDEN_DIM,\n",
    "                device_train = device_cpu\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_embed_dim,encoder_hidden_dim,encoder_hidden_output,enc_num_layers,enc_bidirectional=enc_bidirectional)\n",
    "        enc_bidirectional_factor = 2 if enc_bidirectional else 1\n",
    "        self.decoder = Decoder(dec_vocab_size,enc_bidirectional_factor*encoder_hidden_dim,dec_hidden_dim=dec_hidden_dim,dec_output_dim=dec_vocab_size,emb_dim=dec_embed_dim)\n",
    "        self.device_train = device_train\n",
    "\n",
    "\n",
    "    def create_mask(self, src_lengths,max_src_length):\n",
    "\n",
    "        src_mask = torch.zeros((len(src_lengths),max_src_length),dtype=torch.int64)\n",
    "        for i in range(len(src_lengths)):\n",
    "\n",
    "            src_mask[i,src_lengths[i]:] = 1\n",
    "        return src_mask\n",
    "        \n",
    "    def forward(self,source,source_len,target,teacher_forcing_ratio = 0.0):\n",
    "        #   source = [batch_size, max_src_len]\n",
    "        #   source_len = [length of sentence in the batch]\n",
    "        #   target = [batch_size,traget_length]\n",
    "        #   teacher_forcing_ratio = probability to use teacher forcinbg\n",
    "\n",
    "        batch_size = source.shape[0]\n",
    "        target_length = target.shape[1]\n",
    "        target_vocab_size = self.decoder.vocab_size\n",
    "        outputs= torch.zeros(batch_size,target_length,target_vocab_size).to(self.device_train)\n",
    "        encoder_outputs , hidden = self.encoder(source,source_len)\n",
    "\n",
    "        inp = target[:,0]        \n",
    "        enc_mask = self.create_mask(source_len,int(encoder_outputs.shape[1]))\n",
    "        for t in range(1,target_length):\n",
    "            decoder_output, hidden, attention_vector =  self.decoder(inp,hidden,encoder_outputs,enc_mask)\n",
    "\n",
    "            outputs[:,t,:] = decoder_output # batch_size, vocab_size\n",
    "            teacher_force = random.random() < teacher_forcing_ratio \n",
    "\n",
    "            top1 = decoder_output.argmax(1)\n",
    "\n",
    "            inp = target[:,t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weights(model):\n",
    "    \n",
    "    for name,param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data,mean=0,std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TranslationModel()\n",
    "t.apply(apply_weights)\n",
    "batch_data = next(iter(train_dataloader))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=output_vocab['<eos>'])\n",
    "optimizer = optim.Adam(t.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_output = t(batch_data['src'],batch_data['src_length'],batch_data['trg'])\n",
    "\n",
    "\n",
    "model_out_reshaped = model_output[1:].view(-1,model_output.shape[-1])\n",
    "reshaped_target = batch_data['trg'][1:].view(-1)\n",
    "loss_value = criterion(model_out_reshaped,reshaped_target)\n",
    "loss_value.backward()\n",
    "nn.utils.clip_grad_norm_(t.parameters(),5)\n",
    "optimizer.step()\n",
    "print(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_data['trg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,num_epochs,train_loader,valid_loader,optimizer,criterion,checkpoint_name='translation_model.pth'):\n",
    "    \n",
    "\n",
    "    best_validation_loss = 1000.0\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        training_loss = 0.0\n",
    "        model.train()\n",
    "        for i,batch in enumerate(train_loader):\n",
    "\n",
    "            source, source_length, target = batch['src'], batch['src_length'], batch['trg']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # model_output = [batch_size, output_seq_length,vocab_size]\n",
    "            model_output  = model(source,source_length,target)\n",
    "            \n",
    "            model_out_reshaped = model_output[1:].view(-1,model_output.shape[-1])\n",
    "            reshaped_target = target[1:].view(-1)\n",
    "\n",
    "            loss_value = criterion(model_out_reshaped,reshaped_target)\n",
    "            loss_value.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(),5)\n",
    "\n",
    "            optimizer.step()\n",
    "            training_loss += loss_value.item()\n",
    "        \n",
    "        print(\"Epoch \" + str(e) + \" Training Loss Value = \" + str(training_loss/len(train_loader)))\n",
    "        \n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, batch in enumerate(valid_loader):\n",
    "                source, source_length, target = batch['src'], batch['src_length'], batch['trg']\n",
    "                model_output  = model(source,source_length,target,0)\n",
    "                model_out_reshaped = model_output[1:].view(-1,model_output.shape[-1])\n",
    "                reshaped_target = target[1:].view(-1)\n",
    "                loss_value = criterion(model_out_reshaped,reshaped_target)\n",
    "\n",
    "                validation_loss += loss_value.item()\n",
    "        averaged_validation_loss = validation_loss/len(valid_loader)\n",
    "        print(\"Epoch \" + str(e) + \"Validation Loss Value = \" + str(averaged_validation_loss))\n",
    "        print(\"\")\n",
    "        if (averaged_validation_loss <= best_validation_loss):\n",
    "            best_validation_loss = averaged_validation_loss\n",
    "            torch.save(model.state_dict(),checkpoint_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss Value = 0.5157774696350098\n",
      "Epoch 0Validation Loss Value = 0.26869440615177154\n",
      "\n",
      "Epoch 1 Training Loss Value = 0.3249284700155258\n",
      "Epoch 1Validation Loss Value = 0.2687932537794113\n",
      "\n",
      "Epoch 2 Training Loss Value = 0.2419094487130642\n",
      "Epoch 2Validation Loss Value = 0.2375465840101242\n",
      "\n",
      "Epoch 3 Training Loss Value = 1.3447464601397514\n",
      "Epoch 3Validation Loss Value = 2.0580299339294434\n",
      "\n",
      "Epoch 4 Training Loss Value = 0.9694382290840149\n",
      "Epoch 4Validation Loss Value = 0.4787767434120178\n",
      "\n",
      "Epoch 5 Training Loss Value = 0.3792950532436371\n",
      "Epoch 5Validation Loss Value = 0.3936197602748871\n",
      "\n",
      "Epoch 6 Training Loss Value = 0.3478185900449753\n",
      "Epoch 6Validation Loss Value = 0.2913986191749573\n",
      "\n",
      "Epoch 7 Training Loss Value = 0.5647434580326081\n",
      "Epoch 7Validation Loss Value = 0.3168922102451324\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/depressedcoder/DLNLP/Assignment4/parta/sol.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/depressedcoder/DLNLP/Assignment4/parta/sol.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m t \u001b[39m=\u001b[39m TranslationModel()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/depressedcoder/DLNLP/Assignment4/parta/sol.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m t\u001b[39m.\u001b[39mapply(apply_weights)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/depressedcoder/DLNLP/Assignment4/parta/sol.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_model(t,\u001b[39m50\u001b[39;49m,train_dataloader,valid_dataloader,optim\u001b[39m.\u001b[39;49mAdam(t\u001b[39m.\u001b[39;49mparameters(),lr\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m),nn\u001b[39m.\u001b[39;49mCrossEntropyLoss())\n",
      "\u001b[1;32m/Users/depressedcoder/DLNLP/Assignment4/parta/sol.ipynb Cell 25\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, train_loader, valid_loader, optimizer, criterion, checkpoint_name)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/depressedcoder/DLNLP/Assignment4/parta/sol.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m reshaped_target \u001b[39m=\u001b[39m target[\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/depressedcoder/DLNLP/Assignment4/parta/sol.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss_value \u001b[39m=\u001b[39m criterion(model_out_reshaped,reshaped_target)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/depressedcoder/DLNLP/Assignment4/parta/sol.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss_value\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/depressedcoder/DLNLP/Assignment4/parta/sol.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(model\u001b[39m.\u001b[39mparameters(),\u001b[39m5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/depressedcoder/DLNLP/Assignment4/parta/sol.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/environments/gymenv/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/environments/gymenv/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = TranslationModel()\n",
    "t.apply(apply_weights)\n",
    "train_model(t,50,train_dataloader,valid_dataloader,optim.Adam(t.parameters(),lr=0.01),nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1', '8', '6', '6', '-', '0', '5', '-', '2', '1'],\n",
       " tensor([[[4.6490e-03, 4.1233e-02, 5.2969e-03, 9.4882e-01]],\n",
       " \n",
       "         [[6.4269e-05, 2.1788e-05, 2.8742e-03, 9.9704e-01]],\n",
       " \n",
       "         [[5.6166e-04, 7.5312e-04, 1.1723e-03, 9.9751e-01]],\n",
       " \n",
       "         [[6.4920e-04, 2.1543e-03, 3.1524e-03, 9.9404e-01]],\n",
       " \n",
       "         [[1.5310e-01, 7.7365e-01, 4.0362e-02, 3.2885e-02]],\n",
       " \n",
       "         [[9.7709e-02, 8.6416e-01, 3.4081e-02, 4.0464e-03]],\n",
       " \n",
       "         [[8.7358e-03, 9.3524e-01, 5.3504e-02, 2.5185e-03]],\n",
       " \n",
       "         [[2.3795e-04, 3.7330e-02, 9.6050e-01, 1.9331e-03]],\n",
       " \n",
       "         [[4.6337e-04, 3.7043e-02, 9.6150e-01, 9.8926e-04]],\n",
       " \n",
       "         [[6.9467e-04, 6.7715e-03, 9.9171e-01, 8.1947e-04]]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def translate_sentence(sentence, model, max_len = 10,glove=glove):\n",
    "\n",
    "    model.eval()\n",
    "    nl_date = sentence.replace(\"\\'\",\"\").strip()\n",
    "    split_on_slash = nl_date.split(\"/\")\n",
    "\n",
    "\n",
    "    embeddings = []\n",
    "    for word in nl_date.split(' '):\n",
    "        embeddings.append(glove[word])\n",
    "        \n",
    "    current_inp_length = len(embeddings)\n",
    "    inp_embeddings = torch.stack(embeddings)\n",
    "    inp_embeddings = inp_embeddings.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        encoder_outputs, hidden = model.encoder(inp_embeddings, [current_inp_length])\n",
    "        mask = model.create_mask([current_inp_length],max([current_inp_length]))\n",
    "        attentions = torch.zeros(max_len, 1, inp_embeddings.shape[1])\n",
    "        trg_indexes = []\n",
    "        for i in range(max_len):\n",
    "\n",
    "            if i==0:\n",
    "                trg_tensor = torch.LongTensor([output_vocab['<sos>']])\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                # input,hidden,encoder_outputs,encoder_length_mask\n",
    "                output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
    "\n",
    "            attentions[i] = attention\n",
    "            \n",
    "            pred_token = output.argmax(1).item()\n",
    "        \n",
    "            trg_indexes.append(pred_token)\n",
    "            trg_tensor = torch.LongTensor([pred_token])\n",
    "\n",
    "            if pred_token == output_vocab['<eos>']:\n",
    "                break\n",
    "    \n",
    "    trg_tokens = [output_index_to_word[i] for i in trg_indexes]\n",
    "    return trg_tokens, attentions\n",
    "    #print(trg_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "    \n",
    "    plt.tick_params(labelsize=15)\n",
    "    \n",
    "    x_ticks = [''] +  [t for t in sentence.split(' ')]\n",
    "    y_ticks = [''] + translation\n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/_xvm618541nc11hb7mv_kpj40000gn/T/ipykernel_41915/2550528583.py:15: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "/var/folders/sm/_xvm618541nc11hb7mv_kpj40000gn/T/ipykernel_41915/2550528583.py:16: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(y_ticks)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_tokens , attention = translate_sentence('tuesday may 21 1867',t)\n",
    "display_attention('tuesday may 21 1867',output_tokens,attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(100,64, num_layers= 2 ,batch_first= True ,bidirectional=True)\n",
    "inp = torch.randn((2,8,100))\n",
    "outputs,hidden = rnn(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inq = torch.cat([hidden[-i , : , :] for i in range(4)],dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn(inp)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(10,30)\n",
    "\n",
    "embedding_layer(torch.tensor([[10,2,3],[4,5,6]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gymenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b0110cf1cb03549be737c5657a86ea4daeeb81469a7991ed915d907f3e629c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
