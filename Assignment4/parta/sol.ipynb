{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vivitsu329\\Documents\\environments\\nlpenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torchtext.vocab import GloVe\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import SubsetRandomSampler,DataLoader\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import fasttext\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EMBED_DIM = 300 \n",
    "ENC_BIDIRECTIONAL = True\n",
    "ENC_BIDIRECTIONAL_FACTOR = 2 if ENC_BIDIRECTIONAL else 1\n",
    "ENC_HIDDEN_DIM = 256\n",
    "DEC_HIDDEN_DIM  = 256\n",
    "ENC_OUTPUT_DIM =  DEC_HIDDEN_DIM\n",
    "DEC_EMBED_DIM = 300\n",
    "NUM_EPOCHS = 300\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device_cpu = torch.device('cpu')\n",
    "device_fast = torch.device('cpu')\n",
    "\n",
    "if torch.has_mps:\n",
    "    device_fast = torch.device('mps')\n",
    "elif torch.has_cuda:\n",
    "    device_fast = torch.device('cuda')\n",
    "\n",
    "\n",
    "output_index_to_word = {}\n",
    "output_vocab = {}\n",
    "\n",
    "for i in range(0,10):\n",
    "    output_vocab[str(i)] = i\n",
    "    output_index_to_word[i] = str(i)\n",
    "\n",
    "output_vocab['-'] = 10\n",
    "output_index_to_word[10] = '-'\n",
    "output_vocab['<sos>'] = 11\n",
    "output_index_to_word[11] = '<sos>'\n",
    "output_vocab['<eos>'] = 12\n",
    "output_index_to_word[12] = '<eos>'\n",
    "\n",
    "glove = GloVe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fasttext_model = fasttext.load_model('./crawl-300d-2M-subword/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_training_data(filename='./Assignment4aDataset.txt',glove=glove):\n",
    "    \n",
    "    f = open(filename,'r')\n",
    "    dataset = []\n",
    "    for line in f.readlines():       \n",
    "         \n",
    "        nl_date , out_date = line.split(',')\n",
    "        nl_date = nl_date.replace(\"\\'\",\"\").strip()\n",
    "        out_date = out_date.replace(\"\\'\",\"\").strip()\n",
    "\n",
    "        if \"/\" in nl_date:\n",
    "            #number_list = list(nl_date)\n",
    "            #nl_date = \" \".join(number_list)\n",
    "            split_on_slash = nl_date.split(\"/\")\n",
    "            nl_date = \" / \".join(split_on_slash)\n",
    "\n",
    "        nl_date = nl_date.lower()\n",
    "\n",
    "        embeddings = []\n",
    "        for word in nl_date.split(' '):\n",
    "            if word == '':\n",
    "                continue\n",
    "            embeddings.append(glove[word])\n",
    "            #embeddings.append(torch.tensor(fasttext_model.get_word_vector(word)))\n",
    "        current_inp_length = len(embeddings)\n",
    "        embeddings = torch.stack(embeddings)\n",
    "\n",
    "        target = []\n",
    "        target.append(output_vocab['<sos>'])\n",
    "\n",
    "        for character in list(out_date):\n",
    "            target.append(output_vocab[character])\n",
    "        \n",
    "        target.append(output_vocab['<eos>'])\n",
    "\n",
    "        dataset.append({'in' : embeddings,'in_length' : current_inp_length,'out' : target})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def collate_function(batch_data):\n",
    "    inputs = [b['in'] for b in batch_data]\n",
    "    in_lengths = [b['in_length'] for b in batch_data]\n",
    "    out = torch.tensor([b['out'] for b in batch_data])\n",
    "    inputs = pad_sequence(inputs,batch_first=True)\n",
    "    return {'src': inputs, 'src_length' : in_lengths, 'trg' : out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(get_training_data())\n",
    "\n",
    "train_idx,valid_idx = train_test_split(np.arange(len(train_dataset)), \n",
    "    test_size=0.2,\n",
    "    shuffle= True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_dataloader = DataLoader(train_dataset,BATCH_SIZE,sampler=train_sampler,collate_fn=collate_function)\n",
    "valid_dataloader = DataLoader(train_dataset,BATCH_SIZE,sampler=valid_sampler,collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim = EMBED_DIM,enc_hidden_dim = ENC_HIDDEN_DIM,enc_output_dim = ENC_OUTPUT_DIM,NUM_LAYERS=1,enc_bidirectional=ENC_BIDIRECTIONAL,dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        #self.embedding_layer = nn.Embedding(vocab_size,EMBED_DIM)\n",
    "        self.rnn = nn.GRU(embed_dim,enc_hidden_dim, num_layers = NUM_LAYERS ,batch_first= True ,bidirectional=enc_bidirectional)\n",
    "\n",
    "\n",
    "        # ENCODER_OUTPUT_DIM = DECODER_HIDDEN_SIZE\n",
    "        self.fc = nn.Linear(2*enc_hidden_dim,enc_output_dim) \n",
    "\n",
    "        self.fc_out = nn.Linear(enc_output_dim,1)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inp,inp_len):\n",
    "        \n",
    "        #embedded_input = self.embedding_layer(inp)\n",
    "        embedded_input = inp   # [batch_size, input_seq_length, embed_dim ]\n",
    "        \n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(embedded_input,inp_len,batch_first=True,enforce_sorted=False)\n",
    "        packed_output , hidden = self.rnn(packed_embedding)  # hidden = [D*num_layers, batch_size , hidden_dim ]\n",
    "        outputs, _  = nn.utils.rnn.pad_packed_sequence(packed_output,batch_first=True)  # [batch_size, inp_seq_length, hidden_dim]\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)))  # [batch_size, decoder_hidden_size]\n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear(enc_hidden_dim+dec_hidden_dim,dec_hidden_dim)\n",
    "        self.v = nn.Linear(dec_hidden_dim,1)\n",
    "\n",
    "    def forward(self,hidden,encoder_outputs, encoder_length_mask):\n",
    "        \n",
    "        # encoder_outputs = [batch_size,seq_length, enc_hidden_dim][2*ENCODER_HIDDEN_DIM or ENCODER_HIDDEN_DIM]\n",
    "        # hidden = [batch_size,  dec_hidden_dim]\n",
    "        # encoder_length_mask = [batch_size, seq_length]\n",
    "\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        h = hidden.unsqueeze(1).repeat(1,src_len,1)  # h = [batch_size,seq_length,dec_hidden_dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((h,encoder_outputs),dim=2)))  #[batch_size,seq_length,dec_hidden_dim]\n",
    "        attention_scores = self.v(energy).squeeze(2)  # attention_scores = [batch_size , seq_length ]\n",
    "        attention_scores = attention_scores.masked_fill(encoder_length_mask==1, -1e10)   # Fill padding tokens with a lower value\n",
    "        attention_scores = F.softmax(attention_scores,dim=1)\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src_lengths,max_src_length):\n",
    "\n",
    "        src_mask = torch.zeros((len(src_lengths),max_src_length),dtype=torch.int64)\n",
    "        for i in range(len(src_lengths)):\n",
    "\n",
    "            src_mask[i,src_lengths[i]:] = 1\n",
    "        return src_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc = Encoder(30,30,15,10)\n",
    "\n",
    "inp = torch.randn((3,20,30))\n",
    "inp_len = [20 for i in range(3)]\n",
    "outputs, hidden = enc(inp,inp_len)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,enc_hidden_dim,dec_hidden_dim,dec_output_dim,emb_dim):\n",
    "        \n",
    "        # enc_hidden_dim = 2*ENCODER_HIDDEN_DIM or ENCODER_HIDDEN_DIM\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention = Attention(enc_hidden_dim,dec_hidden_dim)\n",
    "        self.embedding_layer = nn.Embedding(vocab_size,emb_dim)\n",
    "        self.rnn = nn.GRU(enc_hidden_dim + emb_dim,dec_hidden_dim,batch_first = True)\n",
    "\n",
    "        self.fc_out = nn.Linear(enc_hidden_dim + emb_dim + dec_hidden_dim , dec_output_dim)\n",
    "        \n",
    "        #self.fc_tilde = nn.Linear(enc_hidden_dim + emb_dim + dec_hidden_dim , dec_hidden_dim)\n",
    "        #self.max_out_layer = nn.MaxPool1d(kernel_size=2)\n",
    "        #self.W0 = nn.Linear(dec_hidden_dim>>1,emb_dim)\n",
    "        #self.prob_out = nn.Linear(emb_dim,vocab_size)\n",
    "        #self.prob_out.weight = self.embedding_layer.weight\n",
    "        \n",
    "\n",
    "    def forward(self,input,hidden,encoder_outputs,encoder_length_mask):\n",
    "            # encoder outputs =  batch_size , seq_len , encoder_output_dim\n",
    "            # hidden = batch_size , hidden_dim\n",
    "            # input = batch_size\n",
    "            \n",
    "            input = input.unsqueeze(0) # [1,batch_size]\n",
    "            embedded = self.embedding_layer(input) # [1,batch_size,embed_dim]\n",
    "\n",
    "            embedded = embedded.permute(1,0,2) #[ batch_size, seq_length=1, embed_dim ]\n",
    "\n",
    "            attention_vector = self.attention(hidden,encoder_outputs,encoder_length_mask) # [ batch_size , seq_length ]\n",
    "            attention_vector = attention_vector.unsqueeze(1) # [batch_size , 1 , seq_length ]\n",
    "\n",
    "            weighted = torch.bmm(attention_vector,encoder_outputs) # [ batch_size, 1, encoder_output_dim]\n",
    "            #weighted = weighted.permute(1,0,2) #[1 , batch_size , encoder_output_dim]\n",
    "\n",
    "\n",
    "            rnn_input = torch.cat((embedded,weighted),dim=2) #[batch_size, seq_length=1, encoder + decoder]\n",
    "\n",
    "            out,h = self.rnn(rnn_input,hidden.unsqueeze(0)) # consider only a single layer (1.) so unsqueeze(0)\n",
    "\n",
    "            # out = [batch_size, seq_length = 1, decoder_hidden_out (bidirectional)]\n",
    "            # hidden = [D*num_layers,batch_size, decoder_hidden_out]\n",
    "\n",
    "\n",
    "            embedded = embedded.squeeze(1)  # [batch_size,embed_dim]\n",
    "            out = out.squeeze(1)    #[batch_size, decoder_hidden_out] # Have to change if the number of layers is changed to more than 1\n",
    "            weighted = weighted.squeeze(1)  # [batch_size,encoder_output_dim] \n",
    "            prediction = self.fc_out(torch.cat([embedded,out,weighted],dim=1)) #[batch_size, decoder_output_dim]\n",
    "            #prediction = F.softmax(self.fc_out(torch.cat([embedded,out,weighted],dim=1)),dim=1) #[batch_size, decoder_output_dim]\n",
    "            #t_tilde =  self.fc_tilde(torch.cat([embedded,out,weighted],dim=1)) # [batch_size, decoder_hidden_dim]\n",
    "            #t_tilde = self.max_out_layer(t_tilde.unsqueeze(1)) #[batch_size,1,decoder_hidden_dim/2]\n",
    "            #t_tilde = t_tilde.squeeze(1)\n",
    "\n",
    "            #inter_step  = self.W0(t_tilde) # [ batch_size , dec_embeddim]\n",
    "            #prediction = self.prob_out(inter_step) #[  batch_size , vocab_size]\n",
    "            #prediction = F.softmax(prediction,dim=1)\n",
    "            return prediction, h.squeeze(0), attention_vector # Reduce the number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_out_layer = nn.MaxPool1d(2)\n",
    "output = torch.randn((3,1,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder()\n",
    "dec = Decoder(13,2*ENC_HIDDEN_DIM,DEC_HIDDEN_DIM,13,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "encoder_mask  = create_mask(batch_data['src_length'],max(batch_data['src_length']))\n",
    "enc_outputs , hidden = enc(batch_data['src'],batch_data['src_length'])\n",
    "\n",
    "predictions , hidden = dec(torch.randint(13,(BATCH_SIZE,)),hidden,enc_outputs,encoder_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                input_embed_dim = EMBED_DIM,\n",
    "                encoder_hidden_dim = ENC_HIDDEN_DIM,\n",
    "                encoder_hidden_output = ENC_OUTPUT_DIM,\n",
    "                enc_num_layers = 1,\n",
    "                enc_bidirectional = ENC_BIDIRECTIONAL,\n",
    "\n",
    "                dec_vocab_size = len(output_vocab),\n",
    "                dec_embed_dim = DEC_EMBED_DIM,\n",
    "                dec_hidden_dim  =DEC_HIDDEN_DIM,\n",
    "                device_train = device_cpu\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_embed_dim,encoder_hidden_dim,encoder_hidden_output,enc_num_layers,enc_bidirectional=enc_bidirectional)\n",
    "        enc_bidirectional_factor = 2 if enc_bidirectional else 1\n",
    "        self.decoder = Decoder(dec_vocab_size,enc_bidirectional_factor*encoder_hidden_dim,dec_hidden_dim=dec_hidden_dim,dec_output_dim=dec_vocab_size,emb_dim=dec_embed_dim)\n",
    "        self.device_train = device_train\n",
    "\n",
    "\n",
    "    def create_mask(self, src_lengths,max_src_length):\n",
    "\n",
    "        src_mask = torch.zeros((len(src_lengths),max_src_length),dtype=torch.int64)\n",
    "        for i in range(len(src_lengths)):\n",
    "\n",
    "            src_mask[i,src_lengths[i]:] = 1\n",
    "        return src_mask\n",
    "        \n",
    "    def forward(self,source,source_len,target,teacher_forcing_ratio = 0.0):\n",
    "        #   source = [batch_size, max_src_len]\n",
    "        #   source_len = [length of sentence in the batch]\n",
    "        #   target = [batch_size,traget_length]\n",
    "        #   teacher_forcing_ratio = probability to use teacher forcinbg\n",
    "\n",
    "        batch_size = source.shape[0]\n",
    "        target_length = target.shape[1]\n",
    "        target_vocab_size = self.decoder.vocab_size\n",
    "        outputs= torch.zeros(batch_size,target_length,target_vocab_size).to(self.device_train)\n",
    "        encoder_outputs , hidden = self.encoder(source,source_len)\n",
    "\n",
    "        inp = target[:,0]        \n",
    "        enc_mask = self.create_mask(source_len,int(encoder_outputs.shape[1]))\n",
    "        enc_mask = enc_mask.to(self.device_train)\n",
    "        for t in range(1,target_length):\n",
    "            decoder_output, hidden, attention_vector =  self.decoder(inp,hidden,encoder_outputs,enc_mask)\n",
    "\n",
    "            outputs[:,t,:] = decoder_output # batch_size, vocab_size\n",
    "            teacher_force = random.random() < teacher_forcing_ratio \n",
    "\n",
    "            top1 = decoder_output.argmax(1)\n",
    "\n",
    "            inp = target[:,t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weights(model : TranslationModel):\n",
    "    \n",
    "    for name,param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data,mean=0,std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data,0)\n",
    "    \n",
    "    \n",
    "    nn.init.xavier_uniform_(model.encoder.fc_out.weight)\n",
    "    nn.init.normal_(model.decoder.attention.attn.weight, mean=0, std=0.001)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TranslationModel()\n",
    "apply_weights(t)\n",
    "batch_data = next(iter(train_dataloader))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=output_vocab['<eos>'])\n",
    "optimizer = optim.Adam(t.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5649495124816895\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_output = t(batch_data['src'],batch_data['src_length'],batch_data['trg'])\n",
    "\n",
    "\n",
    "model_out_reshaped = model_output[1:].view(-1,model_output.shape[-1])\n",
    "reshaped_target = batch_data['trg'][1:].view(-1)\n",
    "loss_value = criterion(model_out_reshaped,reshaped_target)\n",
    "loss_value.backward()\n",
    "nn.utils.clip_grad_norm_(t.parameters(),5)\n",
    "optimizer.step()\n",
    "print(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11,  1,  7,  ...,  1,  7, 12],\n",
      "        [11,  1,  8,  ...,  0,  4, 12],\n",
      "        [11,  1,  8,  ...,  2,  5, 12],\n",
      "        ...,\n",
      "        [11,  1,  8,  ...,  2,  3, 12],\n",
      "        [11,  1,  5,  ...,  1,  1, 12],\n",
      "        [11,  2,  0,  ...,  2,  1, 12]])\n"
     ]
    }
   ],
   "source": [
    "print(batch_data['trg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,num_epochs,train_loader,valid_loader,optimizer,criterion,checkpoint_name='translation_model.pth',device_train = device_cpu):\n",
    "    \n",
    "\n",
    "    best_validation_loss = 1000.0\n",
    "    model = model.to(device_train)\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        training_loss = 0.0\n",
    "        model.train()\n",
    "        for i,batch in enumerate(train_loader):\n",
    "\n",
    "            source, source_length, target = batch['src'], batch['src_length'], batch['trg']\n",
    "\n",
    "            source = source.to(device_train)\n",
    "            target = target.to(device_train)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # model_output = [batch_size, output_seq_length,vocab_size]\n",
    "            model_output  = model(source,source_length,target)\n",
    "\n",
    "\n",
    "            source = source.to(device_cpu)\n",
    "            target = target.to(device_cpu)\n",
    "            model_output = model_output.to(device_cpu)\n",
    "            \n",
    "            model_out_reshaped = model_output[1:].view(-1,model_output.shape[-1]) \n",
    "            #print(model_out_reshaped.shape)\n",
    "            reshaped_target = target[1:].view(-1)\n",
    "            #print(reshaped_target.shape)\n",
    "\n",
    "            loss_value = criterion(model_out_reshaped,reshaped_target)\n",
    "            loss_value.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(),5)\n",
    "\n",
    "            optimizer.step()\n",
    "            training_loss += loss_value.item()\n",
    "        \n",
    "        print(\"Epoch \" + str(e) + \" Training Loss Value = \" + str(training_loss/len(train_loader)))\n",
    "        \n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, batch in enumerate(valid_loader):\n",
    "                source, source_length, target = batch['src'], batch['src_length'], batch['trg']\n",
    "\n",
    "                source = source.to(device_train)\n",
    "                target = target.to(device_train)\n",
    "\n",
    "                model_output  = model(source,source_length,target,0)\n",
    "\n",
    "\n",
    "                source = source.to(device_cpu)\n",
    "                target = target.to(device_cpu)\n",
    "                model_output = model_output.to(device_cpu)\n",
    "                \n",
    "                model_out_reshaped = model_output[1:].view(-1,model_output.shape[-1])\n",
    "                reshaped_target = target[1:].view(-1)\n",
    "                loss_value = criterion(model_out_reshaped,reshaped_target)\n",
    "\n",
    "                validation_loss += loss_value.item()\n",
    "        averaged_validation_loss = validation_loss/len(valid_loader)\n",
    "        print(\"Epoch \" + str(e) + \" Validation Loss Value = \" + str(averaged_validation_loss))\n",
    "        print(\"\")\n",
    "        if (averaged_validation_loss <= best_validation_loss):\n",
    "            best_validation_loss = averaged_validation_loss\n",
    "            torch.save(model.state_dict(),checkpoint_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss Value = 1.1455968573093414\n",
      "Epoch 0 Validation Loss Value = 0.5464911309499589\n",
      "\n",
      "Epoch 1 Training Loss Value = 0.4253791055679321\n",
      "Epoch 1 Validation Loss Value = 0.34214436913293506\n",
      "\n",
      "Epoch 2 Training Loss Value = 0.3145783317089081\n",
      "Epoch 2 Validation Loss Value = 0.27041108031121514\n",
      "\n",
      "Epoch 3 Training Loss Value = 0.2533230217695236\n",
      "Epoch 3 Validation Loss Value = 0.2419532821291969\n",
      "\n",
      "Epoch 4 Training Loss Value = 0.2353003345131874\n",
      "Epoch 4 Validation Loss Value = 0.23174274227921926\n",
      "\n",
      "Epoch 5 Training Loss Value = 0.22922731286287307\n",
      "Epoch 5 Validation Loss Value = 0.23138246909966545\n",
      "\n",
      "Epoch 6 Training Loss Value = 0.22792264240980148\n",
      "Epoch 6 Validation Loss Value = 0.22640479320571535\n",
      "\n",
      "Epoch 7 Training Loss Value = 0.22628340327739715\n",
      "Epoch 7 Validation Loss Value = 0.22687898552606975\n",
      "\n",
      "Epoch 8 Training Loss Value = 0.22646221441030503\n",
      "Epoch 8 Validation Loss Value = 0.23942398291731637\n",
      "\n",
      "Epoch 9 Training Loss Value = 0.2409355462193489\n",
      "Epoch 9 Validation Loss Value = 0.2362002321178951\n",
      "\n",
      "Epoch 10 Training Loss Value = 0.22548339891433716\n",
      "Epoch 10 Validation Loss Value = 0.2252745358716874\n",
      "\n",
      "Epoch 11 Training Loss Value = 0.22330169188976287\n",
      "Epoch 11 Validation Loss Value = 0.22373684034461067\n",
      "\n",
      "Epoch 12 Training Loss Value = 0.22281725639104843\n",
      "Epoch 12 Validation Loss Value = 0.22495055435195802\n",
      "\n",
      "Epoch 13 Training Loss Value = 0.22447588640451432\n",
      "Epoch 13 Validation Loss Value = 0.2267016340342779\n",
      "\n",
      "Epoch 14 Training Loss Value = 0.22435165798664092\n",
      "Epoch 14 Validation Loss Value = 0.22335886789692772\n",
      "\n",
      "Epoch 15 Training Loss Value = 0.22192653799057008\n",
      "Epoch 15 Validation Loss Value = 0.22333856825790707\n",
      "\n",
      "Epoch 16 Training Loss Value = 0.2218525528907776\n",
      "Epoch 16 Validation Loss Value = 0.22304429728833455\n",
      "\n",
      "Epoch 17 Training Loss Value = 0.2222439883351326\n",
      "Epoch 17 Validation Loss Value = 0.22427576165350657\n",
      "\n",
      "Epoch 18 Training Loss Value = 0.22737710613012313\n",
      "Epoch 18 Validation Loss Value = 0.22486871291720678\n",
      "\n",
      "Epoch 19 Training Loss Value = 0.22169600307941437\n",
      "Epoch 19 Validation Loss Value = 0.2228999412249005\n",
      "\n",
      "Epoch 20 Training Loss Value = 0.22216321557760238\n",
      "Epoch 20 Validation Loss Value = 0.22365961661414493\n",
      "\n",
      "Epoch 21 Training Loss Value = 0.22185617297887802\n",
      "Epoch 21 Validation Loss Value = 0.22393150769528888\n",
      "\n",
      "Epoch 22 Training Loss Value = 0.22176094913482666\n",
      "Epoch 22 Validation Loss Value = 0.2230802895057769\n",
      "\n",
      "Epoch 23 Training Loss Value = 0.22123882567882538\n",
      "Epoch 23 Validation Loss Value = 0.22359966238339743\n",
      "\n",
      "Epoch 24 Training Loss Value = 0.22091324079036712\n",
      "Epoch 24 Validation Loss Value = 0.2244281688379863\n",
      "\n",
      "Epoch 25 Training Loss Value = 0.2207493370771408\n",
      "Epoch 25 Validation Loss Value = 0.22354070085381705\n",
      "\n",
      "Epoch 26 Training Loss Value = 0.22497779321670533\n",
      "Epoch 26 Validation Loss Value = 0.2325617422660192\n",
      "\n",
      "Epoch 27 Training Loss Value = 0.22217827206850052\n",
      "Epoch 27 Validation Loss Value = 0.22388346233065165\n",
      "\n",
      "Epoch 28 Training Loss Value = 0.220221217751503\n",
      "Epoch 28 Validation Loss Value = 0.22312761653983404\n",
      "\n",
      "Epoch 29 Training Loss Value = 0.21989872866868973\n",
      "Epoch 29 Validation Loss Value = 0.22310090987455278\n",
      "\n",
      "Epoch 30 Training Loss Value = 0.21988166588544847\n",
      "Epoch 30 Validation Loss Value = 0.223367774533847\n",
      "\n",
      "Epoch 31 Training Loss Value = 0.21956173169612886\n",
      "Epoch 31 Validation Loss Value = 0.2229418307542801\n",
      "\n",
      "Epoch 32 Training Loss Value = 0.2276402209997177\n",
      "Epoch 32 Validation Loss Value = 0.22766253399470496\n",
      "\n",
      "Epoch 33 Training Loss Value = 0.221442972779274\n",
      "Epoch 33 Validation Loss Value = 0.22402961788669465\n",
      "\n",
      "Epoch 34 Training Loss Value = 0.21996182644367218\n",
      "Epoch 34 Validation Loss Value = 0.2252045347103997\n",
      "\n",
      "Epoch 35 Training Loss Value = 0.21973408091068267\n",
      "Epoch 35 Validation Loss Value = 0.22456326087315878\n",
      "\n",
      "Epoch 36 Training Loss Value = 0.21933018839359283\n",
      "Epoch 36 Validation Loss Value = 0.22367261610333883\n",
      "\n",
      "Epoch 37 Training Loss Value = 0.2193356213569641\n",
      "Epoch 37 Validation Loss Value = 0.22423152909392402\n",
      "\n",
      "Epoch 38 Training Loss Value = 0.22072694849967955\n",
      "Epoch 38 Validation Loss Value = 0.2406405212860259\n",
      "\n",
      "Epoch 39 Training Loss Value = 0.22158426237106324\n",
      "Epoch 39 Validation Loss Value = 0.22644154112490397\n",
      "\n",
      "Epoch 40 Training Loss Value = 0.21909511721134187\n",
      "Epoch 40 Validation Loss Value = 0.22502784099843767\n",
      "\n",
      "Epoch 41 Training Loss Value = 0.2200322625041008\n",
      "Epoch 41 Validation Loss Value = 0.2248764785509261\n",
      "\n",
      "Epoch 42 Training Loss Value = 0.21964701861143113\n",
      "Epoch 42 Validation Loss Value = 0.23356165110118807\n",
      "\n",
      "Epoch 43 Training Loss Value = 0.21891872239112853\n",
      "Epoch 43 Validation Loss Value = 0.22635543819457765\n",
      "\n",
      "Epoch 44 Training Loss Value = 0.21805468314886092\n",
      "Epoch 44 Validation Loss Value = 0.22563348970715963\n",
      "\n",
      "Epoch 45 Training Loss Value = 0.21769709807634355\n",
      "Epoch 45 Validation Loss Value = 0.22590266617517624\n",
      "\n",
      "Epoch 46 Training Loss Value = 0.21733229064941406\n",
      "Epoch 46 Validation Loss Value = 0.22669961173383016\n",
      "\n",
      "Epoch 47 Training Loss Value = 0.21789597171545028\n",
      "Epoch 47 Validation Loss Value = 0.22779046022702779\n",
      "\n",
      "Epoch 48 Training Loss Value = 0.22452679705619813\n",
      "Epoch 48 Validation Loss Value = 0.2273015897898447\n",
      "\n",
      "Epoch 49 Training Loss Value = 0.21710469859838485\n",
      "Epoch 49 Validation Loss Value = 0.22734847310043516\n",
      "\n",
      "Epoch 50 Training Loss Value = 0.21650847470760345\n",
      "Epoch 50 Validation Loss Value = 0.22796652383274502\n",
      "\n",
      "Epoch 51 Training Loss Value = 0.21685255706310272\n",
      "Epoch 51 Validation Loss Value = 0.2282092154972137\n",
      "\n",
      "Epoch 52 Training Loss Value = 0.21607538574934007\n",
      "Epoch 52 Validation Loss Value = 0.22832743042991274\n",
      "\n",
      "Epoch 53 Training Loss Value = 0.2160107445716858\n",
      "Epoch 53 Validation Loss Value = 0.22930141384639438\n",
      "\n",
      "Epoch 54 Training Loss Value = 0.21776865941286086\n",
      "Epoch 54 Validation Loss Value = 0.23058801652893188\n",
      "\n",
      "Epoch 55 Training Loss Value = 0.21685073095560073\n",
      "Epoch 55 Validation Loss Value = 0.22950016719008248\n",
      "\n",
      "Epoch 56 Training Loss Value = 0.2154549959897995\n",
      "Epoch 56 Validation Loss Value = 0.23009105830911605\n",
      "\n",
      "Epoch 57 Training Loss Value = 0.21514767944812774\n",
      "Epoch 57 Validation Loss Value = 0.23124417236873082\n",
      "\n",
      "Epoch 58 Training Loss Value = 0.21512872791290283\n",
      "Epoch 58 Validation Loss Value = 0.23021767182009562\n",
      "\n",
      "Epoch 59 Training Loss Value = 0.21519405549764634\n",
      "Epoch 59 Validation Loss Value = 0.23129881918430328\n",
      "\n",
      "Epoch 60 Training Loss Value = 0.21567802250385285\n",
      "Epoch 60 Validation Loss Value = 0.23132020613503834\n",
      "\n",
      "Epoch 61 Training Loss Value = 0.22217959892749786\n",
      "Epoch 61 Validation Loss Value = 0.23082132495584942\n",
      "\n",
      "Epoch 62 Training Loss Value = 0.2169290720820427\n",
      "Epoch 62 Validation Loss Value = 0.2307956244737383\n",
      "\n",
      "Epoch 63 Training Loss Value = 0.21492197805643082\n",
      "Epoch 63 Validation Loss Value = 0.2322278916835785\n",
      "\n",
      "Epoch 64 Training Loss Value = 0.21458613073825836\n",
      "Epoch 64 Validation Loss Value = 0.2327663472720555\n",
      "\n",
      "Epoch 65 Training Loss Value = 0.2145472300052643\n",
      "Epoch 65 Validation Loss Value = 0.23235318845226652\n",
      "\n",
      "Epoch 66 Training Loss Value = 0.21439236265420913\n",
      "Epoch 66 Validation Loss Value = 0.23385995862975953\n",
      "\n",
      "Epoch 67 Training Loss Value = 0.21489302772283553\n",
      "Epoch 67 Validation Loss Value = 0.23165192632448106\n",
      "\n",
      "Epoch 68 Training Loss Value = 0.21712784600257873\n",
      "Epoch 68 Validation Loss Value = 0.24052913439652276\n",
      "\n",
      "Epoch 69 Training Loss Value = 0.21872901159524918\n",
      "Epoch 69 Validation Loss Value = 0.2324139629564588\n",
      "\n",
      "Epoch 70 Training Loss Value = 0.21497715854644775\n",
      "Epoch 70 Validation Loss Value = 0.23212770313497574\n",
      "\n",
      "Epoch 71 Training Loss Value = 0.2151109430193901\n",
      "Epoch 71 Validation Loss Value = 0.2331233580434133\n",
      "\n",
      "Epoch 72 Training Loss Value = 0.21447508603334428\n",
      "Epoch 72 Validation Loss Value = 0.23381726703946554\n",
      "\n",
      "Epoch 73 Training Loss Value = 0.2147401221394539\n",
      "Epoch 73 Validation Loss Value = 0.233404822292782\n",
      "\n",
      "Epoch 74 Training Loss Value = 0.21472973078489305\n",
      "Epoch 74 Validation Loss Value = 0.23364775474109348\n",
      "\n",
      "Epoch 75 Training Loss Value = 0.21691353452205658\n",
      "Epoch 75 Validation Loss Value = 0.23427970021490066\n",
      "\n",
      "Epoch 76 Training Loss Value = 0.2148755502104759\n",
      "Epoch 76 Validation Loss Value = 0.23449068362750705\n",
      "\n",
      "Epoch 77 Training Loss Value = 0.21486992859840393\n",
      "Epoch 77 Validation Loss Value = 0.23480597091099573\n",
      "\n",
      "Epoch 78 Training Loss Value = 0.21458234274387358\n",
      "Epoch 78 Validation Loss Value = 0.23601030830353026\n",
      "\n",
      "Epoch 79 Training Loss Value = 0.2144600465297699\n",
      "Epoch 79 Validation Loss Value = 0.23527496414525167\n",
      "\n",
      "Epoch 80 Training Loss Value = 0.21438479322195053\n",
      "Epoch 80 Validation Loss Value = 0.2367868572473526\n",
      "\n",
      "Epoch 81 Training Loss Value = 0.21672816914319992\n",
      "Epoch 81 Validation Loss Value = 0.2338533013586014\n",
      "\n",
      "Epoch 82 Training Loss Value = 0.21540562903881072\n",
      "Epoch 82 Validation Loss Value = 0.2360027139149015\n",
      "\n",
      "Epoch 83 Training Loss Value = 0.2149293732047081\n",
      "Epoch 83 Validation Loss Value = 0.23538685530897172\n",
      "\n",
      "Epoch 84 Training Loss Value = 0.21456587463617324\n",
      "Epoch 84 Validation Loss Value = 0.23473883952413285\n",
      "\n",
      "Epoch 85 Training Loss Value = 0.21461568564176559\n",
      "Epoch 85 Validation Loss Value = 0.23608570486780198\n",
      "\n",
      "Epoch 86 Training Loss Value = 0.21468499153852463\n",
      "Epoch 86 Validation Loss Value = 0.23661342547053382\n",
      "\n",
      "Epoch 87 Training Loss Value = 0.22217357414960862\n",
      "Epoch 87 Validation Loss Value = 0.23236306792213804\n",
      "\n",
      "Epoch 88 Training Loss Value = 0.21563946622610092\n",
      "Epoch 88 Validation Loss Value = 0.23443504552992564\n",
      "\n",
      "Epoch 89 Training Loss Value = 0.21455844694375992\n",
      "Epoch 89 Validation Loss Value = 0.2346456509733957\n",
      "\n",
      "Epoch 90 Training Loss Value = 0.2141369515657425\n",
      "Epoch 90 Validation Loss Value = 0.23496805060477483\n",
      "\n",
      "Epoch 91 Training Loss Value = 0.21403721183538438\n",
      "Epoch 91 Validation Loss Value = 0.23660645598456972\n",
      "\n",
      "Epoch 92 Training Loss Value = 0.21401072657108308\n",
      "Epoch 92 Validation Loss Value = 0.23539724260095565\n",
      "\n",
      "Epoch 93 Training Loss Value = 0.21400149089097978\n",
      "Epoch 93 Validation Loss Value = 0.23672630223962995\n",
      "\n",
      "Epoch 94 Training Loss Value = 0.21407670587301253\n",
      "Epoch 94 Validation Loss Value = 0.2365838580188297\n",
      "\n",
      "Epoch 95 Training Loss Value = 0.21494374853372575\n",
      "Epoch 95 Validation Loss Value = 0.23640854538433134\n",
      "\n",
      "Epoch 96 Training Loss Value = 0.2190346847176552\n",
      "Epoch 96 Validation Loss Value = 0.2387526666834241\n",
      "\n",
      "Epoch 97 Training Loss Value = 0.21538192480802537\n",
      "Epoch 97 Validation Loss Value = 0.23632196538032046\n",
      "\n",
      "Epoch 98 Training Loss Value = 0.21465310561656953\n",
      "Epoch 98 Validation Loss Value = 0.23732598409766242\n",
      "\n",
      "Epoch 99 Training Loss Value = 0.2144436758160591\n",
      "Epoch 99 Validation Loss Value = 0.2358944451525098\n",
      "\n",
      "Epoch 100 Training Loss Value = 0.21426072430610657\n",
      "Epoch 100 Validation Loss Value = 0.23635570444757975\n",
      "\n",
      "Epoch 101 Training Loss Value = 0.21419576072692872\n",
      "Epoch 101 Validation Loss Value = 0.23705185973455037\n",
      "\n",
      "Epoch 102 Training Loss Value = 0.21429354727268218\n",
      "Epoch 102 Validation Loss Value = 0.23665277659893036\n",
      "\n",
      "Epoch 103 Training Loss Value = 0.21494871014356612\n",
      "Epoch 103 Validation Loss Value = 0.24011519740498255\n",
      "\n",
      "Epoch 104 Training Loss Value = 0.21881024080514908\n",
      "Epoch 104 Validation Loss Value = 0.23560727801587847\n",
      "\n",
      "Epoch 105 Training Loss Value = 0.2151006833910942\n",
      "Epoch 105 Validation Loss Value = 0.23679649569685496\n",
      "\n",
      "Epoch 106 Training Loss Value = 0.2145277366042137\n",
      "Epoch 106 Validation Loss Value = 0.2364544861373447\n",
      "\n",
      "Epoch 107 Training Loss Value = 0.21912788408994674\n",
      "Epoch 107 Validation Loss Value = 0.2340567357956417\n",
      "\n",
      "Epoch 108 Training Loss Value = 0.21636017030477525\n",
      "Epoch 108 Validation Loss Value = 0.23524212056682223\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vivitsu329\\Documents\\GitHub\\Deep_Learning_For_NLP_IISC\\Assignment4\\parta\\sol.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment4/parta/sol.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m t \u001b[39m=\u001b[39m TranslationModel(device_train\u001b[39m=\u001b[39mdevice_fast)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment4/parta/sol.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m apply_weights(t)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment4/parta/sol.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_model(t,\u001b[39m150\u001b[39;49m,train_dataloader,valid_dataloader,optim\u001b[39m.\u001b[39;49mAdam(t\u001b[39m.\u001b[39;49mparameters(),lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m),nn\u001b[39m.\u001b[39;49mCrossEntropyLoss(),device_train\u001b[39m=\u001b[39;49m device_fast)\n",
      "\u001b[1;32mc:\\Users\\Vivitsu329\\Documents\\GitHub\\Deep_Learning_For_NLP_IISC\\Assignment4\\parta\\sol.ipynb Cell 22\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, num_epochs, train_loader, valid_loader, optimizer, criterion, checkpoint_name, device_train)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment4/parta/sol.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i,batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment4/parta/sol.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     source, source_length, target \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39msrc_length\u001b[39m\u001b[39m'\u001b[39m], batch[\u001b[39m'\u001b[39m\u001b[39mtrg\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment4/parta/sol.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     source \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39;49mto(device_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment4/parta/sol.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(device_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment4/parta/sol.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = TranslationModel(device_train=device_fast)\n",
    "apply_weights(t)\n",
    "train_model(t,150,train_dataloader,valid_dataloader,optim.Adam(t.parameters(),lr=0.001),nn.CrossEntropyLoss(),device_train= device_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TranslationModel()\n",
    "t.load_state_dict(torch.load('./translation_model.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, max_len = 10,glove=glove):\n",
    "\n",
    "    model.eval()\n",
    "    nl_date = sentence.replace(\"\\'\",\"\").strip()\n",
    "    \n",
    "    nl_date = nl_date.lower()\n",
    "\n",
    "    if \"/\" in nl_date:\n",
    "        #number_list = list(nl_date)\n",
    "        #nl_date = \" \".join(number_list)\n",
    "        split_on_slash = nl_date.split(\"/\")\n",
    "        nl_date = \" / \".join(split_on_slash)\n",
    "\n",
    "        \n",
    "    embeddings = []\n",
    "    for word in nl_date.split(' '):\n",
    "        embeddings.append(glove[word])\n",
    "        #embeddings.append(torch.tensor(fasttext_model.get_word_vector(word)))\n",
    "\n",
    "\n",
    "    current_inp_length = len(embeddings)\n",
    "    inp_embeddings = torch.stack(embeddings)\n",
    "    inp_embeddings = inp_embeddings.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        encoder_outputs, hidden = model.encoder(inp_embeddings, [current_inp_length])\n",
    "        mask = model.create_mask([current_inp_length],max([current_inp_length]))\n",
    "        attentions = torch.zeros(max_len, 1, inp_embeddings.shape[1])\n",
    "        trg_indexes = []\n",
    "        for i in range(max_len):\n",
    "\n",
    "            if i==0:\n",
    "                trg_tensor = torch.LongTensor([output_vocab['<sos>']])\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                # input,hidden,encoder_outputs,encoder_length_mask\n",
    "                output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
    "\n",
    "            attentions[i] = attention\n",
    "            \n",
    "            pred_token = output.argmax(1).item()\n",
    "        \n",
    "            trg_indexes.append(pred_token)\n",
    "            trg_tensor = torch.LongTensor([pred_token])\n",
    "\n",
    "            if pred_token == output_vocab['<eos>']:\n",
    "                break\n",
    "    \n",
    "    trg_tokens = [output_index_to_word[i] for i in trg_indexes]\n",
    "    return trg_tokens, attentions\n",
    "    #print(trg_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '0', '1', '9', '-', '0', '4', '-', '2', '1']\n"
     ]
    }
   ],
   "source": [
    "output_tokens , attention = translate_sentence('21/04/2019',t.to(device_cpu))\n",
    "print(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmetrics(model,sentences,ground_truths):\n",
    "    exact_match_count = 0\n",
    "    per_word_matches = [0 for i in range(10)]\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        trg_tokens, attention_weights = translate_sentence(sentences[i],model)\n",
    "        print(trg_tokens)\n",
    "        ground_truth_tokens = list(ground_truths[i])\n",
    "\n",
    "        exact_match = True\n",
    "        for i in range(len(ground_truth_tokens)):\n",
    "\n",
    "            if trg_tokens[i] == ground_truth_tokens[i]:\n",
    "                per_word_matches[i]+=1\n",
    "            else:\n",
    "                exact_match = False\n",
    "        \n",
    "        if exact_match:\n",
    "            exact_match_count+=1\n",
    "\n",
    "    \n",
    "    number = len(sentences)\n",
    "    per_output_accuracy = [ 0.0 for i in range(10)]\n",
    "    exact_match_accuracy = ((1.0*exact_match_count)/number)*100\n",
    "\n",
    "    for i in range(len(per_word_matches)):\n",
    "\n",
    "        per_output_accuracy[i] = ((1.0*per_word_matches[i]) / number) * 100\n",
    "\n",
    "    return exact_match_accuracy,per_output_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '9', '9', '8', '-', '0', '1', '-', '0', '4']\n",
      "['2', '0', '0', '9', '-', '0', '5', '-', '2', '0']\n",
      "['2', '0', '1', '2', '-', '0', '9', '-', '2', '5']\n",
      "['1', '0', '5', '6', '-', '0', '1', '-', '0', '1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(75.0, [75.0, 100.0, 100.0, 100.0, 100.0, 100.0, 75.0, 100.0, 100.0, 100.0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = ['1998-01-04','2009-05-20','2012-09-25','2056-05-01']\n",
    "sentences = ['4 jan 1998','May 20 2009','September 25 2012','05/01/56']\n",
    "\n",
    "getmetrics(t,sentences,ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(sentence, translation, attention):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(attention, cmap='viridis')\n",
    "\n",
    "    nl_date = sentence.replace(\"\\'\",\"\").strip()\n",
    "    nl_date = nl_date.lower()\n",
    "\n",
    "    if \"/\" in nl_date:\n",
    "        #number_list = list(nl_date)\n",
    "        #nl_date = \" \".join(number_list)\n",
    "        split_on_slash = nl_date.split(\"/\")\n",
    "        nl_date = \" / \".join(split_on_slash)\n",
    "\n",
    "    src = nl_date.split(\" \")\n",
    "    trg = list(translation)\n",
    "\n",
    "    ax.set_xticklabels(src, minor=False, rotation='vertical')\n",
    "    ax.set_yticklabels(trg, minor=False)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    # and the x-ticks on top\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(np.arange(attention.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(attention.shape[0]) + 0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.colorbar(heatmap)\n",
    "    plt.show()\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "    \n",
    "    plt.tick_params(labelsize=15)\n",
    "    \n",
    "    #x_ticks = [''] +  [t for t in sentence.split(' ')]\n",
    "    #y_ticks = [''] + translation\n",
    "     \n",
    "    #ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    #ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    #ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    #ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vivitsu329\\AppData\\Local\\Temp\\ipykernel_8536\\3981681835.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(src, minor=False, rotation='vertical')\n",
      "C:\\Users\\Vivitsu329\\AppData\\Local\\Temp\\ipykernel_8536\\3981681835.py:19: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(trg, minor=False)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAAGyCAYAAAAF9aYyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAn6ElEQVR4nO3df3BV9Z3/8ddNIDcgN0EakwBeDeoKy1BJBUnDL3GbhaG7WKc6peoWNtvSFZMukLHVbCuh62hs6yDbXSQu20htVfLdtbZYMa5NG9EaGwiDtVWhVG0CmISsbW5M4QbuOd8/kGuzCTQ359x7cvJ5PmY+49yTc87nTSbmnc/78zmfE7Bt2xYAADBCmtcBAACA1CHxAwBgEBI/AAAGIfEDAGAQEj8AAAYh8QMAYBASPwAABiHxAwBgEBI/AAAGIfEDAGAQEj8AAAYZ43UAAIDUa21t1bvvvqu0tDRddtll+shHPuJ1SEgRRvwAYJCHHnpIl156qaZNm6b58+fr4x//uHJzc7Vw4UK1tLR4HR5SgMQPAIZ44IEHdO+99+rLX/6yHn74YU2fPl2bNm3SM888o8suu0yLFy/Wvn37vA4TSRbgtbwAYIZp06bpoYce0vLlyyVJhw4d0vz589Xe3q4xY8Zo3bp1euONN/Q///M/HkeKZCLxA4AhLrjgAv36179WQUGBJMm2bWVkZKi1tVWTJ0/Wq6++qoULF6qnp8fbQJFUlPoBwBBXXnmlnn/++fjnn/3sZ8rIyFB+fr4kKTMzU4FAwKvwkCKs6gcAQ1RWVurv/u7v9JOf/ESZmZn6wQ9+oH/6p3+KJ/vGxkbNmjXL4yiRbJT6AcAgzz77rL7//e8rGo1q2bJlWrNmTfxr//u//ytJPNo3ypH4AQAwCHP8AABJ0unTp9Xa2up1GEgyEj8AQJL061//WtOmTfM6DCQZiR8AAIOwqh8ADHH11Vef9+snTpxIUSTwEokfAAzx+uuv67Of/ew5y/nvvvuuDh06lOKokGokfgAwxKxZs1RUVKS1a9cO+vUDBw5o+/btKY4KqcYcPwAYYsGCBTp48OA5vx4KhbR48eIURgQv8Bw/AAAGYcQPAIBBmOM/h9///vd6+umntWrVKq9DgSH+6q/+So888oguvfRSr0PBKNfc3Kympia1t7dLkvLz81VcXKx58+Z5HBlSgVL/Obz66qu6+uqrFYvFvA4Fo8yuXbsGPf7pT39a//qv/6pwOCxJuv7661MZFgzQ2dmpG2+8UT//+c91ySWXKC8vT5LU0dGh1tZWLViwQE8++aRyc3M9jhTJZGzij0Qi5/36L3/5S1177bUkfrguLS1NgUBA5/tfLxAI8LMH19100006duyYHnnkEU2fPr3f1w4ePKh/+Id/0JQpU/Rf//VfHkWIVDA28Z/95Xsutm3zyxdJsXz5cqWnp6u2trbfyGrs2LF69dVXNXPmTA+jw2gWCoW0Z88efexjHxv06y0tLVqyZIl6enpSHBlSydg5/lAopK9+9asqKioa9Ou/+c1v9I//+I8pjgomePbZZ/Xggw9q7ty5euihh/S3f/u3XocEQwSDwfNWO3t6ehQMBlMYEbxgbOI/u3XltddeO+jXJ06ceN5SLODEhg0bdN111+nWW2/V008/rQcffNDrkGCAlStXavXq1XrwwQf1iU98QllZWZLOTH02NDSooqJCN998s8dRItmMTfy33HLLefelzs/PV1VVVQojgmkKCwu1d+9ebdiwQYWFhfyhiaTbvHmzLMvSZz/7WZ0+fVoZGRmSpL6+Po0ZM0af//zn9cADD3gcJZLN2Dl+YCR5+umn9dOf/lSVlZWsqEbSRSIRtbS09Hucb86cOfEKAEY3Ej/gga6uLtXW1g76LHVpaakuuugijyMEMFoZvXPfiRMn9NJLL+n1118f8LWTJ0/q0Ucf9SAqjHZ79+7VlVdeqW9/+9vKzs7W4sWLtXjxYmVnZ+vf/u3fNGPGDO3bt8/rMDFK8XsPxo74Dx06pKVLl6q1tVWBQEALFy7Uzp07NXnyZElnNrSYMmUKj/PBdR//+Mc1e/Zs1dTUDHik1LZt3XbbbfrlL3+ppqYmjyLEaDXY770nnnhCU6ZMkcTvPVMYO+K/8847NWvWLHV2durgwYMKhUJasGCBWltbvQ5txNu4caNaWlq8DsO3Xn31VW3YsGHQfSQCgYA2bNigAwcOpD4wH+Bnz5nBfu8tXLiQ33uGMTbxv/zyy6qurlZOTo6uuOIKPf3001q2bJkWLVqkt956y+vwRrQjR45o+fLluvjii7V27Vo9++yz6uvr8zos38jPz1dzc/M5v97c3BzfShX98bPnDL/3IBmc+E+cOKExYz58mjEQCGjbtm1asWKFrr32Wh06dMjD6Ea22tpatbe364knnlAoFNL69euVk5OjG2+8UY8++qjee+89r0Mc0e644w598Ytf1Lp167Rr1y794he/0C9+8Qvt2rVL69at02233aavfOUrXoc5IvGz5wy/9yBJsg11zTXX2I8++uigXysrK7MnTpxop6WlpTgq/3r99dftb3zjG/b8+fPtYDBoL1q0yP7Wt75lHzlyxOvQRqSdO3faRUVF9pgxY+xAIGAHAgF7zJgxdlFRkV1XV+d1eL7Cz97Q8XsPtm3bxi7uq66u1osvvqjdu3cP+vXbb79dNTU1siwrxZH53/Hjx7Vr1y7t2rVLixYt0h133OF1SCPWqVOn1NXVJUnKycnR2LFjPY7I3/jZOz9+70EyeFU/AAAmMnaOHwAAE5H4AQAwCIn/T0SjUW3atEnRaNTrUHyJ79/w8b0bPr53zvD9Mw9z/H8iEokoOztb3d3dvKxiGPj+DR/fu+Hje+cM3z/zMOIHAMAgJH4AAAwy5s+f4i7LsnTs2DGFQqFB9yr3UiQS6fdfJIbv3/DxvRs+vnfOjOTvn23b6unp0ZQpU5SWlrxx6smTJ13b+jkjI0OZmZmu3CtZUj7Hf+TIEYXD4VR2CQDwsba2Nl188cVJuffJkyc17dIJau90542E+fn5evvtt0d08k/5iD8UCkmSFuqTGiN2KQP8IJCe7nUIvvbE6/u8DsGXet639BdzjsTzRjL09fWpvTOm37UUKCvkrKoQ6bF06Zx31NfXR+L/U2fL+2M0VmMCJH7ADwIBEr8TThOK6VIxLTwhFNCEkLN+LI2s6etzSXniBwBgpInZlmIOJ75jtj/eccCfoQAAGIQRPwDAeJZsWXI25Hd6faqQ+AEAxrNkyWmh3vkdUoNSPwAABmHEDwAwXsy2FXO4rY3T61OFxA8AMJ5Jc/yU+gEAMAgjfgCA8SzZihky4ifxAwCMZ1Kpn8QPADCeSYv7mOMHAMAgCSX+6upqXXPNNQqFQsrNzdUNN9yggwcPJis2AABSwnKp+UFCif+FF15QWVmZXnnlFT3//PM6deqUli5dqt7e3mTFBwBA0sU+WNzntPlBQnP89fX1/T7v2LFDubm5amlp0eLFi10NDAAAuM/R4r7u7m5J0qRJk855TjQaVTQajX+ORCJOugQAwHUxWy68ltedWJJt2Iv7LMvS+vXrtWDBAs2aNeuc51VXVys7OzvewuHwcLsEACApmOMfgrKyMv3qV7/Szp07z3teZWWluru7462trW24XQIAAIeGVeovLy/Xj3/8Y+3Zs0cXX3zxec8NBoMKBoPDCg4AgFSwFFBMAcf38IOEEr9t2/rSl76kp556So2NjZo2bVqy4gIAIGUs+0xzeg8/SCjxl5WV6fHHH9ePfvQjhUIhtbe3S5Kys7M1bty4pAQIAADck1Di37ZtmyRpyZIl/Y4/8sgj+vu//3u3YgIAIKViLpT6nV6fKgmX+gEAGG1I/AAAGMSyA7Jsh4v7HF6fKrykBwAAgzDiBwAYj1I/AAAGiSlNMYdF8JhLsSQbpX4AAAzCiB8AYDzbhcV9tk8W95H4AQDGM2mOn1I/AAAGYcQPADBezE5TzHa4uM8ne9yR+AEAxrMUkOWwCG7JH5mfxA8AMJ5Jc/wkfh8JjBnrdQi+Zp8+5XUIvpXG2zcd6Yid9DoEX+qJWV6HMCqR+AEAxnNnjp9SPwAAvnBmjt/hS3p8UurncT4AAAzCiB8AYDzLhb36WdUPAIBPmDTHT6kfAACDMOIHABjPUhob+AAAYIqYHVDM4dv1nF6fKpT6AQAwCCN+AIDxYi6s6o9R6gcAwB8sO02Ww1X9lk9W9ZP4AQDGM2nEzxw/AAAGYcQPADCeJeer8v3yLsFhjfi3bt2qgoICZWZmqqioSM3NzW7HBQBAypx9jt9p84OEo6yrq1NFRYWqqqq0f/9+zZ49W8uWLVNnZ2cy4gMAAC5KOPFv3rxZa9asUWlpqWbOnKmamhqNHz9etbW1yYgPAICkO7tXv9PmBwlF2dfXp5aWFpWUlHx4g7Q0lZSUqKmpadBrotGoIpFIvwYAwEhiKeBK84OEEn9XV5disZjy8vL6Hc/Ly1N7e/ug11RXVys7OzvewuHw8KMFAACOJL0uUVlZqe7u7nhra2tLdpcAACTEpFJ/Qo/z5eTkKD09XR0dHf2Od3R0KD8/f9BrgsGggsHg8CMEACDJ3NnAxx+JP6EoMzIyNGfOHDU0NMSPWZalhoYGFRcXux4cAABwV8Ib+FRUVGj16tWaO3eu5s2bpy1btqi3t1elpaXJiA8AgKSz7IAspxv4+OS1vAkn/pUrV+r48ePauHGj2tvbVVhYqPr6+gEL/gAA8AvLhVK/XzbwGdaWveXl5SovL3c7FgAAPOHO2/n8kfj9ESUAAHAFL+kBABgvpoBiDjfgcXp9qpD4AQDGo9QPAABGJUb8AADjxeS8VB9zJ5SkI/EDAIxHqR8AAIxKJH4AgPG8eknP1q1bVVBQoMzMTBUVFam5ufm852/ZskXTp0/XuHHjFA6HtWHDBp08eTKhPkn8AADj2QrIctjsBNcI1NXVqaKiQlVVVdq/f79mz56tZcuWqbOzc9DzH3/8cd11112qqqrSG2+8oe985zuqq6vTP//zPyfUL4kfAAAPbN68WWvWrFFpaalmzpypmpoajR8/XrW1tYOe//LLL2vBggW65ZZbVFBQoKVLl+rmm2/+s1WC/4vEDwAwnpul/kgk0q9Fo9EB/fX19amlpUUlJSXxY2lpaSopKVFTU9OgMc6fP18tLS3xRP/WW29p9+7d+uQnP5nQv5VV/T5inz7ldQgwlHVy4C8uDN1F6Rleh+BLwXQrZX25+Xa+cDjc73hVVZU2bdrU71hXV5disdiAF9zl5eXpzTffHPT+t9xyi7q6urRw4ULZtq3Tp0/rtttuS7jUT+IHABgv5sLb+c5e39bWpqysrPjxYDDo6L5nNTY26r777tNDDz2koqIiHT58WOvWrdM999yju+++e8j3IfEDAOCirKysfol/MDk5OUpPT1dHR0e/4x0dHcrPzx/0mrvvvluf+9zn9IUvfEGS9NGPflS9vb364he/qK9+9atKSxvaHy7M8QMAjHe21O+0DVVGRobmzJmjhoaGD2OwLDU0NKi4uHjQa/74xz8OSO7p6emSJNu2h9w3I34AgPEspclyOBZO9PqKigqtXr1ac+fO1bx587Rlyxb19vaqtLRUkrRq1SpNnTpV1dXVkqQVK1Zo8+bN+tjHPhYv9d99991asWJF/A+AoSDxAwDggZUrV+r48ePauHGj2tvbVVhYqPr6+viCv9bW1n4j/K997WsKBAL62te+pqNHj+qiiy7SihUrdO+99ybUb8BOpD7ggkgkouzsbC3RpzQmMDaVXQMYpsAY/l914qnf/dzrEHwp0mNpyvQj6u7u/rNz5sPu44OctPbFTys4wdnPefT9U9q26AdJjdcNjPgBAMZz83G+kY7FfQAAGIQRPwDAeLYLr+W1ffJaXhI/AMB4MQUUS/AlO4Pdww/88ecJAABwBSN+AIDxLNv54jwrpc/IDR+JHwBgPMuFOX6n16cKiR8AYDxLAVkO5+idXp8qCf95smfPHq1YsUJTpkxRIBDQD3/4wySEBQAAkiHhxN/b26vZs2dr69atyYgHAICUi9kBV5ofJFzqX758uZYvX56MWAAA8ARz/C6KRqOKRqPxz5FIJNldAgCAc0j6nyfV1dXKzs6Ot3A4nOwuAQBIiKVAfL/+YbfRurgvUZWVleru7o63tra2ZHcJAEBC7A9W9Ttptk8Sf9JL/cFgUMFgMNndAACAIeA5fgCA8Ux6LW/Cif/999/X4cOH45/ffvttHThwQJMmTdIll1zianAAAKQCq/rPY9++fbruuuvinysqKiRJq1ev1o4dO1wLDAAAuC/hxL9kyRLZtk/eRAAAwBBQ6gcAwCAm7dVP4gcAGM+kEb8/ViIAAABXMOIHABjPpBE/iR8AYDyTEj+lfgAADMKIHwBgPJNG/CR+AIDxbDl/HM8vO9xQ6gcAwCCM+AEAxqPUDwCAQUj8APCnbMvrCHwtjVnVYeG7lhwkfgCA8RjxAwBgEBI/AAAGse2AbIeJ2+n1qcIUCgAABmHEDwAwnqWA4w18nF6fKiR+AIDxTJrjp9QPAIBBGPEDAIxn0uI+Ej8AwHiU+gEAwKjEiB8AYDxK/QAAGMR2odTvl8RPqR8AAIMw4gcAGM+WZNvO7+EHCY/4e3p6tH79el166aUaN26c5s+fr7179yYjNgAAUuLszn1Omx8knPi/8IUv6Pnnn9f3vvc9vfbaa1q6dKlKSkp09OjRZMQHAEDSnV3c57T5QUKJ/8SJE3ryySf1zW9+U4sXL9YVV1yhTZs26YorrtC2bduSFSMAAHBJQnP8p0+fViwWU2ZmZr/j48aN00svvTToNdFoVNFoNP45EokMI0wAAJLHsgMKsIHPQKFQSMXFxbrnnnt07NgxxWIxff/731dTU5PefffdQa+prq5WdnZ2vIXDYVcCBwDALbbtTvODhOf4v/e978m2bU2dOlXBYFDf/va3dfPNNystbfBbVVZWqru7O97a2tocBw0AAIYn4cf5Lr/8cr3wwgvq7e1VJBLR5MmTtXLlSl122WWDnh8MBhUMBh0HCgBAspi0c9+wN/C54IILNHnyZP3+97/Xc889p0996lNuxgUAQMqYtKo/4RH/c889J9u2NX36dB0+fFhf/vKXNWPGDJWWliYjPgAA4KKEE393d7cqKyt15MgRTZo0STfeeKPuvfdejR07NhnxAQCQdCat6k848X/mM5/RZz7zmWTEAgCAJ9xYlT9qV/UDAAD/4iU9AADjnRnxO13V71IwSUbiBwAYz6TH+Uj8AADj2XL+Wl2fDPiZ4wcAwCSM+AEAxqPUDwCASQyq9VPqBwDAI1u3blVBQYEyMzNVVFSk5ubm857/hz/8QWVlZZo8ebKCwaCuvPJK7d69O6E+GfEDAODGXvsJXl9XV6eKigrV1NSoqKhIW7Zs0bJly3Tw4EHl5uYOOL+vr09//dd/rdzcXP33f/+3pk6dqt/97neaOHFiQv2S+AEAxvNi577NmzdrzZo18Xfd1NTU6JlnnlFtba3uuuuuAefX1tbqvffe08svvxzfJr+goCDhOCn1AwDgokgk0q9Fo9EB5/T19amlpUUlJSXxY2lpaSopKVFTU9Og9921a5eKi4tVVlamvLw8zZo1S/fdd59isVhC8Xk24k+f+RdKTw961b0v2W+1eR2Cr3XUXeJ1CL6V/xXL6xB87dPXDCzb4s87bfVJejglfbm5qj8cDvc7XlVVpU2bNvU71tXVpVgspry8vH7H8/Ly9Oabbw56/7feeks//elPdeutt2r37t06fPiwbr/9dp06dUpVVVVDjpNSPwAAdiDhOfpB7yGpra1NWVlZ8cPBoDuDXMuylJubq//4j/9Qenq65syZo6NHj+pb3/oWiR8AAK9kZWX1S/yDycnJUXp6ujo6Ovod7+joUH5+/qDXTJ48WWPHjlV6enr82F/+5V+qvb1dfX19ysjIGFJ8zPEDAIx3dnGf0zZUGRkZmjNnjhoaGuLHLMtSQ0ODiouLB71mwYIFOnz4sCzrw6m3Q4cOafLkyUNO+hKJHwCADzfwcdoSUFFRoe3bt+u73/2u3njjDa1du1a9vb3xVf6rVq1SZWVl/Py1a9fqvffe07p163To0CE988wzuu+++1RWVpZQv5T6AQDG82LL3pUrV+r48ePauHGj2tvbVVhYqPr6+viCv9bWVqWlfTg+D4fDeu6557RhwwZdddVVmjp1qtatW6c777wzoX5J/AAAeKS8vFzl5eWDfq2xsXHAseLiYr3yyiuO+iTxAwAg+WavfadI/AAA45n0dj4W9wEAYBBG/AAAGPRaXhI/AAAKfNCc3mPko9QPAIBBGPEDAECpHwAAgxiU+Cn1AwBgEEb8AAC4+FrekY7EDwAwXqJv1zvXPfwg6Yk/Go0qGo3GP0cikWR3CQBAYpjjH5rHHntMEyZMiLcXX3xxwDnV1dXKzs6Ot3A47KRLAADggKMR//XXX6+ioqL456lTpw44p7KyUhUVFfHPkUiE5A8AGFmY4x+aUCikUCh03nOCwaCCwaCTbgAASKqAfaY5vYcf8DgfAAAGYVU/AAAGLe4j8QMAYNAcP6V+AAAMwogfAABK/QAAGMSgxE+pHwAAgzDiBwDAoBE/iR8AAINW9ZP4AQDGY+c+AAAwKjHiBwDAoDl+RvwAABiExA8AgEEo9QMAjBeQC4v7XIkk+TxL/PZvW2UHMrzq3p/SKdA4UfvR73odgm/d9ZuFXofga3Ys5nUIvnTaPpW6zgx6nI9MAgCAQSj1AwBg0Kp+Ej8AAAYlfkr9AAAYhBE/AMB4Jm3ZS+IHAMCgUj+JHwAAgxI/c/wAABiEET8AwHjM8QMAYBJ27gMAAKMRI34AAAxa3EfiBwAYz6Q5fkr9AAAYZFiJf+vWrSooKFBmZqaKiorU3NzsdlwAAKSO7VLzgYQTf11dnSoqKlRVVaX9+/dr9uzZWrZsmTo7O5MRHwAAyWd/WO4fbhu1iX/z5s1as2aNSktLNXPmTNXU1Gj8+PGqra1NRnwAAMBFCSX+vr4+tbS0qKSk5MMbpKWppKRETU1Ng14TjUYViUT6NQAARhRK/YPr6upSLBZTXl5ev+N5eXlqb28f9Jrq6mplZ2fHWzgcHn60AAAkA4nfPZWVleru7o63tra2ZHcJAEBCnM7vu/E4YKok9Bx/Tk6O0tPT1dHR0e94R0eH8vPzB70mGAwqGAwOP0IAAOCahEb8GRkZmjNnjhoaGuLHLMtSQ0ODiouLXQ8OAAC4K+Gd+yoqKrR69WrNnTtX8+bN05YtW9Tb26vS0tJkxAcAQPKxZe+5rVy5UsePH9fGjRvV3t6uwsJC1dfXD1jwBwAARp5h7dVfXl6u8vJyt2MBAMATJu3Vz0t6AACQfFOqd4qX9AAAYBBG/AAAsLgPAABzmDTHT6kfAACDMOIHAIBSPwAA5qDUDwCASTx6O9/WrVtVUFCgzMxMFRUVqbm5eUjX7dy5U4FAQDfccEPCfZL4AQDwQF1dnSoqKlRVVaX9+/dr9uzZWrZsmTo7O8973TvvvKM77rhDixYtGla/JH4AADwY8W/evFlr1qxRaWmpZs6cqZqaGo0fP161tbXnvCYWi+nWW2/V17/+dV122WWJdfgBEj8AwHhn5/idNkmKRCL9WjQaHdBfX1+fWlpaVFJSEj+WlpamkpISNTU1nTPOf/mXf1Fubq4+//nPD/vf6t3iPtuWZHnWvR9ZJ/q8DsHXJqad8joEGCptLOuohyPNtiUf/toLh8P9PldVVWnTpk39jnV1dSkWiw14wV1eXp7efPPNQe/70ksv6Tvf+Y4OHDjgKD5+GgEAcPFxvra2NmVlZcUPB4NBhzeWenp69LnPfU7bt29XTk6Oo3uR+AEAcDHxZ2Vl9Uv8g8nJyVF6ero6Ojr6He/o6FB+fv6A83/729/qnXfe0YoVK+LHLOtM1XzMmDE6ePCgLr/88iGFyRw/AAAplpGRoTlz5qihoSF+zLIsNTQ0qLi4eMD5M2bM0GuvvaYDBw7E2/XXX6/rrrtOBw4cGDC9cD6M+AEAxvNiA5+KigqtXr1ac+fO1bx587Rlyxb19vaqtLRUkrRq1SpNnTpV1dXVyszM1KxZs/pdP3HiREkacPzPIfEDAODBlr0rV67U8ePHtXHjRrW3t6uwsFD19fXxBX+tra1KS3O/ME/iBwDAI+Xl5SovLx/0a42Njee9dseOHcPqk8QPADCeSXv1k/gBAODtfAAAGMSgxM/jfAAAGIQRPwDAeIEPmtN7+AGJHwAASv0AAGA0YsQPADCeSY/zORrx33///QoEAlq/fr1L4QAA4AHbpeYDw078e/fu1cMPP6yrrrrKzXgAAEASDSvxv//++7r11lu1fft2XXjhhW7HBABA6hkw2peGmfjLysr0N3/zNyopKfmz50ajUUUikX4NAICR5Owcv9PmBwkv7tu5c6f279+vvXv3Dun86upqff3rX084MAAA4L6ERvxtbW1at26dHnvsMWVmZg7pmsrKSnV3d8dbW1vbsAIFACBpDFrcl9CIv6WlRZ2dnbr66qvjx2KxmPbs2aN///d/VzQaVXp6er9rgsGggsGgO9ECAJAEJj3Ol1Di/8QnPqHXXnut37HS0lLNmDFDd95554CkDwCALxi0c19CiT8UCmnWrFn9jl1wwQX6yEc+MuA4AAAYedi5DwBgPEr9CWhsbHQhDAAAPGRQqZ+X9AAAYBBK/QAAGDTiJ/EDAIxn0hw/pX4AAAzCiB8AAEr9AACYI2DbCtjOMrfT61OFxA8AgEEjfub4AQAwCCN+AIDxTFrVT+IHAIBSPwAAGI28G/HPvFxKD3rWvR+l/6bV6xB8bd3bN3kdgm8F0v/gdQi+lpYzyesQfCnNikrHUtMXpX4AAExCqR8AAIxGjPgBAMaj1A8AgEko9QMAgNGIET8AAPJPqd4pEj8AALZ9pjm9hw+Q+AEAxjNpcR9z/AAAGIQRPwAABq3qJ/EDAIwXsM40p/fwA0r9AAAYhBE/AACU+gEAMAer+gEAwKjEiB8AADbwAQDAHCaV+pOe+KPRqKLRaPxzJBJJdpcAAOAcHM3xP/bYY5owYUK8vfjiiwPOqa6uVnZ2dryFw2EnXQIA4D7bpeYDjkb8119/vYqKiuKfp06dOuCcyspKVVRUxD9HIhGSPwBgRKHUP0ShUEihUOi85wSDQQWDQSfdAACQXAYt7uNxPgAADMKqfgCA8Sj1AwBgEoO27KXUDwCAQRjxAwCMR6kfAACTWPaZ5vQePkCpHwAAgzDiBwDAoMV9JH4AgPECcmGO35VIko9SPwAABmHEDwAAW/YCAGCOs4/zOW2J2rp1qwoKCpSZmamioiI1Nzef89zt27dr0aJFuvDCC3XhhReqpKTkvOefC4kfAAAPXstbV1eniooKVVVVaf/+/Zo9e7aWLVumzs7OQc9vbGzUzTffrJ/97GdqampSOBzW0qVLdfTo0YT6JfEDAOCBzZs3a82aNSotLdXMmTNVU1Oj8ePHq7a2dtDzH3vsMd1+++0qLCzUjBkz9J//+Z+yLEsNDQ0J9UviBwAYL2DbrjRJikQi/Vo0Gh3QX19fn1paWlRSUhI/lpaWppKSEjU1NQ0p5j/+8Y86deqUJk2alNC/1bPFfU/+v/+nrFC6V9370rIps70Owddi1/Z4HQIMZR095nUIvnTaPpW6zqwPmtN7SAqHw/0OV1VVadOmTf2OdXV1KRaLKS8vr9/xvLw8vfnmm0Pq7s4779SUKVP6/fEwFKzqBwDARW1tbcrKyop/DgaDrvdx//33a+fOnWpsbFRmZmZC15L4AQDG+9NSvZN7SFJWVla/xD+YnJwcpaenq6Ojo9/xjo4O5efnn/faBx54QPfff79+8pOf6Kqrrko4Tub4AQBI8ar+jIwMzZkzp9/CvLML9YqLi8953Te/+U3dc889qq+v19y5cxP4B36IET8AAB6oqKjQ6tWrNXfuXM2bN09btmxRb2+vSktLJUmrVq3S1KlTVV1dLUn6xje+oY0bN+rxxx9XQUGB2tvbJUkTJkzQhAkThtwviR8AAA927lu5cqWOHz+ujRs3qr29XYWFhaqvr48v+GttbVVa2oeF+W3btqmvr0833XRTv/sMtnjwfEj8AADjDXfnvf97j0SVl5ervLx80K81Njb2+/zOO+8k3sEgmOMHAMAgjPgBADDoJT0kfgCA8QLWmeb0Hn5A4gcAwKARP3P8AAAYhBE/AADDeK3uoPfwARI/AMB4bm7ZO9JR6gcAwCAJJf7q6mpdc801CoVCys3N1Q033KCDBw8mKzYAAFLj7OI+p80HEkr8L7zwgsrKyvTKK6/o+eef16lTp7R06VL19vYmKz4AAJLPlmQ5bP7I+4nN8dfX1/f7vGPHDuXm5qqlpUWLFy92NTAAAOA+R4v7uru7JUmTJk065znRaFTRaDT+ORKJOOkSAADXsbhvCCzL0vr167VgwQLNmjXrnOdVV1crOzs73sLh8HC7BAAgOWy5MMfv9T9iaIad+MvKyvSrX/1KO3fuPO95lZWV6u7ujre2trbhdgkAABwaVqm/vLxcP/7xj7Vnzx5dfPHF5z03GAwqGAwOKzgAAFLCoC17E0r8tm3rS1/6kp566ik1NjZq2rRpyYoLAIDUsSQFXLiHDySU+MvKyvT444/rRz/6kUKhkNrb2yVJ2dnZGjduXFICBAAg2Vjcdw7btm1Td3e3lixZosmTJ8dbXV1dsuIDAAAuSrjUDwDAqMMcPwAABjEo8fOSHgAADMKIHwAAg0b8JH4AAAx6nI9SPwAABmHEDwAwnknP8ZP4AQAwaI6fUj8AAAZhxA8AgGVLAYcjdssfI34SPwAABpX6SfwAAMiFxC9/JH7m+AEAMAgjfgAAKPUDAGAQy5bjUr1PFvdR6gcAwCCM+AEAsK0zzek9fIDEDwCAQXP8lPoBADAII34AAAxa3EfiBwCAUj8AABiNGPEDAGDLhRG/K5EkHYkfAACDSv0kfgAALEuSw+fwLX88x88cPwAABmHEDwCAQaX+hEf8e/bs0YoVKzRlyhQFAgH98Ic/TEJYAACk0NnE77T5QMKJv7e3V7Nnz9bWrVuTEQ8AAEiihEv9y5cv1/Lly5MRCwAA3mDnPvdEo1FFo9H450gkkuwuAQBIiG1bsh2+Xc/p9amS9FX91dXVys7OjrdwOJzsLgEAwDkkPfFXVlaqu7s73tra2pLdJQAAibHtM6V6J80ni/uSXuoPBoMKBoPJ7gYAgOGzXZjj90niZwMfAAAMkvCI//3339fhw4fjn99++20dOHBAkyZN0iWXXOJqcAAApIRlSQGHi/N8srgv4cS/b98+XXfddfHPFRUVkqTVq1drx44drgUGAEDKGFTqTzjxL1myRLZP/nEAAAyFbVmyHY74eZwPAACMOLykBwAASv0AABjEsqWAGYmfUj8AAAZhxA8AgG1Lcvo4nz9G/CR+AIDxbMuW7bDU75cn3ij1AwBgEEb8AADYlpyX+v3xHD+JHwBgPEr9AABgVEr5iP/sX0SR9/1REhlJTtunvA4BAFLmtM78zkvFSPq0HXVcqj8b70iX8sTf09MjSbr06ndS3fUo8JbXAQBAyvX09Cg7Ozsp987IyFB+fr5eat/tyv3y8/OVkZHhyr2SJWCneFLCsiwdO3ZMoVBIgUAglV0DAHzEtm319PRoypQpSktL3sz0yZMn1dfX58q9MjIylJmZ6cq9kiXliR8AAHiHxX0AABiExA8AgEFI/AAAGITEDwCAQUj8AAAYhMQPAIBBSPwAABjk/wMT9TgL3tNjFwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_tokens , attention = translate_sentence('21/04/2019',t.to(device_cpu))\n",
    "plot_heatmap('21/04/2019',''.join(output_tokens),attention.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '9', '9', '6', '-', '0', '9', '-', '2', '4']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_tokens , attention = translate_sentence('20/06/1997',t.to(device_cpu))\n",
    "print(output_tokens)\n",
    "display_attention('3/8/60',output_tokens,attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(100,64, num_layers= 2 ,batch_first= True ,bidirectional=True)\n",
    "inp = torch.randn((2,8,100))\n",
    "outputs,hidden = rnn(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inq = torch.cat([hidden[-i , : , :] for i in range(4)],dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn(inp)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(10,30)\n",
    "\n",
    "embedding_layer(torch.tensor([[10,2,3],[4,5,6]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('nlpenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db937d70f0b42a9864491ccec092d177c951bc6bfc86971ae63a9e250c306d60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
