{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torchtext.vocab import GloVe\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import SubsetRandomSampler,DataLoader\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "\n",
    "TRAINABLE_EMBEDDINGS = False\n",
    "EMBED_DIM = 300 \n",
    "ENC_BIDIRECTIONAL = True\n",
    "ENC_BIDIRECTIONAL_FACTOR = 2 if ENC_BIDIRECTIONAL else 1\n",
    "ENC_HIDDEN_DIM = 256\n",
    "DEC_HIDDEN_DIM  = 256\n",
    "ENC_OUTPUT_DIM =  DEC_HIDDEN_DIM\n",
    "DEC_EMBED_DIM = 300\n",
    "NUM_EPOCHS = 300\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device_cpu = torch.device('cpu')\n",
    "device_fast = torch.device('cpu')\n",
    "\n",
    "if torch.has_mps:\n",
    "    device_fast = torch.device('mps')\n",
    "elif torch.has_cuda:\n",
    "    device_fast = torch.device('cuda')\n",
    "\n",
    "\n",
    "output_index_to_word = {}\n",
    "output_vocab = {}\n",
    "\n",
    "for i in range(0,10):\n",
    "    output_vocab[str(i)] = i\n",
    "    output_index_to_word[i] = str(i)\n",
    "\n",
    "output_vocab['-'] = 10\n",
    "output_index_to_word[10] = '-'\n",
    "output_vocab['<sos>'] = 11\n",
    "output_index_to_word[11] = '<sos>'\n",
    "output_vocab['<eos>'] = 12\n",
    "output_index_to_word[12] = '<eos>'\n",
    "\n",
    "glove = GloVe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_training_data(filename='./Assignment4aDataset.txt',glove=glove):\n",
    "    \n",
    "    f = open(filename,'r')\n",
    "    dataset = []\n",
    "    for line in f.readlines():       \n",
    "         \n",
    "        nl_date , out_date = line.split(',')\n",
    "        nl_date = nl_date.replace(\"\\'\",\"\").strip()\n",
    "        out_date = out_date.replace(\"\\'\",\"\").strip()\n",
    "\n",
    "        if \"/\" in nl_date:\n",
    "            #number_list = list(nl_date)\n",
    "            #nl_date = \" \".join(number_list)\n",
    "            split_on_slash = nl_date.split(\"/\")\n",
    "            nl_date = \" / \".join(split_on_slash)\n",
    "\n",
    "        nl_date = nl_date.lower()\n",
    "\n",
    "        embeddings = []\n",
    "        for word in nl_date.split(' '):\n",
    "            if word == '':\n",
    "                continue\n",
    "            if not TRAINABLE_EMBEDDINGS:\n",
    "                embeddings.append(glove[word])\n",
    "            else:\n",
    "                if word not in glove.stoi:\n",
    "                    embeddings.append(glove.stoi['unk'])\n",
    "                else:\n",
    "                    embeddings.append(glove.stoi[word])\n",
    "            \n",
    "            #embeddings.append(torch.tensor(fasttext_model.get_word_vector(word)))\n",
    "        current_inp_length = len(embeddings)\n",
    "        if not TRAINABLE_EMBEDDINGS:\n",
    "            embeddings = torch.stack(embeddings)\n",
    "        else:\n",
    "            embeddings = torch.tensor(embeddings)\n",
    "\n",
    "        target = []\n",
    "        target.append(output_vocab['<sos>'])\n",
    "\n",
    "        for character in list(out_date):\n",
    "            target.append(output_vocab[character])\n",
    "        \n",
    "        target.append(output_vocab['<eos>'])\n",
    "\n",
    "        dataset.append({'in' : embeddings,'in_length' : current_inp_length,'out' : target})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def collate_function(batch_data):\n",
    "    inputs = [b['in'] for b in batch_data]\n",
    "    in_lengths = [b['in_length'] for b in batch_data]\n",
    "    out = torch.tensor([b['out'] for b in batch_data])\n",
    "    inputs = pad_sequence(inputs,batch_first=True)\n",
    "    return {'src': inputs, 'src_length' : in_lengths, 'trg' : out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(get_training_data())\n",
    "\n",
    "train_idx,valid_idx = train_test_split(np.arange(len(train_dataset)), \n",
    "    test_size=0.2,\n",
    "    shuffle= True,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_dataloader = DataLoader(train_dataset,BATCH_SIZE,sampler=train_sampler,collate_fn=collate_function)\n",
    "valid_dataloader = DataLoader(train_dataset,BATCH_SIZE,sampler=valid_sampler,collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim = EMBED_DIM,enc_hidden_dim = ENC_HIDDEN_DIM,enc_output_dim = ENC_OUTPUT_DIM,NUM_LAYERS=1,enc_bidirectional=ENC_BIDIRECTIONAL,dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        #self.embedding_layer = nn.Embedding(vocab_size,EMBED_DIM)\n",
    "        self.rnn = nn.GRU(embed_dim,enc_hidden_dim, num_layers = NUM_LAYERS ,batch_first= True ,bidirectional=enc_bidirectional)\n",
    "\n",
    "        if TRAINABLE_EMBEDDINGS:\n",
    "            self.embedding_layer = nn.Embedding.from_pretrained(glove.vectors,freeze=False)\n",
    "\n",
    "        # ENCODER_OUTPUT_DIM = DECODER_HIDDEN_SIZE\n",
    "        self.fc = nn.Linear(2*enc_hidden_dim,enc_output_dim) \n",
    "\n",
    "        self.fc_out = nn.Linear(enc_output_dim,1)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inp,inp_len):\n",
    "        \n",
    "        if TRAINABLE_EMBEDDINGS:\n",
    "            embedded_input = self.embedding_layer(inp)\n",
    "        else:\n",
    "            embedded_input = inp   # [batch_size, input_seq_length, embed_dim ]\n",
    "        \n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(embedded_input,inp_len,batch_first=True,enforce_sorted=False)\n",
    "        packed_output , hidden = self.rnn(packed_embedding)  # hidden = [D*num_layers, batch_size , hidden_dim ]\n",
    "        outputs, _  = nn.utils.rnn.pad_packed_sequence(packed_output,batch_first=True)  # [batch_size, inp_seq_length, hidden_dim]\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)))  # [batch_size, decoder_hidden_size]\n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear(enc_hidden_dim+dec_hidden_dim,dec_hidden_dim)\n",
    "        self.v = nn.Linear(dec_hidden_dim,1)\n",
    "\n",
    "    def forward(self,hidden,encoder_outputs, encoder_length_mask):\n",
    "        \n",
    "        # encoder_outputs = [batch_size,seq_length, enc_hidden_dim][2*ENCODER_HIDDEN_DIM or ENCODER_HIDDEN_DIM]\n",
    "        # hidden = [batch_size,  dec_hidden_dim]\n",
    "        # encoder_length_mask = [batch_size, seq_length]\n",
    "\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        h = hidden.unsqueeze(1).repeat(1,src_len,1)  # h = [batch_size,seq_length,dec_hidden_dim]\n",
    "        energy = torch.tanh(self.attn(torch.cat((h,encoder_outputs),dim=2)))  #[batch_size,seq_length,dec_hidden_dim]\n",
    "        attention_scores = self.v(energy).squeeze(2)  # attention_scores = [batch_size , seq_length ]\n",
    "        attention_scores = attention_scores.masked_fill(encoder_length_mask==1, -1e10)   # Fill padding tokens with a lower value\n",
    "        attention_scores = F.softmax(attention_scores,dim=1)\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src_lengths,max_src_length):\n",
    "\n",
    "        src_mask = torch.zeros((len(src_lengths),max_src_length),dtype=torch.int64)\n",
    "        for i in range(len(src_lengths)):\n",
    "\n",
    "            src_mask[i,src_lengths[i]:] = 1\n",
    "        return src_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc = Encoder(30,30,15,10)\n",
    "\n",
    "inp = torch.randn((3,20,30))\n",
    "inp_len = [20 for i in range(3)]\n",
    "outputs, hidden = enc(inp,inp_len)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,enc_hidden_dim,dec_hidden_dim,dec_output_dim,emb_dim):\n",
    "        \n",
    "        # enc_hidden_dim = 2*ENCODER_HIDDEN_DIM or ENCODER_HIDDEN_DIM\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention = Attention(enc_hidden_dim,dec_hidden_dim)\n",
    "        self.embedding_layer = nn.Embedding(vocab_size,emb_dim)\n",
    "        self.rnn = nn.GRU(enc_hidden_dim + emb_dim,dec_hidden_dim,batch_first = True)\n",
    "\n",
    "        self.fc_out = nn.Linear(enc_hidden_dim + emb_dim + dec_hidden_dim , dec_output_dim)\n",
    "        \n",
    "        #self.fc_tilde = nn.Linear(enc_hidden_dim + emb_dim + dec_hidden_dim , dec_hidden_dim)\n",
    "        #self.max_out_layer = nn.MaxPool1d(kernel_size=2)\n",
    "        #self.W0 = nn.Linear(dec_hidden_dim>>1,emb_dim)\n",
    "        #self.prob_out = nn.Linear(emb_dim,vocab_size)\n",
    "        #self.prob_out.weight = self.embedding_layer.weight\n",
    "        \n",
    "\n",
    "    def forward(self,input,hidden,encoder_outputs,encoder_length_mask):\n",
    "            # encoder outputs =  batch_size , seq_len , encoder_output_dim\n",
    "            # hidden = batch_size , hidden_dim\n",
    "            # input = batch_size\n",
    "            \n",
    "            input = input.unsqueeze(0) # [1,batch_size]\n",
    "            embedded = self.embedding_layer(input) # [1,batch_size,embed_dim]\n",
    "\n",
    "            embedded = embedded.permute(1,0,2) #[ batch_size, seq_length=1, embed_dim ]\n",
    "\n",
    "            attention_vector = self.attention(hidden,encoder_outputs,encoder_length_mask) # [ batch_size , seq_length ]\n",
    "            attention_vector = attention_vector.unsqueeze(1) # [batch_size , 1 , seq_length ]\n",
    "\n",
    "            weighted = torch.bmm(attention_vector,encoder_outputs) # [ batch_size, 1, encoder_output_dim]\n",
    "            #weighted = weighted.permute(1,0,2) #[1 , batch_size , encoder_output_dim]\n",
    "\n",
    "\n",
    "            rnn_input = torch.cat((embedded,weighted),dim=2) #[batch_size, seq_length=1, encoder + decoder]\n",
    "\n",
    "            out,h = self.rnn(rnn_input,hidden.unsqueeze(0)) # consider only a single layer (1.) so unsqueeze(0)\n",
    "\n",
    "            # out = [batch_size, seq_length = 1, decoder_hidden_out (bidirectional)]\n",
    "            # hidden = [D*num_layers,batch_size, decoder_hidden_out]\n",
    "\n",
    "\n",
    "            embedded = embedded.squeeze(1)  # [batch_size,embed_dim]\n",
    "            out = out.squeeze(1)    #[batch_size, decoder_hidden_out] # Have to change if the number of layers is changed to more than 1\n",
    "            weighted = weighted.squeeze(1)  # [batch_size,encoder_output_dim] \n",
    "            prediction = self.fc_out(torch.cat([embedded,out,weighted],dim=1)) #[batch_size, decoder_output_dim]\n",
    "            \n",
    "            \n",
    "            #prediction = F.softmax(self.fc_out(torch.cat([embedded,out,weighted],dim=1)),dim=1) #[batch_size, decoder_output_dim]\n",
    "            #t_tilde =  self.fc_tilde(torch.cat([embedded,out,weighted],dim=1)) # [batch_size, decoder_hidden_dim]\n",
    "            #t_tilde = self.max_out_layer(t_tilde.unsqueeze(1)) #[batch_size,1,decoder_hidden_dim/2]\n",
    "            #t_tilde = t_tilde.squeeze(1)\n",
    "\n",
    "            #inter_step  = self.W0(t_tilde) # [ batch_size , dec_embeddim]\n",
    "            #prediction = self.prob_out(inter_step) #[  batch_size , vocab_size]\n",
    "            #prediction = F.softmax(prediction,dim=1)\n",
    "            return prediction, h.squeeze(0), attention_vector # Reduce the number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                input_embed_dim = EMBED_DIM,\n",
    "                encoder_hidden_dim = ENC_HIDDEN_DIM,\n",
    "                encoder_hidden_output = ENC_OUTPUT_DIM,\n",
    "                enc_num_layers = 1,\n",
    "                enc_bidirectional = ENC_BIDIRECTIONAL,\n",
    "\n",
    "                dec_vocab_size = len(output_vocab),\n",
    "                dec_embed_dim = DEC_EMBED_DIM,\n",
    "                dec_hidden_dim  =DEC_HIDDEN_DIM,\n",
    "                device_train = device_cpu\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_embed_dim,encoder_hidden_dim,encoder_hidden_output,enc_num_layers,enc_bidirectional=enc_bidirectional)\n",
    "        enc_bidirectional_factor = 2 if enc_bidirectional else 1\n",
    "        self.decoder = Decoder(dec_vocab_size,enc_bidirectional_factor*encoder_hidden_dim,dec_hidden_dim=dec_hidden_dim,dec_output_dim=dec_vocab_size,emb_dim=dec_embed_dim)\n",
    "        self.device_train = device_train\n",
    "\n",
    "\n",
    "    def create_mask(self, src_lengths,max_src_length):\n",
    "\n",
    "        src_mask = torch.zeros((len(src_lengths),max_src_length),dtype=torch.int64)\n",
    "        for i in range(len(src_lengths)):\n",
    "\n",
    "            src_mask[i,src_lengths[i]:] = 1\n",
    "        return src_mask\n",
    "        \n",
    "    def forward(self,source,source_len,target,teacher_forcing_ratio = 0.0):\n",
    "        #   source = [batch_size, max_src_len]\n",
    "        #   source_len = [length of sentence in the batch]\n",
    "        #   target = [batch_size,traget_length]\n",
    "        #   teacher_forcing_ratio = probability to use teacher forcinbg\n",
    "\n",
    "        batch_size = source.shape[0]\n",
    "        target_length = target.shape[1]\n",
    "        target_vocab_size = self.decoder.vocab_size\n",
    "        outputs= torch.zeros(batch_size,target_length,target_vocab_size).to(self.device_train)\n",
    "        encoder_outputs , hidden = self.encoder(source,source_len)\n",
    "\n",
    "        inp = target[:,0]        \n",
    "        enc_mask = self.create_mask(source_len,int(encoder_outputs.shape[1]))\n",
    "        enc_mask = enc_mask.to(self.device_train)\n",
    "        for t in range(1,target_length):\n",
    "            decoder_output, hidden, attention_vector =  self.decoder(inp,hidden,encoder_outputs,enc_mask)\n",
    "\n",
    "            outputs[:,t,:] = decoder_output # batch_size, vocab_size\n",
    "            teacher_force = random.random() < teacher_forcing_ratio \n",
    "\n",
    "            top1 = decoder_output.argmax(1)\n",
    "\n",
    "            inp = target[:,t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_weights(model : TranslationModel):\n",
    "    \n",
    "    for name,param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data,mean=0,std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data,0)\n",
    "    \n",
    "    \n",
    "    nn.init.xavier_uniform_(model.encoder.fc_out.weight)\n",
    "    nn.init.normal_(model.decoder.attention.attn.weight, mean=0, std=0.001)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = TranslationModel()\n",
    "apply_weights(t)\n",
    "batch_data = next(iter(train_dataloader))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=output_vocab['<eos>'])\n",
    "optimizer = optim.Adam(t.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "model_output = t(batch_data['src'],batch_data['src_length'],batch_data['trg'])\n",
    "\n",
    "\n",
    "model_out_reshaped = model_output[1:].view(-1,model_output.shape[-1])\n",
    "reshaped_target = batch_data['trg'][1:].view(-1)\n",
    "loss_value = criterion(model_out_reshaped,reshaped_target)\n",
    "loss_value.backward()\n",
    "nn.utils.clip_grad_norm_(t.parameters(),5)\n",
    "optimizer.step()\n",
    "print(loss_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import  datetime\n",
    "\n",
    "import os\n",
    "\n",
    "def train_model(model,num_epochs,train_loader,valid_loader,optimizer,criterion,checkpoint_name='translation_model.pth',device_train = device_cpu):\n",
    "    \n",
    "    current_datetime = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    tensorboard_name =  checkpoint_name + current_datetime\n",
    "    writer = SummaryWriter('runs/' + tensorboard_name)\n",
    "\n",
    "    best_validation_loss = 1000.0\n",
    "    model = model.to(device_train)\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        training_loss = 0.0\n",
    "        model.train()\n",
    "        for i,batch in enumerate(train_loader):\n",
    "\n",
    "            source, source_length, target = batch['src'], batch['src_length'], batch['trg']\n",
    "\n",
    "            source = source.to(device_train)\n",
    "            target = target.to(device_train)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # model_output = [batch_size, output_seq_length,vocab_size]\n",
    "            model_output  = model(source,source_length,target)\n",
    "\n",
    "\n",
    "            source = source.to(device_cpu)\n",
    "            target = target.to(device_cpu)\n",
    "            model_output = model_output.to(device_cpu)\n",
    "            \n",
    "            model_out_reshaped = model_output[1:].view(-1,model_output.shape[-1]) \n",
    "            #print(model_out_reshaped.shape)\n",
    "            reshaped_target = target[1:].view(-1)\n",
    "            #print(reshaped_target.shape)\n",
    "\n",
    "            loss_value = criterion(model_out_reshaped,reshaped_target)\n",
    "            loss_value.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(),5)\n",
    "\n",
    "            optimizer.step()\n",
    "            training_loss += loss_value.item()\n",
    "        \n",
    "        print(\"Epoch \" + str(e) + \" Training Loss Value = \" + str(training_loss/len(train_loader)))\n",
    "        writer.add_scalars('Train/Loss vs Epoch',{'train' :training_loss/len(train_loader)},e)\n",
    "\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, batch in enumerate(valid_loader):\n",
    "                source, source_length, target = batch['src'], batch['src_length'], batch['trg']\n",
    "\n",
    "                source = source.to(device_train)\n",
    "                target = target.to(device_train)\n",
    "\n",
    "                model_output  = model(source,source_length,target,0)\n",
    "\n",
    "\n",
    "                source = source.to(device_cpu)\n",
    "                target = target.to(device_cpu)\n",
    "                model_output = model_output.to(device_cpu)\n",
    "                \n",
    "                model_out_reshaped = model_output[1:].view(-1,model_output.shape[-1])\n",
    "                reshaped_target = target[1:].view(-1)\n",
    "                loss_value = criterion(model_out_reshaped,reshaped_target)\n",
    "\n",
    "                validation_loss += loss_value.item()\n",
    "        averaged_validation_loss = validation_loss/len(valid_loader)\n",
    "        writer.add_scalars('Train/Loss vs Epoch',{'valid' :averaged_validation_loss},e)\n",
    "\n",
    "        print(\"Epoch \" + str(e) + \" Validation Loss Value = \" + str(averaged_validation_loss))\n",
    "        print(\"\")\n",
    "        if (averaged_validation_loss <= best_validation_loss):\n",
    "            best_validation_loss = averaged_validation_loss\n",
    "            torch.save(model.state_dict(),checkpoint_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss Value = 1.1532845988273621\n",
      "Epoch 0 Validation Loss Value = 0.5831969664210365\n",
      "\n",
      "Epoch 1 Training Loss Value = 0.4372729243040085\n",
      "Epoch 1 Validation Loss Value = 0.3482016736552829\n",
      "\n",
      "Epoch 2 Training Loss Value = 0.3581791228055954\n",
      "Epoch 2 Validation Loss Value = 0.2847760773840405\n",
      "\n",
      "Epoch 3 Training Loss Value = 0.26458987951278684\n",
      "Epoch 3 Validation Loss Value = 0.24891380989362324\n",
      "\n",
      "Epoch 4 Training Loss Value = 0.2414941673874855\n",
      "Epoch 4 Validation Loss Value = 0.23556175213011485\n",
      "\n",
      "Epoch 5 Training Loss Value = 0.23177989387512207\n",
      "Epoch 5 Validation Loss Value = 0.2293891329613943\n",
      "\n",
      "Epoch 6 Training Loss Value = 0.22964910048246384\n",
      "Epoch 6 Validation Loss Value = 0.22811415933427356\n",
      "\n",
      "Epoch 7 Training Loss Value = 0.22680952256917952\n",
      "Epoch 7 Validation Loss Value = 0.22600516203850035\n",
      "\n",
      "Epoch 8 Training Loss Value = 0.2256039871573448\n",
      "Epoch 8 Validation Loss Value = 0.22521047483360956\n",
      "\n",
      "Epoch 9 Training Loss Value = 0.2259898294210434\n",
      "Epoch 9 Validation Loss Value = 0.22831840292801933\n",
      "\n",
      "Epoch 10 Training Loss Value = 0.22555622398853303\n",
      "Epoch 10 Validation Loss Value = 0.227070956476151\n",
      "\n",
      "Epoch 11 Training Loss Value = 0.23245284301042557\n",
      "Epoch 11 Validation Loss Value = 0.22423041836609917\n",
      "\n",
      "Epoch 12 Training Loss Value = 0.22312434709072113\n",
      "Epoch 12 Validation Loss Value = 0.22411127601351058\n",
      "\n",
      "Epoch 13 Training Loss Value = 0.22261813122034074\n",
      "Epoch 13 Validation Loss Value = 0.2236154902549017\n",
      "\n",
      "Epoch 14 Training Loss Value = 0.22451747035980224\n",
      "Epoch 14 Validation Loss Value = 0.22381513293773408\n",
      "\n",
      "Epoch 15 Training Loss Value = 0.22284484076499939\n",
      "Epoch 15 Validation Loss Value = 0.22348501308569832\n",
      "\n",
      "Epoch 16 Training Loss Value = 0.2217968137860298\n",
      "Epoch 16 Validation Loss Value = 0.22479403704877884\n",
      "\n",
      "Epoch 17 Training Loss Value = 0.2222013781070709\n",
      "Epoch 17 Validation Loss Value = 0.22560606636698283\n",
      "\n",
      "Epoch 18 Training Loss Value = 0.22473962849378587\n",
      "Epoch 18 Validation Loss Value = 0.2262946610885953\n",
      "\n",
      "Epoch 19 Training Loss Value = 0.22379909002780915\n",
      "Epoch 19 Validation Loss Value = 0.22557390658628373\n",
      "\n",
      "Epoch 20 Training Loss Value = 0.22226842564344407\n",
      "Epoch 20 Validation Loss Value = 0.22303841511408487\n",
      "\n",
      "Epoch 21 Training Loss Value = 0.22114408200979233\n",
      "Epoch 21 Validation Loss Value = 0.22277068334912498\n",
      "\n",
      "Epoch 22 Training Loss Value = 0.22524513667821885\n",
      "Epoch 22 Validation Loss Value = 0.26362655654786127\n",
      "\n",
      "Epoch 23 Training Loss Value = 0.22569258111715318\n",
      "Epoch 23 Validation Loss Value = 0.22362668339222197\n",
      "\n",
      "Epoch 24 Training Loss Value = 0.22102309376001358\n",
      "Epoch 24 Validation Loss Value = 0.22313910911953638\n",
      "\n",
      "Epoch 25 Training Loss Value = 0.22077121704816818\n",
      "Epoch 25 Validation Loss Value = 0.22277802796590895\n",
      "\n",
      "Epoch 26 Training Loss Value = 0.2205759194493294\n",
      "Epoch 26 Validation Loss Value = 0.22380972972937993\n",
      "\n",
      "Epoch 27 Training Loss Value = 0.22050515311956406\n",
      "Epoch 27 Validation Loss Value = 0.22302092067779056\n",
      "\n",
      "Epoch 28 Training Loss Value = 0.2209068821668625\n",
      "Epoch 28 Validation Loss Value = 0.22924875148705073\n",
      "\n",
      "Epoch 29 Training Loss Value = 0.24083380442857744\n",
      "Epoch 29 Validation Loss Value = 0.22415897723228212\n",
      "\n",
      "Epoch 30 Training Loss Value = 0.22065118932723998\n",
      "Epoch 30 Validation Loss Value = 0.2229993880268127\n",
      "\n",
      "Epoch 31 Training Loss Value = 0.22024752366542816\n",
      "Epoch 31 Validation Loss Value = 0.2234445862353794\n",
      "\n",
      "Epoch 32 Training Loss Value = 0.21995991784334182\n",
      "Epoch 32 Validation Loss Value = 0.22481537905950394\n",
      "\n",
      "Epoch 33 Training Loss Value = 0.21995172280073166\n",
      "Epoch 33 Validation Loss Value = 0.22411191345207274\n",
      "\n",
      "Epoch 34 Training Loss Value = 0.2199119465947151\n",
      "Epoch 34 Validation Loss Value = 0.22359947339882927\n",
      "\n",
      "Epoch 35 Training Loss Value = 0.22380836844444274\n",
      "Epoch 35 Validation Loss Value = 0.2250209493296487\n",
      "\n",
      "Epoch 36 Training Loss Value = 0.2197598260641098\n",
      "Epoch 36 Validation Loss Value = 0.22354102323925684\n",
      "\n",
      "Epoch 37 Training Loss Value = 0.21990617209672927\n",
      "Epoch 37 Validation Loss Value = 0.22656199407009853\n",
      "\n",
      "Epoch 38 Training Loss Value = 0.21972091579437256\n",
      "Epoch 38 Validation Loss Value = 0.22458063893847996\n",
      "\n",
      "Epoch 39 Training Loss Value = 0.22062726098299026\n",
      "Epoch 39 Validation Loss Value = 0.2243202193861916\n",
      "\n",
      "Epoch 40 Training Loss Value = 0.21918323731422423\n",
      "Epoch 40 Validation Loss Value = 0.2244922835675497\n",
      "\n",
      "Epoch 41 Training Loss Value = 0.2204349566102028\n",
      "Epoch 41 Validation Loss Value = 0.22914474090886494\n",
      "\n",
      "Epoch 42 Training Loss Value = 0.2215315516591072\n",
      "Epoch 42 Validation Loss Value = 0.22484559080903493\n",
      "\n",
      "Epoch 43 Training Loss Value = 0.21873549008369444\n",
      "Epoch 43 Validation Loss Value = 0.22444036532016026\n",
      "\n",
      "Epoch 44 Training Loss Value = 0.2184119662642479\n",
      "Epoch 44 Validation Loss Value = 0.22458465279094755\n",
      "\n",
      "Epoch 45 Training Loss Value = 0.21821583193540572\n",
      "Epoch 45 Validation Loss Value = 0.22568954716599177\n",
      "\n",
      "Epoch 46 Training Loss Value = 0.21819252854585647\n",
      "Epoch 46 Validation Loss Value = 0.22592106532482875\n",
      "\n",
      "Epoch 47 Training Loss Value = 0.2194312198162079\n",
      "Epoch 47 Validation Loss Value = 0.2389403943504606\n",
      "\n",
      "Epoch 48 Training Loss Value = 0.22154972445964813\n",
      "Epoch 48 Validation Loss Value = 0.22620579316502526\n",
      "\n",
      "Epoch 49 Training Loss Value = 0.2176620051264763\n",
      "Epoch 49 Validation Loss Value = 0.22543432007706354\n",
      "\n",
      "Epoch 50 Training Loss Value = 0.21721447879076003\n",
      "Epoch 50 Validation Loss Value = 0.2266359996227991\n",
      "\n",
      "Epoch 51 Training Loss Value = 0.21833804559707642\n",
      "Epoch 51 Validation Loss Value = 0.231737558094282\n",
      "\n",
      "Epoch 52 Training Loss Value = 0.21825413191318513\n",
      "Epoch 52 Validation Loss Value = 0.22814129080091203\n",
      "\n",
      "Epoch 53 Training Loss Value = 0.2185845489501953\n",
      "Epoch 53 Validation Loss Value = 0.23054189436019412\n",
      "\n",
      "Epoch 54 Training Loss Value = 0.21679368668794632\n",
      "Epoch 54 Validation Loss Value = 0.2282372150156233\n",
      "\n",
      "Epoch 55 Training Loss Value = 0.21636871767044066\n",
      "Epoch 55 Validation Loss Value = 0.22819920168036506\n",
      "\n",
      "Epoch 56 Training Loss Value = 0.21627030903100966\n",
      "Epoch 56 Validation Loss Value = 0.22881333411686003\n",
      "\n",
      "Epoch 57 Training Loss Value = 0.2185711720585823\n",
      "Epoch 57 Validation Loss Value = 0.2278825905587938\n",
      "\n",
      "Epoch 58 Training Loss Value = 0.2165010307431221\n",
      "Epoch 58 Validation Loss Value = 0.22842066628592356\n",
      "\n",
      "Epoch 59 Training Loss Value = 0.21591862112283708\n",
      "Epoch 59 Validation Loss Value = 0.22892346103039998\n",
      "\n",
      "Epoch 60 Training Loss Value = 0.2154573394060135\n",
      "Epoch 60 Validation Loss Value = 0.22887912960279555\n",
      "\n",
      "Epoch 61 Training Loss Value = 0.2154950234889984\n",
      "Epoch 61 Validation Loss Value = 0.22957391587514725\n",
      "\n",
      "Epoch 62 Training Loss Value = 0.2184722096323967\n",
      "Epoch 62 Validation Loss Value = 0.23310393307890212\n",
      "\n",
      "Epoch 63 Training Loss Value = 0.21681498408317565\n",
      "Epoch 63 Validation Loss Value = 0.22943866678646632\n",
      "\n",
      "Epoch 64 Training Loss Value = 0.21514156192541123\n",
      "Epoch 64 Validation Loss Value = 0.23033437463972303\n",
      "\n",
      "Epoch 65 Training Loss Value = 0.21497661024332046\n",
      "Epoch 65 Validation Loss Value = 0.23064674082256498\n",
      "\n",
      "Epoch 66 Training Loss Value = 0.21458703851699829\n",
      "Epoch 66 Validation Loss Value = 0.231547778087949\n",
      "\n",
      "Epoch 67 Training Loss Value = 0.21452328556776046\n",
      "Epoch 67 Validation Loss Value = 0.2314697558444644\n",
      "\n",
      "Epoch 68 Training Loss Value = 0.2146718606352806\n",
      "Epoch 68 Validation Loss Value = 0.23188430047224437\n",
      "\n",
      "Epoch 69 Training Loss Value = 0.2228137484192848\n",
      "Epoch 69 Validation Loss Value = 0.23165127871528504\n",
      "\n",
      "Epoch 70 Training Loss Value = 0.21797902661561966\n",
      "Epoch 70 Validation Loss Value = 0.2316356977773091\n",
      "\n",
      "Epoch 71 Training Loss Value = 0.21496852707862854\n",
      "Epoch 71 Validation Loss Value = 0.2314910888671875\n",
      "\n",
      "Epoch 72 Training Loss Value = 0.2143518922328949\n",
      "Epoch 72 Validation Loss Value = 0.23169639990443275\n",
      "\n",
      "Epoch 73 Training Loss Value = 0.21426143372058867\n",
      "Epoch 73 Validation Loss Value = 0.23214164494522035\n",
      "\n",
      "Epoch 74 Training Loss Value = 0.21420726042985916\n",
      "Epoch 74 Validation Loss Value = 0.2318412715953494\n",
      "\n",
      "Epoch 75 Training Loss Value = 0.21412023657560347\n",
      "Epoch 75 Validation Loss Value = 0.2328794276903546\n",
      "\n",
      "Epoch 76 Training Loss Value = 0.21409514528512955\n",
      "Epoch 76 Validation Loss Value = 0.23360862287264023\n",
      "\n",
      "Epoch 77 Training Loss Value = 0.2140947706103325\n",
      "Epoch 77 Validation Loss Value = 0.23354891723110563\n",
      "\n",
      "Epoch 78 Training Loss Value = 0.21415472340583802\n",
      "Epoch 78 Validation Loss Value = 0.23405992133276804\n",
      "\n",
      "Epoch 79 Training Loss Value = 0.21497516757249832\n",
      "Epoch 79 Validation Loss Value = 0.23418584962685904\n",
      "\n",
      "Epoch 80 Training Loss Value = 0.22365692377090454\n",
      "Epoch 80 Validation Loss Value = 0.2305690355244137\n",
      "\n",
      "Epoch 81 Training Loss Value = 0.21648856496810914\n",
      "Epoch 81 Validation Loss Value = 0.23198472483763619\n",
      "\n",
      "Epoch 82 Training Loss Value = 0.2150345922112465\n",
      "Epoch 82 Validation Loss Value = 0.2325417659585438\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63598/1990177444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_fast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mapply_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdevice_fast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'translation_model_4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_63598/2593114935.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, train_loader, valid_loader, optimizer, criterion, checkpoint_name, device_train)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# model_output = [batch_size, output_seq_length,vocab_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mmodel_output\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/depressedcoder/anaconda3/envs/pytorchenv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_63598/406695933.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, source_len, target, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0menc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0menc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_63598/406695933.py\u001b[0m in \u001b[0;36mcreate_mask\u001b[0;34m(self, src_lengths, max_src_length)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0msrc_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_lengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = TranslationModel(device_train=device_fast)\n",
    "apply_weights(t)\n",
    "train_model(t,150,train_dataloader,valid_dataloader,optim.Adam(t.parameters(),lr=0.001),nn.CrossEntropyLoss(),device_train= device_fast,checkpoint_name='translation_model_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TranslationModel()\n",
    "t.load_state_dict(torch.load('./translation_model_4'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, max_len = 10,glove=glove):\n",
    "\n",
    "    model.eval()\n",
    "    nl_date = sentence.replace(\"\\'\",\"\").strip()\n",
    "    \n",
    "    nl_date = nl_date.lower()\n",
    "\n",
    "    if \"/\" in nl_date:\n",
    "        #number_list = list(nl_date)\n",
    "        #nl_date = \" \".join(number_list)\n",
    "        split_on_slash = nl_date.split(\"/\")\n",
    "        nl_date = \" / \".join(split_on_slash)\n",
    "\n",
    "        \n",
    "    embeddings = []\n",
    "    for word in nl_date.split(' '):\n",
    "        if not TRAINABLE_EMBEDDINGS:\n",
    "            embeddings.append(glove[word])\n",
    "        else:\n",
    "            if word not in glove.stoi:\n",
    "                embeddings.append(glove.stoi['unk'])\n",
    "            else:\n",
    "                embeddings.append(glove.stoi[word])\n",
    "        #embeddings.append(torch.tensor(fasttext_model.get_word_vector(word)))\n",
    "\n",
    "\n",
    "    current_inp_length = len(embeddings)\n",
    "    if not TRAINABLE_EMBEDDINGS:\n",
    "        inp_embeddings = torch.stack(embeddings)\n",
    "    else:\n",
    "        inp_embeddings = torch.tensor(embeddings)\n",
    "    inp_embeddings = inp_embeddings.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        encoder_outputs, hidden = model.encoder(inp_embeddings, [current_inp_length])\n",
    "        mask = model.create_mask([current_inp_length],max([current_inp_length]))\n",
    "        attentions = torch.zeros(max_len, 1, inp_embeddings.shape[1])\n",
    "        trg_indexes = []\n",
    "        for i in range(max_len):\n",
    "\n",
    "            if i==0:\n",
    "                trg_tensor = torch.LongTensor([output_vocab['<sos>']])\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                # input,hidden,encoder_outputs,encoder_length_mask\n",
    "                output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\n",
    "\n",
    "            attentions[i] = attention\n",
    "            \n",
    "            pred_token = output.argmax(1).item()\n",
    "        \n",
    "            trg_indexes.append(pred_token)\n",
    "            trg_tensor = torch.LongTensor([pred_token])\n",
    "\n",
    "            if pred_token == output_vocab['<eos>']:\n",
    "                break\n",
    "    \n",
    "    trg_tokens = [output_index_to_word[i] for i in trg_indexes]\n",
    "    return trg_tokens, attentions\n",
    "    #print(trg_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '0', '1', '9', '-', '0', '4', '-', '2', '1']\n"
     ]
    }
   ],
   "source": [
    "output_tokens , attention = translate_sentence('21/04/2019',t.to(device_cpu))\n",
    "print(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmetrics(model,sentences,ground_truths):\n",
    "    exact_match_count = 0\n",
    "    per_word_matches = [0 for i in range(10)]\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "\n",
    "        trg_tokens, attention_weights = translate_sentence(sentences[i],model)\n",
    "        print(trg_tokens)\n",
    "        ground_truth_tokens = list(ground_truths[i])\n",
    "\n",
    "        exact_match = True\n",
    "        for i in range(len(ground_truth_tokens)):\n",
    "\n",
    "            if trg_tokens[i] == ground_truth_tokens[i]:\n",
    "                per_word_matches[i]+=1\n",
    "            else:\n",
    "                exact_match = False\n",
    "        \n",
    "        if exact_match:\n",
    "            exact_match_count+=1\n",
    "\n",
    "    \n",
    "    number = len(sentences)\n",
    "    per_output_accuracy = [ 0.0 for i in range(10)]\n",
    "    exact_match_accuracy = ((1.0*exact_match_count)/number)*100\n",
    "\n",
    "    for i in range(len(per_word_matches)):\n",
    "\n",
    "        per_output_accuracy[i] = ((1.0*per_word_matches[i]) / number) * 100\n",
    "\n",
    "    return exact_match_accuracy,per_output_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '9', '9', '9', '-', '0', '1', '-', '0', '4']\n",
      "['2', '0', '0', '9', '-', '0', '5', '-', '2', '0']\n",
      "['2', '0', '1', '2', '-', '0', '9', '-', '2', '5']\n",
      "['2', '0', '5', '6', '-', '0', '5', '-', '0', '1']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(75.0, [100.0, 100.0, 100.0, 75.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = ['1998-01-04','2009-05-20','2012-09-25','2056-05-01']\n",
    "sentences = ['4 jan 1998','May 20 2009','September 25 2012','01/05/56']\n",
    "\n",
    "getmetrics(t,sentences,ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap(sentence, translation, attention):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(attention, cmap='viridis')\n",
    "\n",
    "    nl_date = sentence.replace(\"\\'\",\"\").strip()\n",
    "    nl_date = nl_date.lower()\n",
    "\n",
    "    if \"/\" in nl_date:\n",
    "        #number_list = list(nl_date)\n",
    "        #nl_date = \" \".join(number_list)\n",
    "        split_on_slash = nl_date.split(\"/\")\n",
    "        nl_date = \" / \".join(split_on_slash)\n",
    "\n",
    "    src = nl_date.split(\" \")\n",
    "    trg = list(translation)\n",
    "\n",
    "    ax.set_xticklabels(src, minor=False, rotation='vertical')\n",
    "    ax.set_yticklabels(trg, minor=False)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    # and the x-ticks on top\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(np.arange(attention.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(attention.shape[0]) + 0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    plt.colorbar(heatmap)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63598/2566052727.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(src, minor=False, rotation='vertical')\n",
      "/tmp/ipykernel_63598/2566052727.py:19: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(trg, minor=False)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAAHBCAYAAACIQ9ldAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAt6klEQVR4nO3df3RU9Z3/8dckmAlIEkVIAjLGCFriRqgEQYJRpBpOtFhrRVZ2ASXxGIJwQqq2+dICy1Zy7LEcai2pWMXSRTb+KP6oWTEeq8TlsJYQtV0soKCJmhiJmkF+TGTmfv+AzDomgUzmx53h83yc8zm7c+f+eLuz+s77/fncex2WZVkCAABGSLA7AAAAED0kfgAADELiBwDAICR+AAAMQuIHAMAgJH4AAAxC4gcAwCAkfgAADELiBwDAICR+AAAMQuIHAMAgJH4A/TZ16lRt2LBBR44csTsUAH1E4gfQb3l5ebr33nuVmZmpO+64Q9u3b7c7JACnQOIH0G+/+tWv9PHHH2vDhg367LPPdOWVV+riiy/WAw88oE8//dTu8AD0wMFreQGEy2effaaHH35Y9913n7xer6677jotXrxY06ZNszs0ACdQ8QMIizfffFPLli3TAw88oPT0dFVWVio9PV0zZszQ3XffbXd4AE6g4gfQb21tbfrjH/+o9evXa+/evZoxY4ZKSko0ffp0ORwOSdIrr7yiG2+8UV999ZXN0QKQpAF2BwAgfo0cOVKjRo3S/Pnzddttt2nYsGHd9pk4caIuu+wyG6ID0BMqfgD9Vl9fr4KCArvDABAEEj8AAAah1Q8gJE8//bSefPJJNTU1qbOzM+C7nTt32hQVgN6wqh9Avz344IO6/fbblZ6ersbGRk2cOFHnnHOO9u3bp6KiIrvDA9ADWv0A+m3MmDFavny5br31VqWkpOjtt9/WBRdcoGXLlunzzz/XQw89ZHeIAL6Fih9AvzU1NSk/P1+SNHDgQB08eFCSNGfOHG3atMnO0AD0gsQPoN8yMzPV3t4uScrKyvI/q3///v2imQjEJhI/gH6bNm2aXnjhBUlScXGxlixZomuvvVazZs3SD3/4Q5ujA9AT5viD9OWXX+qpp55SU1OTsrKyNHPmTKWlpdkdFmALn88nn8+nAQOO3yD05JNP6o033tDo0aNVWlqqpKQkmyME8G0k/lO4+eabNXv2bN10003atWuXrrrqKjkcDl1wwQX64IMP5HA49OqrryonJ8fuUAEAOCUS/ykMGzZM27Zt04UXXqjrrrtOZ599ttavX6+kpCR9/fXXWrBggZqbm7Vlyxa7QwWi4p133unzvmPHjo1gJAD6g8R/CoMGDdLf/vY3jRo1SiNGjNCLL76oSy+91P/9nj17NHHiRH355Zf2BQlEUUJCghwOhyzL8r+IpzderzdKUQHoKxb3ncLYsWP16quvSjq+gvnDDz8M+P7DDz/UwIED7QgNsMX+/fu1b98+7d+/X88884yys7O1du1aNTY2qrGxUWvXrtWoUaP0zDPP2B0qgB7wyN5T+PnPf665c+fqjDPO0OLFi7VkyRK1t7crJydHu3fv1vLlyzVnzhy7wwSiJisry/+/z5w5Uw8++KCuu+46/7axY8fK5XLp5z//uW688UYbIgRwMrT6++CZZ55ReXm5Pvnkk4B7k51Op0pLS/XAAw8oMTHRxggBewwcOFA7d+7strj13Xff1fjx43XkyBGbIjPT1KlTNX/+fM2cOZNOJHpF4u8jr9ernTt3at++ffL5fBo+fLjy8vKUkpJid2iAbcaPH6+cnBw9+uijSk5OliR5PB7Nnz9f7777Li/pibIf//jH2rhxo44cOaJbbrlFxcXFuvzyy+0OCzGGxA+g3958803NmDFDPp9P48aNkyS9/fbbcjgc+vOf/6yJEyfaHKF5vF6v/vznP2v9+vWqra3V6NGjNX/+fM2ZM0cZGRl2h4cYQOIP0RdffKEXXnhBc+fOtTsUwBaHDx/Wf/zHf+gf//iHLMvSxRdfrNmzZ+vMM8+0OzTjffbZZ3r44Yd13333yev16rrrrtPixYs1bdo0u0ODjUj8IXr77bc1fvx4blsCEFPefPNNrV+/Xps2bVJaWppuu+02tbS0aOPGjVqwYIEeeOABu0OETUj8p+B2u0/6/TvvvKOrrrqKxA9j7dmzR6+99pra2trk8/kCvlu2bJlNUZmpra1Nf/zjH7V+/Xrt3btXM2bMUElJiaZPn+5/5sIrr7yiG2+8UV999ZXN0cIuJP5T6HpYSW+6HmJC4oeJHnnkES1YsEBDhw5VZmZmwL8rDoeDxX1RlpSUpFGjRmn+/Pm67bbbNGzYsG77uN1u/eAHP9Bf/vIXGyJELCDxn0JaWpqWLl2qSZMm9fj93r17deedd5L4YaSsrCyVlZXpJz/5id2hQFJ9fb0KCgrsDgMxjgf4nML48eMlSVdddVWP35911lm8dxzG+uKLLzRz5ky7w8AJJH30BYn/FGbPnn3Sh5BkZmZq+fLlUYwIiB0zZ87Uyy+/rNLSUrtDwQlPP/20nnzySTU1NamzszPgO6ZeINHqBxCCqqoqrV69Wtdff70uueQSnXHGGQHfL1682KbIzPTggw9q6dKlmjdvnh555BHdfvvtev/99/XXv/5VCxcu1H333Wd3iIgBJH4A/Zadnd3rdw6HQ/v27YtiNBgzZoyWL1+uW2+9VSkpKXr77bd1wQUXaNmyZfr888/10EMP2R0iYgCJvw8OHTqkJ554Qtu2bVNra6scDocyMjI0ZcoU3XrrrTyoJIr4LYDeDRo0SO+++66ysrKUnp6uuro6jRs3Tnv37tXll1+u9vZ2u0NEDOC1vKewa9cuXXTRRbr33nv1xRdf6LzzztPIkSP1xRdf6J577tF3vvMd7dq1y+4wjcBvAZxcZmamP7lnZWVp+/btko6/SpkaD12o+E/h6quvVmZmpv7whz8oKSkp4LvOzk7/07C4Jzby+C1iz/z580/6/WOPPRalSCBJJSUlcrlcWr58uX73u9+poqJCU6ZM0Y4dO3TTTTfp0UcftTtExAAS/ykMGjRIO3bs0MUXX9zj93//+981ceJEHT58OMqRmYffIvb88Ic/DPj89ddf6+9//7u+/PJLTZs2TX/6059sisxMPp9PPp9PAwYcv2HrqaeeUn19vUaPHq0FCxZ0W3wJM3E73ymcffbZ2rt3b6/J5r333tPZZ58d5ajMxG8RezZv3txtm8/nU1lZmS644AIbIjJbQkKCOjs7tXPnTrW1tcnpdOqaa66RJL300kuaMWOGzREiFpD4T+GOO+7QvHnz9LOf/UzXXnutMjIy5HA41Nraqrq6Oq1atUrl5eV2h2kEfov4kJCQoCVLlmjq1Km699577Q7HKC+99JLmzJnT4yI+Hi2OLrT6++D+++/Xr3/9a/8qcun4M/ozMzNVXl7Of9yiiN8iPtTW1mrevHn67LPP7A7FKKNHj9b06dO1bNkyZWRk2B0OYhSJPwj79+9Xa2urpOOrZ092DzMii98iNlRUVAR8tixLLS0tevHFFzVv3jzuG4+y1NRUNTY2atSoUXaHghhG4gfQb1dffXXA54SEBA0bNkzTpk3T/Pnz/YvMEB3z58/XlClTVFxcbHcoiGEk/j44cuSIGhoaNGTIkG4Ly44ePaonn3xSc+fOtSk6s/BbxJbDhw/Lsiz/g5M++OADPfvss8rJydH06dNtjs48hw8f1syZMzVs2DAeoYxekfhPYc+ePSosLFRTU5McDocKCgq0adMmDR8+XJL06aefasSIESyaiQJ+i9hTWFiom266SaWlpfryyy81ZswYnXHGGTpw4IBWr16tBQsW2B2iUX7/+9+rtLRUAwcO1DnnnONfByPxCGX8H57cdwo/+clPdMkll6itrU27d+9WamqqpkyZoqamJrtDMw6/RezZuXOn/1WwTz/9tDIyMvThhx9qw4YNevDBB22Ozjw/+9nPtHLlSnV0dOiDDz7Q/v37/YOkjy4k/lPYtm2bVq1apaFDh2r06NF6/vnnVVRUpIKCAv5FijJ+i9hz+PBhpaSkSJJefvll3XTTTUpISNDll1+uDz/80ObozNPZ2alZs2YpIYH/tKN3/H/HKRw5cqTbAqXf/va3uuGGG3TVVVdpz549NkVmHn6L2DN69Gg9++yzam5u1pYtW1RYWChJamtrU2pqqs3RmWfevHmqqamxOwzEOJbcnsKYMWO0Y8cO5eTkBGz/zW9+I8uydMMNN9gUmXn4LWLPsmXLNHv2bC1ZskTf+973NHnyZEnHq/9LL73U5ujM4/V69ctf/lJbtmzR2LFjuy3uW716tU2RIZawuO8UqqqqVF9fr9ra2h6/Lysr0+9+9zv5fL4oR2YefovY1NraqpaWFo0bN87fYn7zzTeVmpqqMWPG2BydWb59e+U3ORwOvfrqq1GMBrGKxA8AgEGY4wcAwCAkfgAADELi7wePx6MVK1bI4/HYHYrx+C1iC79H7OC3QG+Y4+8Ht9uttLQ0dXR0cMuSzfgtYgu/R+zgt0BvqPgBADAIiR8AAINE/QE+Pp9Pn3zyiVJSUgJeIBFP3G53wP+EffgtYgu/R+w4HX4Ly7J08OBBjRgxIqKPIT569Kg6OzvDcq6kpCQlJyeH5VyREvU5/o8++kgulyualwQAxLHm5maNHDkyIuc+evSosrMGq7UtPG/1zMzM1P79+2M6+Ue94u96ocf+nVlKHcxMg51+dNFYu0MAYpOD/zbFgmPW13pDf/bnjUjo7OxUa5tXHzacr9SU0H5390GfsvI+UGdnJ4n/m7ra+6mDE0L+PzJCM8Bxxql3AkxE4o8dlqIyLTw4xaHBKaFdx6f4mL7mJT0AAON5LZ+8IU58e634eE8If9YCAGAQKn4AgPF8suRTaCV/qMdHC4kfAGA8n3wKtVEf+hmig1Y/AAAGoeIHABjPa1nyhvhYm1CPjxYSPwDAeCbN8dPqBwDAIFT8AADj+WTJa0jFT+IHABjPpFY/iR8AYDyTFvcxxw8AgEGCSvxVVVW67LLLlJKSovT0dN14443avXt3pGIDACAqfGEa8SCoxP/6669r4cKF2r59u+rq6nTs2DEVFhbq0KFDkYoPAICI855Y3BfqiAdBzfG/9NJLAZ/Xr1+v9PR0NTQ06MorrwxrYAAAIPxCWtzX0dEhSRoyZEiv+3g8Hnk8Hv9nt9sdyiUBAAg7r6UwvJY3PLFEWr8X91mWpYqKCl1xxRXKzc3tdb+qqiqlpaX5h8vl6u8lAQCICOb4++Cuu+7SO++8o02bNp10v8rKSnV0dPhHc3Nzfy8JAABC1K9W/6JFi/T8889r69atGjly5En3dTqdcjqd/QoOAIBo8MkhrxwhnyMeBJX4LcvSokWLtHnzZr322mvKzs6OVFwAAESNzzo+Qj1HPAgq8S9cuFBPPPGEnnvuOaWkpKi1tVWSlJaWpoEDB0YkQAAAIs0bhoo/1OOjJag5/urqanV0dGjq1KkaPny4f9TU1EQqPgAAEEZBt/oBADjdmFTx85IeAIDxfJZDPivExX0hHh8tvKQHAACDUPEDAIxHqx8AAIN4lSBviE1wb5hiiTRa/QAAGISKHwBgPCsMi/usOFncR+IHABjPpDl+Wv0AABiEih8AYDyvlSCvFeLivjh5xh2JHwBgPJ8c8oXYBPcpPjI/iR8AYDyT5vhtS/wJciiBJQYAYpHlszsCSPwOEULFDwAwXnjm+Gn1AwAQF47P8Yf4kp44afXTawcAwCBU/AAA4/nC8Kx+VvUDABAnTJrjp9UPAIBBqPgBAMbzKYEH+AAAYAqv5ZA3xLfrhXp8tNDqBwDAIFT8AADjecOwqt9Lqx8AgPjgsxLkC3FVvy9OVvWT+AEAxjOp4meOHwAAg1DxAwCM51Poq/Lj5V2C/ar4165dq+zsbCUnJysvL0/19fXhjgsAgKjpuo8/1BEPgo6ypqZG5eXlWrp0qRobG1VQUKCioiI1NTVFIj4AABBGQSf+1atXq7i4WCUlJcrJydGaNWvkcrlUXV0difgAAIi4rmf1hzriQVBRdnZ2qqGhQYWFhQHbCwsLtW3bth6P8Xg8crvdAQMAgFjikyMsIx4ElfgPHDggr9erjIyMgO0ZGRlqbW3t8ZiqqiqlpaX5h8vl6n+0AAAgJP3qSzgcgX/VWJbVbVuXyspKdXR0+Edzc3N/LgkAQMSY1OoP6na+oUOHKjExsVt139bW1q0L0MXpdMrpdPY/QgAAIiw8D/CJj8QfVJRJSUnKy8tTXV1dwPa6ujrl5+eHNTAAABB+QT/Ap6KiQnPmzNGECRM0efJkrVu3Tk1NTSotLY1EfAAARJzPcsgX6gN84uS1vEEn/lmzZqm9vV0rV65US0uLcnNzVVtbq6ysrEjEBwBAxPnC0OqPlwf49OuRvWVlZSorKwt3LAAA2CI8b+eLj8QfH1ECAICw4CU9AADjeeWQN8QH8IR6fLSQ+AEAxqPVDwAATktU/AAA43kVeqveG55QIo7EDwAwHq1+AABwWiLxAwCMZ9dLetauXavs7GwlJycrLy9P9fX1J91/48aNGjdunAYNGqThw4fr9ttvV3t7e1DXJPEDAIxnySFfiMMKco1ATU2NysvLtXTpUjU2NqqgoEBFRUVqamrqcf833nhDc+fOVXFxsf73f/9XTz31lP7617+qpKQkqOuS+AEAsMHq1atVXFyskpIS5eTkaM2aNXK5XKquru5x/+3bt+v888/X4sWLlZ2drSuuuEJ33nmnduzYEdR1SfwAAOOFs9XvdrsDhsfj6Xa9zs5ONTQ0qLCwMGB7YWGhtm3b1mOM+fn5+uijj1RbWyvLsvTpp5/q6aef1vXXXx/UP6ttq/p9suSTz67LA0DvHNREsSFBsqJzpXC+nc/lcgVsX758uVasWBGw7cCBA/J6vcrIyAjYnpGRodbW1h7Pn5+fr40bN2rWrFk6evSojh07phtuuEG/+c1vgoqT2/kAAMbzhuHtfF3HNzc3KzU11b/d6XT2eozDEfjHhmVZ3bZ12bVrlxYvXqxly5Zp+vTpamlp0T333KPS0lI9+uijfY6TxA8AQBilpqYGJP6eDB06VImJid2q+7a2tm5dgC5VVVWaMmWK7rnnHknS2LFjdeaZZ6qgoEC/+MUvNHz48D7FRz8LAGC8rlZ/qKOvkpKSlJeXp7q6uoDtdXV1ys/P7/GYw4cPKyEhMG0nJiZKOt4p6CsqfgCA8XxKkC/EWjjY4ysqKjRnzhxNmDBBkydP1rp169TU1KTS0lJJUmVlpT7++GNt2LBBkjRjxgzdcccdqq6u9rf6y8vLNXHiRI0YMaLP1yXxAwBgg1mzZqm9vV0rV65US0uLcnNzVVtbq6ysLElSS0tLwD39t912mw4ePKiHHnpIP/7xj3XWWWdp2rRpuv/++4O6rsMKpj8QBm63W2lpaWrfk63UFGYa7FQ04lK7QwBiE6v6Y8Ix62u9Zm1WR0fHKefM+6srJy2ov0nOwWeEdC7PV1+ruuBPEY03HKj4AQDGC+ftfLGOP2sBADAIFT8AwHhWGF7La8XJa3lJ/AAA43nlkDfIl+z0dI54EB9/ngAAgLCg4gcAGM9nhb44zxfVe+T6j8QPADCeLwxz/KEeHy0kfgCA8XxyyBfiHH2ox0dL0H+ebN26VTNmzNCIESPkcDj07LPPRiAsAAAQCUEn/kOHDmncuHF66KGHIhEPAABR57UcYRnxIOhWf1FRkYqKiiIRCwAAtmCOP4w8Ho88Ho//s9vtjvQlAQBALyL+50lVVZXS0tL8w+VyRfqSAAAExSeH/3n9/R6n6+K+YFVWVqqjo8M/mpubI31JAACCYp1Y1R/KsOIk8Ue81e90OuV0OiN9GQAA0Afcxw8AMJ5Jr+UNOvF/9dVXeu+99/yf9+/fr7feektDhgzReeedF9bgAACIBlb1n8SOHTt09dVX+z9XVFRIkubNm6fHH388bIEBAIDwCzrxT506VZYVJ28iAACgD2j1AwBgEJOe1U/iBwAYz6SKPz5WIgAAgLCg4gcAGM+kip/EDwAwnkmJn1Y/AAAGoeIHABjPpIqfxA8AMJ6l0G/Hi5cn3NDqBwDAIFT8AADj0eoHAMAgJP4o+OHuIg0402nX5SEpMfETu0PANzgSE+0OASe4bxpvdwiQdOzro9LTm+0O47RDxQ/EAJI+YC8qfgAADELiBwDAIJblkBVi4g71+Gjhdj4AAAxCxQ8AMJ5PjpAf4BPq8dFC4gcAGM+kOX5a/QAAGISKHwBgPJMW95H4AQDGo9UPAABOS1T8AADj0eoHAMAgVhha/fGS+Gn1AwBgECp+AIDxLEmWFfo54kHQFf/HH3+sf/3Xf9U555yjQYMG6bvf/a4aGhoiERsAAFHR9eS+UEc8CKri/+KLLzRlyhRdffXV+q//+i+lp6fr/fff11lnnRWh8AAAiDwW9/Xi/vvvl8vl0vr16/3bzj///HDHBAAAIiSoVv/zzz+vCRMmaObMmUpPT9ell16qRx555KTHeDweud3ugAEAQCzpeoBPqCMeBJX49+3bp+rqal144YXasmWLSktLtXjxYm3YsKHXY6qqqpSWluYfLpcr5KABAAgnywrPiAdBJX6fz6fx48dr1apVuvTSS3XnnXfqjjvuUHV1da/HVFZWqqOjwz+am5tDDhoAAPRPUHP8w4cP18UXXxywLScnR88880yvxzidTjmdzv5FBwBAFLC4rxdTpkzR7t27A7bt2bNHWVlZYQ0KAIBoMinxB9XqX7JkibZv365Vq1bpvffe0xNPPKF169Zp4cKFkYoPAACEUVCJ/7LLLtPmzZu1adMm5ebm6t///d+1Zs0a/cu//Euk4gMAIOJMWtUf9CN7v//97+v73/9+JGIBAMAW4ViVf1qu6gcAAPGNl/QAAIx3vOIPdXFfmIKJMBI/AMB4Jq3qJ/EDAIxnKfTX6sZJwc8cPwAAJqHiBwAYj1Y/AAAmMajXT6sfAACbrF27VtnZ2UpOTlZeXp7q6+tPur/H49HSpUuVlZUlp9OpUaNG6bHHHgvqmlT8AACEodWvII+vqalReXm51q5dqylTpujhhx9WUVGRdu3apfPOO6/HY2655RZ9+umnevTRRzV69Gi1tbXp2LFjQV2XxA8AMJ4dT+5bvXq1iouLVVJSIklas2aNtmzZourqalVVVXXb/6WXXtLrr7+uffv2aciQIZKk888/P+g4afUDABBGbrc7YHg8nm77dHZ2qqGhQYWFhQHbCwsLtW3bth7P+/zzz2vChAn65S9/qXPPPVcXXXSR7r77bh05ciSo+Gyr+JPmHdGABK9dl4ekgzMm2B0CvsH5eafdIeCE1ny7I4Ak+Y5Iejo61wrnqn6XyxWwffny5VqxYkXAtgMHDsjr9SojIyNge0ZGhlpbW3s8/759+/TGG28oOTlZmzdv1oEDB1RWVqbPP/88qHl+Wv1ADCDpAzazHEHP0fd4DknNzc1KTU31b3Y6nb0e4nAEXtOyrG7buvh8PjkcDm3cuFFpaWmSjk8X3Hzzzfrtb3+rgQMH9ilMWv0AAIRRampqwOgp8Q8dOlSJiYndqvu2trZuXYAuw4cP17nnnutP+pKUk5Mjy7L00Ucf9Tk+Ej8AwHhdi/tCHX2VlJSkvLw81dXVBWyvq6tTfn7Pc01TpkzRJ598oq+++sq/bc+ePUpISNDIkSP7fG0SPwAAVphGECoqKvT73/9ejz32mN59910tWbJETU1NKi0tlSRVVlZq7ty5/v1nz56tc845R7fffrt27dqlrVu36p577tH8+fP73OaXmOMHAMCWR/bOmjVL7e3tWrlypVpaWpSbm6va2lplZWVJklpaWtTU1OTff/Dgwaqrq9OiRYs0YcIEnXPOObrlllv0i1/8IqjrkvgBALBJWVmZysrKevzu8ccf77ZtzJgx3aYHgkXiBwBAiptn7YeKxA8AMJ5Jb+djcR8AAAah4gcAwKDX8pL4AQCQ48QI9Ryxj1Y/AAAGoeIHAIBWPwAABjEo8dPqBwDAIFT8AACE8bW8sY7EDwAwXrBv1+vtHPEg4onf4/HI4/H4P7vd7khfEgCA4DDH3zcbN27U4MGD/aO+vr7bPlVVVUpLS/MPl8sVyiUBAEAIQqr4b7jhBk2aNMn/+dxzz+22T2VlpSoqKvyf3W43yR8AEFuY4++blJQUpaSknHQfp9Mpp9MZymUAAIgoh3V8hHqOeMDtfAAAGIRV/QAAGLS4j8QPAIBBc/y0+gEAMAgVPwAAtPoBADCIQYmfVj8AAAah4gcAwKCKn8QPAIBBq/pJ/AAA4/HkPgAAcFqi4gcAwKA5fip+AAAMQuIHAMAgtPoBAMZzKAyL+8ISSeTZlviPff655DjDrstD0uC/HLM7BHyDddRjdwg44cU/vmp3CJD01UGfJt0bpYsZdDsfrX4gBpD0AUQLrX4AAAxa1U/iBwDAoMRPqx8AAINQ8QMAjGfSI3tJ/AAAGNTqJ/EDAGBQ4meOHwAAg1DxAwCMxxw/AAAm4cl9AADgdETFDwCAQYv7SPwAAOOZNMdPqx8AAIP0K/GvXbtW2dnZSk5OVl5enurr68MdFwAA0WOFacSBoBN/TU2NysvLtXTpUjU2NqqgoEBFRUVqamqKRHwAAESe9X/t/v6O0zbxr169WsXFxSopKVFOTo7WrFkjl8ul6urqSMQHAADCKKjE39nZqYaGBhUWFgZsLyws1LZt23o8xuPxyO12BwwAAGIKrf6eHThwQF6vVxkZGQHbMzIy1Nra2uMxVVVVSktL8w+Xy9X/aAEAiAQS/8k5HIFPJ7Isq9u2LpWVlero6PCP5ubm/lwSAICICXV+Pxy3A0ZLUPfxDx06VImJid2q+7a2tm5dgC5Op1NOp7P/EQIAgLAJquJPSkpSXl6e6urqArbX1dUpPz8/rIEBAIDwC/rJfRUVFZozZ44mTJigyZMna926dWpqalJpaWkk4gMAIPJ4ZG/vZs2apfb2dq1cuVItLS3Kzc1VbW2tsrKyIhEfAAAIo349q7+srExlZWXhjgUAAFuY9Kx+XtIDAIAUN636UPGSHgAADELFDwAAi/sAADCHSXP8tPoBADAIFT8AALT6AQAwB61+AABMYtPb+dauXavs7GwlJycrLy9P9fX1fTruv//7vzVgwAB997vfDfqaJH4AAGxQU1Oj8vJyLV26VI2NjSooKFBRUZGamppOelxHR4fmzp2r733ve/26LokfAAAbKv7Vq1eruLhYJSUlysnJ0Zo1a+RyuVRdXX3S4+68807Nnj1bkydPDu6CJ5D4AQDG65rjD3VIktvtDhgej6fb9To7O9XQ0KDCwsKA7YWFhdq2bVuvca5fv17vv/++li9f3u9/Vhb3Gcz6+mu7Q0CXxARZPfzHAfY4KyFOVmmd5hLj9HdwuVwBn5cvX64VK1YEbDtw4IC8Xq8yMjICtmdkZKi1tbXH8+7du1c//elPVV9frwED+p++SfxADCDpAzYL4+18zc3NSk1N9W92Op29HuJwOAJPYVndtkmS1+vV7Nmz9W//9m+66KKLQgqTxA8AQBgTf2pqakDi78nQoUOVmJjYrbpva2vr1gWQpIMHD2rHjh1qbGzUXXfdJUny+XyyLEsDBgzQyy+/rGnTpvUpTOb4AQCIsqSkJOXl5amuri5ge11dnfLz87vtn5qaqr/97W966623/KO0tFTf+c539NZbb2nSpEl9vjYVPwDAeHY8wKeiokJz5szRhAkTNHnyZK1bt05NTU0qLS2VJFVWVurjjz/Whg0blJCQoNzc3IDj09PTlZyc3G37qZD4AQCw4ZG9s2bNUnt7u1auXKmWlhbl5uaqtrZWWVlZkqSWlpZT3tPfHw7LsqK6bNLtdistLU1T9QMNcJwRzUvjWxIGDbI7BJzA4r7Y8vgHW+0OAZIOHvRpTM6n6ujoOOWceX915aQxi1Yp0Zkc0rm8nqP6x2/+X0TjDQcqfgCA8Ux6Vj+JHwAA3s4HAIBBDEr83M4HAIBBqPgBAMZznBihniMekPgBAKDVDwAATkdU/AAA45l0O19QFf+KFSvkcDgCRmZmZqRiAwAgOqwwjTgQdMX/T//0T3rllVf8nxMTE8MaEAAAiJygE/+AAQOo8gEAp584qdhDFfTivr1792rEiBHKzs7WP//zP2vfvn0n3d/j8cjtdgcMAABiSdccf6gjHgSV+CdNmqQNGzZoy5YteuSRR9Ta2qr8/Hy1t7f3ekxVVZXS0tL8w+VyhRw0AADon6ASf1FRkX70ox/pkksu0TXXXKMXX3xRkvSHP/yh12MqKyvV0dHhH83NzaFFDABAuLG4r2/OPPNMXXLJJdq7d2+v+zidTjmdzlAuAwBARHE7Xx95PB69++67Gj58eLjiAQAg+gyq+INK/Hfffbdef/117d+/X//zP/+jm2++WW63W/PmzYtUfAAAIIyCavV/9NFHuvXWW3XgwAENGzZMl19+ubZv366srKxIxQcAQMSZ1OoPKvH/53/+Z6TiAADAPrykBwAAnI54SQ8AAAZV/CR+AIDxTJrjp9UPAIBBqPgBAKDVDwCAORyWJYcVWuYO9fhoIfEDAGBQxc8cPwAABqHiBwAYz6RV/SR+AABo9QMAgNORbRV/4uAzlehIsuvykNRy+1i7Q8A3DF//jt0h4IT8VxfbHQIk+Y4clfRvUbkWrX4AUUXSB2xGqx8AAJyOqPgBAMaj1Q8AgElo9QMAgNMRFT8AAIqfVn2oSPwAAFjW8RHqOeIAiR8AYDyTFvcxxw8AgEGo+AEAMGhVP4kfAGA8h+/4CPUc8YBWPwAABqHiBwCAVj8AAOZgVT8AADgtUfEDAMADfAAAMIdJrf6IJ36PxyOPx+P/7Ha7I31JAADQi5Dm+Ddu3KjBgwf7R319fbd9qqqqlJaW5h8ulyuUSwIAEH5WmEYcCKniv+GGGzRp0iT/53PPPbfbPpWVlaqoqPB/drvdJH8AQEyh1d9HKSkpSklJOek+TqdTTqczlMsAABBZBi3u43Y+AAAMwqp+AIDxaPUDAGASgx7ZS6sfAACDUPEDAIxHqx8AAJP4rOMj1HPEAVr9AAAYhIofAACDFveR+AEAxnMoDHP8YYkk8mj1AwBgECp+AAB4ZC8AAOboup0v1BGstWvXKjs7W8nJycrLy+vxLbdd/vSnP+naa6/VsGHDlJqaqsmTJ2vLli1BX5PEDwCADa/lrampUXl5uZYuXarGxkYVFBSoqKhITU1NPe6/detWXXvttaqtrVVDQ4OuvvpqzZgxQ42NjUFdl8QPAIANVq9ereLiYpWUlCgnJ0dr1qyRy+VSdXV1j/uvWbNG9957ry677DJdeOGFWrVqlS688EK98MILQV2XxA8AMJ7DssIyJMntdgcMj8fT7XqdnZ1qaGhQYWFhwPbCwkJt27atTzH7fD4dPHhQQ4YMCeqf1bbFfd6vDsnh6LTr8pDUWNnzX5WwQaU0fcQ4u6PACRfOa7A7BEg6Zn2t5mhdzHdihHoOSS6XK2Dz8uXLtWLFioBtBw4ckNfrVUZGRsD2jIwMtba29ulyv/rVr3To0CHdcsstQYXJqn4gBpD0gdNHc3OzUlNT/Z+dTmev+zocgXf/W5bVbVtPNm3apBUrVui5555Tenp6UPGR+AEAxvtmqz6Uc0hSampqQOLvydChQ5WYmNitum9ra+vWBfi2mpoaFRcX66mnntI111wTdJzM8QMAEOVV/UlJScrLy1NdXV3A9rq6OuXn5/d63KZNm3TbbbfpiSee0PXXX9/3C34DFT8AADaoqKjQnDlzNGHCBE2ePFnr1q1TU1OTSktLJUmVlZX6+OOPtWHDBknHk/7cuXP161//Wpdffrm/WzBw4EClpaX1+bokfgAAbHhy36xZs9Te3q6VK1eqpaVFubm5qq2tVVZWliSppaUl4J7+hx9+WMeOHdPChQu1cOFC//Z58+bp8ccf7/N1SfwAAOP198l73z5HsMrKylRWVtbjd99O5q+99lrwF+gBc/wAABiEih8AAINe0kPiBwAYz+E7PkI9Rzwg8QMAYFDFzxw/AAAGoeIHAKAfr9Xt8RxxgMQPADBeOB/ZG+to9QMAYJCgEn9VVZUuu+wypaSkKD09XTfeeKN2794dqdgAAIiOrsV9oY44EFTif/3117Vw4UJt375ddXV1OnbsmAoLC3Xo0KFIxQcAQORZknwhjvjI+8HN8b/00ksBn9evX6/09HQ1NDToyiuvDGtgAAAg/EJa3NfR0SFJGjJkSK/7eDweeTwe/2e32x3KJQEACDsW9/WBZVmqqKjQFVdcodzc3F73q6qqUlpamn+4XK7+XhIAgMiwFIY5frv/Ifqm34n/rrvu0jvvvKNNmzaddL/Kykp1dHT4R3Nzc38vCQAAQtSvVv+iRYv0/PPPa+vWrRo5cuRJ93U6nXI6nf0KDgCAqDDokb1BJX7LsrRo0SJt3rxZr732mrKzsyMVFwAA0eOT5AjDOeJAUIl/4cKFeuKJJ/Tcc88pJSVFra2tkqS0tDQNHDgwIgECABBpLO7rRXV1tTo6OjR16lQNHz7cP2pqaiIVHwAACKOgW/0AAJx2mOMHAMAgBiV+XtIDAIBBqPgBADCo4ifxAwBg0O18tPoBADAIFT8AwHgm3cdP4gcAwKA5flr9AAAYhIofAACfJTlCrNh98VHxk/gBADCo1U/iBwBAYUj8io/Ezxw/EAO2fPK23SEAMAQVPxADpo8YZ3cIgNlo9QMAYBCfpZBb9XGyuI9WPwAABqHiBwDA8h0foZ4jDpD4AQAwaI6fVj8AAAah4gcAwKDFfSR+AABo9QMAgNMRFT8AAJbCUPGHJZKII/EDAGBQq5/EDwCAzycpxPvwffFxHz9z/AAAGISKHwAAg1r9/ar4165dq+zsbCUnJysvL0/19fXhjgsAgOjpSvyhjjgQdOKvqalReXm5li5dqsbGRhUUFKioqEhNTU2RiA8AAIRR0Il/9erVKi4uVklJiXJycrRmzRq5XC5VV1dHIj4AACLPZ4VnxIGg5vg7OzvV0NCgn/70pwHbCwsLtW3bth6P8Xg88ng8/s9ut7sfYQIAEDmW5ZMV4tv1Qj0+WoKq+A8cOCCv16uMjIyA7RkZGWptbe3xmKqqKqWlpfmHy+Xqf7QAACAk/Vrc53A4Aj5bltVtW5fKykp1dHT4R3Nzc38uCQBA5FhhaPPHyeK+oFr9Q4cOVWJiYrfqvq2trVsXoIvT6ZTT6ex/hAAARJoVhrfzxUniD6riT0pKUl5enurq6gK219XVKT8/P6yBAQCA8Av6AT4VFRWaM2eOJkyYoMmTJ2vdunVqampSaWlpJOIDACDyfD7JEeLivDhZ3Bd04p81a5ba29u1cuVKtbS0KDc3V7W1tcrKyopEfAAARJ5Brf5+PbK3rKxMZWVl4Y4FAABbWD6frBAr/tPydj4AABDfeEkPAAC0+gEAMIjPkhxmJH5a/QAAGISKHwAAy5IU6u188VHxk/gBAMazfJasEFv9Vpwkflr9AAAYhIofAADLp9Bb/fFxHz+JHwBgPFr9AADgtBT1ir/rL6Jj+jrkZyUgNO6DXrtDwAnHrK/tDgGIOcd0/N+LaFTSxyxPyK36rnhjXdQT/8GDByVJb6g22pfGt5x9kd0R4P/sszsAIGYdPHhQaWlpETl3UlKSMjMz9UZreHJSZmamkpKSwnKuSHFYUZ6U8Pl8+uSTT5SSkiKHwxHNSwMA4ohlWTp48KBGjBihhITIzUwfPXpUnZ2dYTlXUlKSkpOTw3KuSIl64gcAAPZhcR8AAAYh8QMAYBASPwAABiHxAwBgEBI/AAAGIfEDAGAQEj8AAAb5/1gNTqogPTmVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_tokens , attention = translate_sentence('2016 20 Sunday May',t.to(device_cpu))\n",
    "plot_heatmap('2016 20 Sunday May',''.join(output_tokens),attention.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(100,64, num_layers= 2 ,batch_first= True ,bidirectional=True)\n",
    "inp = torch.randn((2,8,100))\n",
    "outputs,hidden = rnn(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inq = torch.cat([hidden[-i , : , :] for i in range(4)],dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn(inp)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(10,30)\n",
    "\n",
    "embedding_layer(torch.tensor([[10,2,3],[4,5,6]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcc965cbd87e810c5948a362c404a650a7d80b88f993f5ac595feee5cfd87ac7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
