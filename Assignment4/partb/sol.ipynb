{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/depressedcoder/environments/gymenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import spacy\n",
    "import jsonlines\n",
    "import json\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence,pack_padded_sequence,pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import SubsetRandomSampler,DataLoader,Subset\n",
    "from torchtext.vocab import GloVe\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# SENTENCE_SPLITTING_USED; whether to use the splitting of reviews into sentences.\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "ATTENTION_DIM = 256\n",
    "NUM_FILTERS = 86\n",
    "\n",
    "PATIENCE_PARAMETER = 7\n",
    "VALIDATION_LOSS_COMPUTE_STEP = 1\n",
    "EXPAND_CONTRACTIONS = True\n",
    "\n",
    "\n",
    "device_cpu = torch.device('cpu')\n",
    "device_fast = torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "if torch.has_mps:\n",
    "    device_fast = torch.device('mps')\n",
    "elif torch.has_cuda:\n",
    "    device_fast = torch.device('cuda')\n",
    "\n",
    "#torch.manual_seed(0)\n",
    "#np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "glove = GloVe()\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions_text(text):    \n",
    "    flags = re.IGNORECASE | re.MULTILINE\n",
    "    text = re.sub(r'`', \"'\", text, flags = flags)\n",
    "    ## starts / ends with '\n",
    "    text = re.sub(\n",
    "        r\"(\\s|^)'(aight|cause)(\\s|$)\",\n",
    "        '\\g<1>\\g<2>\\g<3>',\n",
    "        text, flags = flags\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"(\\s|^)'t(was|is)(\\s|$)\", r'\\g<1>it \\g<2>\\g<3>',\n",
    "        text,\n",
    "        flags = flags\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"(\\s|^)ol'(\\s|$)\",\n",
    "        '\\g<1>old\\g<2>',\n",
    "        text, flags = flags\n",
    "    )\n",
    "        \n",
    "    text = re.sub(r\"\\b(aight)\\b\", 'alright', text, flags = flags)\n",
    "    text = re.sub(r'\\bcause\\b', 'because', text, flags = flags)\n",
    "    text = re.sub(r'\\b(finna|gonna)\\b', 'going to', text, flags = flags)\n",
    "    text = re.sub(r'\\bgimme\\b', 'give me', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgive'n\\b\", 'given', text, flags = flags)\n",
    "    text = re.sub(r\"\\bhowdy\\b\", 'how do you do', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgotta\\b\", 'got to', text, flags = flags)\n",
    "    text = re.sub(r\"\\binnit\\b\", 'is it not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(can)(not)\\b\", r'\\g<1> \\g<2>', text, flags = flags)\n",
    "    text = re.sub(r\"\\bwanna\\b\", 'want to', text, flags = flags)\n",
    "    text = re.sub(r\"\\bmethinks\\b\", 'me thinks', text, flags = flags)\n",
    "    text = re.sub(r\"\\bo'er\\b\", r'over', text, flags = flags)\n",
    "    text = re.sub(r\"\\bne'er\\b\", r'never', text, flags = flags)\n",
    "    text = re.sub(r\"\\bo'?clock\\b\", 'of the clock', text, flags = flags)\n",
    "    text = re.sub(r\"\\bma'am\\b\", 'madam', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgiv'n\\b\", 'given', text, flags = flags)\n",
    "    text = re.sub(r\"\\be'er\\b\", 'ever', text, flags = flags)\n",
    "    text = re.sub(r\"\\bd'ye\\b\", 'do you', text, flags = flags)\n",
    "    text = re.sub(r\"\\be'er\\b\", 'ever', text, flags = flags)\n",
    "    text = re.sub(r\"\\bd'ye\\b\", 'do you', text, flags = flags)\n",
    "    text = re.sub(r\"\\bg'?day\\b\", 'good day', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(ain|amn)'?t\\b\", 'am not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(are|can)'?t\\b\", r'\\g<1> not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(let)'?s\\b\", r'\\g<1> us', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all'dn't've'd\\b\", 'you all would not have had', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all're\\b\", 'you all are', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all'd've\\b\", 'you all would have', text, flags = flags)\n",
    "    text = re.sub(r\"(\\s)y'all(\\s)\", r'\\g<1>you all\\g<2>', text, flags = flags)  \n",
    "    text = re.sub(r\"\\b(won)'?t\\b\", 'will not', text, flags = flags)\n",
    "    text = re.sub(r\"\\bhe'd\\b\", 'he had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I|we|who)'?d'?ve\\b\", r'\\g<1> would have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(could|would|must|should|would)n'?t'?ve\\b\", r'\\g<1> not have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(he)'?dn'?t'?ve'?d\\b\", r'\\g<1> would not have had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(daren|daresn|dasn)'?t\", 'dare not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(he|how|i|it|she|that|there|these|they|we|what|where|which|who|you)'?ll\\b\", r'\\g<1> will', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(everybody|everyone|he|how|it|she|somebody|someone|something|that|there|this|what|when|where|which|who|why)'?s\\b\", r'\\g<1> is', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m'a\\b\", r'\\g<1> am about to', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m'o\\b\", r'\\g<1> am going to', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m\\b\", r'\\g<1> am', text, flags = flags)\n",
    "    text = re.sub(r\"\\bshan't\\b\", 'shall not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(are|could|did|does|do|go|had|has|have|is|may|might|must|need|ought|shall|should|was|were|would)n'?t\\b\", r'\\g<1> not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(could|had|he|i|may|might|must|should|these|they|those|to|we|what|where|which|who|would|you)'?ve\\b\", r'\\g<1> have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(how|so|that|there|these|they|those|we|what|where|which|who|why|you)'?re\\b\", r'\\g<1> are', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I|it|she|that|there|they|we|which|you)'?d\\b\", r'\\g<1> had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(how|what|where|who|why)'?d\\b\", r'\\g<1> did', text, flags = flags)    \n",
    "    return text\n",
    "\n",
    "\n",
    "class ExpandContractionsClass:\n",
    "    def __init__(self, nlp: Language):\n",
    "        self.nlp = nlp\n",
    "    \n",
    "    def __call__(self,doc: Doc):\n",
    "        text = doc.text\n",
    "        return self.nlp.make_doc(expand_contractions_text(text))\n",
    "    \n",
    "@Language.factory(\"expand_contractions_component\")\n",
    "def create_expand_contractions_component(nlp : Language, name: str):\n",
    "    return ExpandContractionsClass(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "if EXPAND_CONTRACTIONS:\n",
    "    nlp.add_pipe(\"expand_contractions_component\",before='tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):    \n",
    "    text = re.sub(r'<br /><br />',\"$$\",text)\n",
    "    text = BeautifulSoup(text,'lxml').get_text().strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = ' '.join(re.findall(r\"[\\w']+|[.,!;/\\\"]\", text))\n",
    "    \n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word == '':\n",
    "            continue\n",
    "        new_text.append(word)\n",
    "    \n",
    "    text = ' '.join(new_text)\n",
    "    words = nlp(text)\n",
    "    text =  \" \".join([token.text for token in words if not token.is_punct or token.text=='/' or token.text==\"\\\"\" or token.text==\".\"]).strip()\n",
    "    new_words = []\n",
    "    for word in text.split(\" \"):\n",
    "        #if word == 'n\\'t':\n",
    "        #    if len(new_words) > 1:\n",
    "        #        new_words[-1] = new_words[-1] + word\n",
    "        #    else:\n",
    "        #        new_words.append(word)\n",
    "        if word == '\\'s':\n",
    "            pass\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    \n",
    "    text = \" \".join(new_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the training data which was given for Assignment 2\n",
    "def process_assignment2_training_data():\n",
    "    preprocessed_dataset = []\n",
    "    train_dataset_labels = []\n",
    "    with open(\"./Train dataset.csv\") as csvfile:\n",
    "        csvFile = csv.reader(csvfile)\n",
    "        next(csvFile)\n",
    "        json_writer = jsonlines.open('processed_dataset.jsonl','w')\n",
    "\n",
    "        for line in csvFile:\n",
    "            processed_text = preprocess_text(line[0])\n",
    "            label = 1.0 if line[1] == 'positive' else 0.0\n",
    "            train_dataset_labels.append(label)\n",
    "            json_writer.write({\"text\":processed_text,\"label\":label})\n",
    "            preprocessed_dataset.append({\"text\":processed_text,\"label\":label})\n",
    "    \n",
    "        json_writer.close()\n",
    "\n",
    "\n",
    "#process_assignment2_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = []\n",
    "train_dataset_labels = []\n",
    "\n",
    "\n",
    "TRAIN_FILE_NAME = './processed_dataset.jsonl'\n",
    "\n",
    "with open(TRAIN_FILE_NAME ,encoding='utf-8') as f:\n",
    "#with open('processed_dataset.jsonl',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sample = json.loads(line)\n",
    "        train_dataset_labels.append(sample['label'])\n",
    "        preprocessed_dataset.append(sample)\n",
    "      \n",
    "train_dataset_labels = np.array(train_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordEmbeddingforText(text,glove=glove):\n",
    "    length = 0\n",
    "    words = []\n",
    "    text = text.strip()\n",
    "    for word in text.split(' '):\n",
    "        w = word.strip()\n",
    "        if w=='':\n",
    "            continue\n",
    "        length+=1\n",
    "        word_embedding = glove[w]\n",
    "        words.append(word_embedding)\n",
    "    \n",
    "    return torch.stack(words),length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences, word\n",
    "def review_to_embed(review,glove=glove): \n",
    "    sentences = review.split(\".\")\n",
    "    sentence_lengths = []\n",
    "    review_embeddings = []\n",
    "    num_sentences = 0\n",
    "    for sentence in sentences:\n",
    "        s = sentence.strip()\n",
    "        if s == '':\n",
    "            continue\n",
    "        num_sentences += 1\n",
    "        sentence_word_embeddings,sentence_length = getWordEmbeddingforText(s,glove)\n",
    "        sentence_lengths.append(sentence_length)\n",
    "        review_embeddings.append(sentence_word_embeddings)\n",
    "\n",
    "    return torch.nn.utils.rnn.pad_sequence(review_embeddings,batch_first=True),sentence_lengths,num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self,reviews):\n",
    "        super().__init__()\n",
    "        self.reviews = reviews\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.reviews[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for review in preprocessed_dataset:\n",
    "        embeddings, sent_length ,n_sents = review_to_embed(review['text'])\n",
    "        processed_dataset.append({'review': embeddings,'sent_lengths': sent_length,'length' : n_sents,'label' : review['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_function(batch_data):   \n",
    "    \n",
    "    inputs = [b['review'] for b in batch_data]\n",
    "    sent_lengths = [ b['sent_lengths'] for b in batch_data ]\n",
    "    n_sentences = [ b['length'] for b in batch_data ]\n",
    "    labels = torch.tensor([b['label'] for b in batch_data])\n",
    "\n",
    "\n",
    "    labels = labels.unsqueeze(1)\n",
    "    max_n_sentences = max([i.shape[0] for i in inputs] )\n",
    "    max_n_words = max([i.shape[1] for i in inputs])\n",
    "\n",
    " \n",
    "    processed_inputs = []\n",
    "    for inp in inputs:\n",
    "\n",
    "        t1 = torch.permute(inp,(2,1,0))\n",
    "        t1 = torch.nn.functional.pad(t1,(0,max_n_sentences-inp.shape[0],0,max_n_words-inp.shape[1]))\n",
    "        t1 = torch.permute(t1,(2,1,0))\n",
    "        processed_inputs.append(t1)\n",
    "\n",
    "    final_inp = torch.stack(processed_inputs)\n",
    "    #inputs = pad_sequence(inputs,batch_first=True)\n",
    "    return  {'input' : final_inp , 'sent_lengths': sent_lengths , 'lengths' : n_sentences ,'labels' : labels }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx,valid_idx = train_test_split(np.arange(train_dataset_labels.shape[0]), \n",
    "    test_size=0.2,\n",
    "    shuffle= True,\n",
    "    stratify= train_dataset_labels,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "dataset = ReviewDataSet(processed_dataset)\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_dataloader = DataLoader(dataset,64,sampler=train_sampler,collate_fn=collate_function)\n",
    "valid_dataloader = DataLoader(dataset,64,sampler=valid_sampler,collate_fn=collate_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class WordAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        hidden_dim = HIDDEN_DIM,\n",
    "        attention_dim = ATTENTION_DIM,\n",
    "        num_layers=1,\n",
    "        bidirectional=True,\n",
    "        device_train=device_cpu,\n",
    "        rnn_dropout = 0.0,\n",
    "        fc_dropout = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(embed_dim,hidden_dim,num_layers=num_layers,batch_first=True,bidirectional=bidirectional,dropout=rnn_dropout)\n",
    "        bidirectional_factor = 2 if bidirectional else 1\n",
    "        self.word_attention = nn.Linear(bidirectional_factor*hidden_dim,attention_dim)\n",
    "        self.u_w = nn.Linear(attention_dim,1)\n",
    "        self.device_train = device_train\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "    \n",
    "    def create_mask(self,inp_len):\n",
    "\n",
    "        mask = torch.ones(len(inp_len),max(inp_len),dtype=torch.int64)\n",
    "        for i in range(len(inp_len)):\n",
    "            mask[i,inp_len[i]:] = 0\n",
    "        return mask\n",
    "        \n",
    "    def forward(self,inp,inp_len):\n",
    "        \n",
    "        # inp = 1 review  = [num_sentences , num_words , embed_dim]\n",
    "        # inp_len = length = num_sentences , each element number of words in  sentence.\n",
    "\n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(inp,inp_len,batch_first=True,enforce_sorted=False)\n",
    "        packed_output,hidden = self.rnn(packed_embedding)\n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(packed_output,batch_first=True)\n",
    "        \n",
    "        attention_outs = torch.tanh(self.fc_dropout(self.word_attention(outputs)))\n",
    "        attention_scores = self.u_w(attention_outs)\n",
    "        attention_scores = attention_scores.squeeze(2)\n",
    "        attention_mask = self.create_mask(inp_len).to(self.device_train)\n",
    "        attention_scores = attention_scores.masked_fill(attention_mask==0, -1e10)   # Fill padding tokens with a lower value\n",
    "        attention_probs = F.softmax(attention_scores,dim=1)\n",
    "        attention_probs = attention_scores.unsqueeze(2)\n",
    "\n",
    "        weighted_embeddings = attention_probs * outputs\n",
    "        output = torch.sum(weighted_embeddings,dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "            embed_dim=EMBED_DIM,\n",
    "            hidden_dim = HIDDEN_DIM,\n",
    "            attention_dim=ATTENTION_DIM,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            train_device = device_cpu,\n",
    "            rnn_dropout = 0.0,\n",
    "            fc_dropout = 0.3\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(embed_dim,hidden_dim,num_layers=num_layers,batch_first=True,bidirectional=bidirectional,dropout=rnn_dropout)\n",
    "        bidirectional_factor = 2 if bidirectional else 1\n",
    "        self.sentence_attention = nn.Linear(bidirectional_factor*hidden_dim,attention_dim)\n",
    "        self.u_s = nn.Linear(attention_dim,1)\n",
    "        self.train_device = train_device\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "\n",
    "    def create_mask(self,sent_len):\n",
    "        mask = torch.ones(len(sent_len),max(sent_len),dtype=torch.int64)\n",
    "        for i in range(len(sent_len)):\n",
    "            mask[i,sent_len[i]:] = 0\n",
    "        return mask\n",
    "    \n",
    " \n",
    "    def forward(self,sents,sent_len):\n",
    "        \n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(sents,sent_len,enforce_sorted=False)\n",
    "        packed_output,hidden = self.rnn(packed_embedding)\n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(packed_output,batch_first=True)\n",
    "\n",
    "        attention_outs = torch.tanh(self.fc_dropout(self.sentence_attention(outputs)))\n",
    "        attention_scores = self.u_s(attention_outs)\n",
    "        attention_scores = attention_scores.squeeze(2)\n",
    "        attention_mask = self.create_mask(sent_len).to(self.train_device)\n",
    "        attention_scores = attention_scores.masked_fill(attention_mask==0, -1e10)   # Fill padding tokens with a lower value\n",
    "        attention_probs = F.softmax(attention_scores,dim=1)\n",
    "        attention_probs = attention_scores.unsqueeze(2)\n",
    "        weighted_embeddings = attention_probs*outputs\n",
    "        output = torch.sum(weighted_embeddings,dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchialAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                \n",
    "                input_embed_dim = EMBED_DIM,\n",
    "                word_encoder_hidden_dim = HIDDEN_DIM,\n",
    "                word_encoder_num_layers = 1,\n",
    "                word_encoder_bidirectional = True,\n",
    "                word_encoder_attention_dim = HIDDEN_DIM,\n",
    "                word_encoder_fc_dropout = 0.3,\n",
    "\n",
    "                sentence_encoder_hidden_dim = HIDDEN_DIM,\n",
    "                sentence_encoder_num_layers = 1,\n",
    "                sentence_encoder_bidirectional= True,\n",
    "                sentence_encoder_attention_dim = HIDDEN_DIM,\n",
    "                sentence_encoder_fc_dropout = 0.3,\n",
    "\n",
    "                rnn_dropout = 0.0,\n",
    "                fc_dropout = 0.3,\n",
    "                train_device = device_cpu\n",
    "            ):\n",
    "      \n",
    "        super().__init__()\n",
    "\n",
    "        rnn_dropout_word = 0.3 if word_encoder_num_layers > 1 else  0.0\n",
    "        rnn_dropout_sentence = 0.3 if sentence_encoder_num_layers > 1 else 0.0\n",
    "\n",
    "        self.word_encoder = WordAttention(input_embed_dim,word_encoder_hidden_dim,word_encoder_attention_dim,word_encoder_num_layers,word_encoder_bidirectional,train_device,rnn_dropout_word,word_encoder_fc_dropout)\n",
    "        bidirectional_factor = 2 if word_encoder_bidirectional else 1\n",
    "        self.sentence_encoder = SentenceAttention(bidirectional_factor*word_encoder_hidden_dim,sentence_encoder_hidden_dim,sentence_encoder_attention_dim,sentence_encoder_num_layers,sentence_encoder_bidirectional,train_device,rnn_dropout_sentence,sentence_encoder_fc_dropout)\n",
    "        \n",
    "        division_factor = 2\n",
    "        self.fc_list = [\n",
    "                nn.Linear(bidirectional_factor*sentence_encoder_hidden_dim,sentence_encoder_hidden_dim),\n",
    "        ] \n",
    "\n",
    "        #for i in range(division_factor):\n",
    "        #    self.fc_list.append(nn.Linear(sentence_encoder_hidden_dim/(2**i),sentence_encoder_hidden_dim/(2**(i+1))))\n",
    "        \n",
    "        self.fc = nn.ModuleList(self.fc_list)\n",
    "        #self.fc_out = nn.Linear(sentence_encoder_hidden_dim/(2**(division_factor)),1)\n",
    "        self.fc_out = nn.Linear(sentence_encoder_hidden_dim,1)\n",
    "        self.fc_dropout_layer = nn.Dropout(p=fc_dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,inp,inp_sentence_lengths,inp_words_lengths):\n",
    "        \n",
    "        sentence_embeddings = []\n",
    "        for i in range(inp.shape[0]):\n",
    "            sentence_embeddings.append(self.word_encoder(inp[i],inp_words_lengths[i]))\n",
    "        \n",
    "        \n",
    "        batch_sentences = pad_sequence(sentence_embeddings)\n",
    "        #batch_sentences = torch.stack(sentence_embeddings)\n",
    "        doc_embedding = self.sentence_encoder(batch_sentences,inp_sentence_lengths)\n",
    "\n",
    "        out = doc_embedding\n",
    "        for i,l in enumerate(self.fc_list):\n",
    "            out = self.fc_dropout_layer(F.relu(l(out)))\n",
    "        \n",
    "        out = self.sigmoid(self.fc_out(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchial Model with Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class WordSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        hidden_dim = HIDDEN_DIM,\n",
    "        attention_dim = ATTENTION_DIM,\n",
    "        num_layers=1,\n",
    "        bidirectional=True,\n",
    "        device_train=device_cpu,\n",
    "        rnn_dropout = 0.0,\n",
    "        fc_dropout = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.rnn = nn.GRU(embed_dim,hidden_dim,num_layers=num_layers,batch_first=True,bidirectional=bidirectional,dropout=rnn_dropout)\n",
    "        bidirectional_factor = 2 if bidirectional else 1\n",
    "        \n",
    "        self.query_attention = nn.Linear(bidirectional_factor*hidden_dim,attention_dim)\n",
    "        self.key_attention = nn.Linear(bidirectional_factor*hidden_dim,attention_dim)\n",
    "       \n",
    "        self.device_train = device_train\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "    \n",
    "    def create_mask(self,inp_len):\n",
    "\n",
    "        mask = torch.ones(len(inp_len),max(inp_len),dtype=torch.int64)\n",
    "        for i in range(len(inp_len)):\n",
    "            mask[i,inp_len[i]:] = 0\n",
    "        return mask\n",
    "    \n",
    "    def create_self_attention_mask(self,inp_len):\n",
    "        mask = torch.ones(len(inp_len),max(inp_len),max(inp_len),dtype=torch.int64)\n",
    "        for i in range(len(inp_len)):\n",
    "            mask[i,:,inp_len[i]:] = 0 \n",
    "            mask[i,inp_len[i]:,:] = 0\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def forward(self,inp,inp_len):\n",
    "        \n",
    "        # inp = 1 review  = [num_sentences , num_words , embed_dim]\n",
    "        # inp_len = length = num_sentences , each element number of words in  sentence.\n",
    "\n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(inp,inp_len,batch_first=True,enforce_sorted=False)\n",
    "        packed_output,hidden = self.rnn(packed_embedding)\n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(packed_output,batch_first=True)\n",
    "        \n",
    "        queries = torch.tanh(self.query_attention(outputs))\n",
    "        keys = torch.tanh(self.key_attention(outputs))\n",
    "        \n",
    "        P_matrix = torch.matmul(queries,torch.transpose(keys,1,2)) * (1/np.sqrt(self.embed_dim))\n",
    "\n",
    "        attention_mask = self.create_self_attention_mask(inp_len).to(self.device_train)\n",
    "        normalized_P_matrix = P_matrix.masked_fill(attention_mask==0,-1e10)\n",
    "        attention_probs = F.softmax(normalized_P_matrix,dim=2)\n",
    "\n",
    "        final_outputs = torch.matmul(attention_probs,outputs)\n",
    "        output = torch.sum(final_outputs,dim=1)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "            embed_dim=EMBED_DIM,\n",
    "            hidden_dim = HIDDEN_DIM,\n",
    "            attention_dim=ATTENTION_DIM,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            train_device = device_cpu,\n",
    "            rnn_dropout = 0.0,\n",
    "            fc_dropout = 0.3\n",
    "        ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.rnn = nn.GRU(embed_dim,hidden_dim,num_layers=num_layers,batch_first=True,bidirectional=bidirectional,dropout=rnn_dropout)\n",
    "        bidirectional_factor = 2 if bidirectional else 1\n",
    "      \n",
    "\n",
    "        self.query_attention = nn.Linear(bidirectional_factor*hidden_dim,attention_dim)\n",
    "        self.key_attention = nn.Linear(bidirectional_factor*hidden_dim,attention_dim)\n",
    "        self.device_train = train_device\n",
    "        self.train_device = train_device\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "\n",
    "    def create_mask(self,sent_len):\n",
    "        mask = torch.ones(len(sent_len),max(sent_len),dtype=torch.int64)\n",
    "        for i in range(len(sent_len)):\n",
    "            mask[i,sent_len[i]:] = 0\n",
    "        return mask\n",
    "\n",
    "    def create_self_attention_mask(self,sent_len):\n",
    "        mask = torch.ones(len(sent_len),max(sent_len),max(sent_len),dtype=torch.int64)\n",
    "        for i in range(len(sent_len)):\n",
    "            mask[i,:,sent_len[i]:] = 0 \n",
    "            mask[i,sent_len[i]:,:] = 0\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def forward(self,sents,sent_len):\n",
    "        \n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(sents,sent_len,enforce_sorted=False)\n",
    "        packed_output,hidden = self.rnn(packed_embedding)\n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(packed_output,batch_first=True)\n",
    " \n",
    "        queries = torch.tanh(self.query_attention(outputs))\n",
    "        keys = torch.tanh(self.key_attention(outputs))\n",
    "        \n",
    "        P_matrix = torch.matmul(queries,torch.transpose(keys,1,2)) * (1/np.sqrt(self.embed_dim))\n",
    "\n",
    "        attention_mask = self.create_self_attention_mask(sent_len).to(self.device_train)\n",
    "        normalized_P_matrix = P_matrix.masked_fill(attention_mask==0,-1e10)\n",
    "        attention_probs = F.softmax(normalized_P_matrix,dim=2)\n",
    "\n",
    "        final_outputs = torch.matmul(attention_probs,outputs)\n",
    "        output = torch.sum(final_outputs,dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchialSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                \n",
    "                input_embed_dim = EMBED_DIM,\n",
    "                word_encoder_hidden_dim = HIDDEN_DIM,\n",
    "                word_encoder_num_layers = 1,\n",
    "                word_encoder_bidirectional = True,\n",
    "                word_encoder_attention_dim = HIDDEN_DIM,\n",
    "                word_encoder_fc_dropout = 0.3,\n",
    "\n",
    "                sentence_encoder_hidden_dim = HIDDEN_DIM,\n",
    "                sentence_encoder_num_layers = 1,\n",
    "                sentence_encoder_bidirectional= True,\n",
    "                sentence_encoder_attention_dim = HIDDEN_DIM,\n",
    "                sentence_encoder_fc_dropout = 0.3,\n",
    "\n",
    "                rnn_dropout = 0.0,\n",
    "                fc_dropout = 0.3,\n",
    "                train_device = device_cpu\n",
    "            ):\n",
    "      \n",
    "        super().__init__()\n",
    "\n",
    "        rnn_dropout_word = 0.3 if word_encoder_num_layers > 1 else  0.0\n",
    "        rnn_dropout_sentence = 0.3 if sentence_encoder_num_layers > 1 else 0.0\n",
    "\n",
    "        self.word_encoder = WordSelfAttention(input_embed_dim,word_encoder_hidden_dim,word_encoder_attention_dim,word_encoder_num_layers,word_encoder_bidirectional,train_device,rnn_dropout_word,word_encoder_fc_dropout)\n",
    "        bidirectional_factor = 2 if word_encoder_bidirectional else 1\n",
    "        self.sentence_encoder = SentenceSelfAttention(bidirectional_factor*word_encoder_hidden_dim,sentence_encoder_hidden_dim,sentence_encoder_attention_dim,sentence_encoder_num_layers,sentence_encoder_bidirectional,train_device,rnn_dropout_sentence,sentence_encoder_fc_dropout)\n",
    "        \n",
    "        division_factor = 2\n",
    "        self.fc_list = [\n",
    "                nn.Linear(bidirectional_factor*sentence_encoder_hidden_dim,sentence_encoder_hidden_dim),\n",
    "        ] \n",
    "\n",
    "        #for i in range(division_factor):\n",
    "        #    self.fc_list.append(nn.Linear(sentence_encoder_hidden_dim/(2**i),sentence_encoder_hidden_dim/(2**(i+1))))\n",
    "        \n",
    "        self.fc = nn.ModuleList(self.fc_list)\n",
    "        #self.fc_out = nn.Linear(sentence_encoder_hidden_dim/(2**(division_factor)),1)\n",
    "        self.fc_out = nn.Linear(sentence_encoder_hidden_dim,1)\n",
    "        self.fc_dropout_layer = nn.Dropout(p=fc_dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,inp,inp_sentence_lengths,inp_words_lengths):\n",
    "        \n",
    "        sentence_embeddings = []\n",
    "        for i in range(inp.shape[0]):\n",
    "            sentence_embeddings.append(self.word_encoder(inp[i],inp_words_lengths[i]))\n",
    "        \n",
    "        \n",
    "        batch_sentences = pad_sequence(sentence_embeddings)\n",
    "        #batch_sentences = torch.stack(sentence_embeddings)\n",
    "        doc_embedding = self.sentence_encoder(batch_sentences,inp_sentence_lengths)\n",
    "\n",
    "        out = doc_embedding\n",
    "        for i,l in enumerate(self.fc_list):\n",
    "            out = self.fc_dropout_layer(F.relu(l(out)))\n",
    "        \n",
    "        out = self.sigmoid(self.fc_out(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5331],\n",
       "        [0.5682],\n",
       "        [0.5059],\n",
       "        [0.5730],\n",
       "        [0.5651],\n",
       "        [0.5421],\n",
       "        [0.5232],\n",
       "        [0.5149],\n",
       "        [0.5291],\n",
       "        [0.5245],\n",
       "        [0.5332],\n",
       "        [0.5713],\n",
       "        [0.5375],\n",
       "        [0.5519],\n",
       "        [0.5352],\n",
       "        [0.5574],\n",
       "        [0.5159],\n",
       "        [0.5097],\n",
       "        [0.5325],\n",
       "        [0.5776],\n",
       "        [0.5261],\n",
       "        [0.5253],\n",
       "        [0.5219],\n",
       "        [0.5539],\n",
       "        [0.5497],\n",
       "        [0.5109],\n",
       "        [0.5441],\n",
       "        [0.5504],\n",
       "        [0.5425],\n",
       "        [0.5570],\n",
       "        [0.5176],\n",
       "        [0.5407],\n",
       "        [0.5422],\n",
       "        [0.5749],\n",
       "        [0.5553],\n",
       "        [0.5183],\n",
       "        [0.5217],\n",
       "        [0.5033],\n",
       "        [0.5195],\n",
       "        [0.6177],\n",
       "        [0.5528],\n",
       "        [0.5488],\n",
       "        [0.5626],\n",
       "        [0.5535],\n",
       "        [0.6021],\n",
       "        [0.5398],\n",
       "        [0.5579],\n",
       "        [0.5458],\n",
       "        [0.5376],\n",
       "        [0.5630],\n",
       "        [0.5568],\n",
       "        [0.5622],\n",
       "        [0.5582],\n",
       "        [0.5461],\n",
       "        [0.5686],\n",
       "        [0.5885],\n",
       "        [0.5638],\n",
       "        [0.5527],\n",
       "        [0.5510],\n",
       "        [0.5374],\n",
       "        [0.5952],\n",
       "        [0.5820],\n",
       "        [0.5362],\n",
       "        [0.5179]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_self_model = HierarchialSelfAttention()\n",
    "h_self_model(batch_data['input'],batch_data['lengths'],batch_data['sent_lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4989],\n",
       "        [0.4990],\n",
       "        [0.5012],\n",
       "        [0.4987],\n",
       "        [0.4979],\n",
       "        [0.5004],\n",
       "        [0.5037],\n",
       "        [0.4994],\n",
       "        [0.4994],\n",
       "        [0.4989],\n",
       "        [0.5017],\n",
       "        [0.5006],\n",
       "        [0.4987],\n",
       "        [0.4995],\n",
       "        [0.5009],\n",
       "        [0.4995],\n",
       "        [0.5020],\n",
       "        [0.5010],\n",
       "        [0.4988],\n",
       "        [0.5013],\n",
       "        [0.4999],\n",
       "        [0.4995],\n",
       "        [0.5000],\n",
       "        [0.5005],\n",
       "        [0.5012],\n",
       "        [0.5002],\n",
       "        [0.4998],\n",
       "        [0.5001],\n",
       "        [0.4993],\n",
       "        [0.5014],\n",
       "        [0.4994],\n",
       "        [0.5005],\n",
       "        [0.5006],\n",
       "        [0.4992],\n",
       "        [0.4977],\n",
       "        [0.5023],\n",
       "        [0.5012],\n",
       "        [0.5011],\n",
       "        [0.4984],\n",
       "        [0.5025],\n",
       "        [0.5022],\n",
       "        [0.4978],\n",
       "        [0.4998],\n",
       "        [0.4968],\n",
       "        [0.5004],\n",
       "        [0.4990],\n",
       "        [0.5014],\n",
       "        [0.5023],\n",
       "        [0.5000],\n",
       "        [0.5007],\n",
       "        [0.5011],\n",
       "        [0.5036],\n",
       "        [0.4987],\n",
       "        [0.5000],\n",
       "        [0.4973],\n",
       "        [0.5019],\n",
       "        [0.5018],\n",
       "        [0.5015],\n",
       "        [0.5006],\n",
       "        [0.4995],\n",
       "        [0.5002],\n",
       "        [0.5019],\n",
       "        [0.5001],\n",
       "        [0.5000]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_model = HierarchialAttention()\n",
    "h_model(batch_data['input'],batch_data['lengths'],batch_data['sent_lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import  datetime\n",
    "\n",
    "def train(model,train_dataloader,valid_dataloader,num_epochs,criterion,optimizer,\n",
    "    checkpoint_name='best_model.pt',\n",
    "    device_train = device_fast,use_rnn = False,log=True):\n",
    "\n",
    "    tensorboard_name='Ensemble'\n",
    "    if log == True:\n",
    "        current_datetime = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "        tensorboard_name = tensorboard_name + \"_\" + current_datetime\n",
    "        writer = SummaryWriter('runs/' + tensorboard_name)\n",
    "    \n",
    "    \n",
    "    model = model.to(device_train)\n",
    "    clip = 0\n",
    "    if use_rnn:\n",
    "        clip = 5\n",
    "\n",
    "    best_validation_loss = 1000.0\n",
    "    valdiation_loss_not_decreased_steps = 0\n",
    "    \n",
    "    model.train()\n",
    "    for e in range(num_epochs):\n",
    "        \n",
    "        training_set_size = 0\n",
    "        training_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for data in tqdm(train_dataloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            input_reviews,sent_lengths,n_sents,output_labels = data['input'], data['sent_lengths'],data['lengths'],data['labels']\n",
    "            input_reviews = input_reviews.to(device_train)\n",
    "            training_set_size += input_reviews.shape[0]\n",
    "            output = model(input_reviews,n_sents,sent_lengths)\n",
    "            output = output.to(device_cpu)\n",
    "            loss = criterion(output,output_labels.float())\n",
    "            training_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if use_rnn:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "            optimizer.step()\n",
    "        \n",
    "        current_training_loss = training_loss\n",
    "        if log==True:\n",
    "            print(\"Epoch \" + str(e) + \" Average Training Loss = \" +  str(current_training_loss))\n",
    "            writer.add_scalars(tensorboard_name + 'Training Loss vs Epoch',{'train' : current_training_loss},e)\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        if valid_dataloader is None:\n",
    "            continue\n",
    "        \n",
    "        validation_set_size  = 0 \n",
    "        if e% VALIDATION_LOSS_COMPUTE_STEP==0:\n",
    "            correct_count = 0\n",
    "            validation_loss = 0\n",
    "\n",
    "            for i,data in enumerate(valid_dataloader,0):\n",
    "                \n",
    "                input_reviews,sent_lengths,n_sents,output_labels = data['input'], data['sent_lengths'],data['lengths'],data['labels']\n",
    "                input_reviews = input_reviews.to(device_train)\n",
    "                validation_set_size += input_reviews.shape[0]\n",
    "                output = model(input_reviews,n_sents,sent_lengths)\n",
    "                output = output.to(device_cpu)\n",
    "                loss = criterion(output,output_labels.float())\n",
    "                validation_loss += loss.item()\n",
    "                nearest_class = torch.round(output)\n",
    "\n",
    "                correct = (nearest_class == output_labels.float()).float()\n",
    "                correct_count += correct.sum()\n",
    "            correct_count = int(correct_count)\n",
    "            current_validation_accuracy = (correct_count/validation_set_size)*100\n",
    "            current_validation_loss = (1.0* validation_loss)\n",
    "            if log == True:\n",
    "                print(\"Epoch \" + str(e) + \" \" +  \"Validation Loss = \" + str(current_validation_loss) )\n",
    "                print(\"Validation Set Accuracy = \" + str((correct_count/validation_set_size)*100) )\n",
    "                writer.add_scalar(tensorboard_name + ' Validation Accuracy vs Epoch ',(correct_count/validation_set_size*100),e)\n",
    "                writer.add_scalars(tensorboard_name + 'Validation Loss vs Epoch',{'valid' : current_validation_loss},e)\n",
    "\n",
    "            \n",
    "            if log==True:\n",
    "                if current_validation_loss < best_validation_loss:\n",
    "                    valdiation_loss_not_decreased_steps = 0\n",
    "                    torch.save(model.state_dict(),checkpoint_name)\n",
    "                    best_validation_loss = current_validation_loss\n",
    "                else:\n",
    "                    valdiation_loss_not_decreased_steps +=1\n",
    "        if log == True:\n",
    "            if valdiation_loss_not_decreased_steps >= PATIENCE_PARAMETER:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embed_dim = EMBED_DIM\n",
    "word_encoder_hidden_dim = HIDDEN_DIM\n",
    "word_encoder_num_layers = 2\n",
    "word_encoder_bidirectional = True\n",
    "word_encoder_attention_dim = HIDDEN_DIM\n",
    "word_encoder_fc_dropout = 0.3\n",
    "\n",
    "sentence_encoder_hidden_dim = HIDDEN_DIM\n",
    "sentence_encoder_num_layers = 2\n",
    "sentence_encoder_bidirectional= True\n",
    "sentence_encoder_attention_dim = HIDDEN_DIM\n",
    "sentence_encoder_fc_dropout = 0.3\n",
    "\n",
    "rnn_dropout = 0.0\n",
    "fc_dropout = 0.3\n",
    "train_device = device_cpu\n",
    "\n",
    "hierarchial_model_rnn_dropout = 0.0\n",
    "hierarchial_model_fc_dropout = 0.3\n",
    "\n",
    "hierarchial_model = HierarchialAttention(\n",
    "      \n",
    "        input_embed_dim = input_embed_dim,\n",
    "        word_encoder_hidden_dim = word_encoder_hidden_dim,\n",
    "        word_encoder_num_layers = word_encoder_num_layers,\n",
    "        word_encoder_bidirectional = word_encoder_bidirectional,\n",
    "        word_encoder_attention_dim = word_encoder_attention_dim,\n",
    "        word_encoder_fc_dropout= word_encoder_fc_dropout,\n",
    "\n",
    "        sentence_encoder_hidden_dim = sentence_encoder_hidden_dim,\n",
    "        sentence_encoder_num_layers = sentence_encoder_num_layers,\n",
    "        sentence_encoder_bidirectional= sentence_encoder_bidirectional,\n",
    "        sentence_encoder_attention_dim = sentence_encoder_hidden_dim,\n",
    "        sentence_encoder_fc_dropout= sentence_encoder_fc_dropout,\n",
    "\n",
    "        rnn_dropout = hierarchial_model_rnn_dropout,\n",
    "        fc_dropout = hierarchial_model_fc_dropout,\n",
    "        train_device = device_fast  \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = str(input_embed_dim) + \"_\" +  str(word_encoder_hidden_dim) + \"_\" + str(word_encoder_num_layers) + \"_\" + str(word_encoder_bidirectional) + \"_\" + str(word_encoder_attention_dim) + \"_\" + str(sentence_encoder_attention_dim) + \"_\"+ str(sentence_encoder_num_layers) + \"_\" + str(sentence_encoder_bidirectional) + \"_\"+ str(sentence_encoder_attention_dim) + \"_\" + str(hierarchial_model_rnn_dropout) + \"_\"+ str(hierarchial_model_fc_dropout)+\"_\"+str(word_encoder_fc_dropout)+\"_\" +str(sentence_encoder_fc_dropout) +\".pth\"\n",
    "train(hierarchial_model,train_dataloader,valid_dataloader,50,nn.BCELoss(),optim.Adam(hierarchial_model.parameters(),lr=0.001),checkpoint_name,device_train=device_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_name,test_data,sentence_lengths,test_lengths,test_labels):\n",
    "    model = HierarchialAttention(\n",
    "\n",
    "        input_embed_dim = input_embed_dim,\n",
    "        word_encoder_hidden_dim = word_encoder_hidden_dim,\n",
    "        word_encoder_num_layers = word_encoder_num_layers,\n",
    "        word_encoder_bidirectional = word_encoder_bidirectional,\n",
    "        word_encoder_attention_dim = word_encoder_attention_dim,\n",
    "\n",
    "        sentence_encoder_hidden_dim = sentence_encoder_hidden_dim,\n",
    "        sentence_encoder_num_layers = sentence_encoder_num_layers,\n",
    "        sentence_encoder_bidirectional= sentence_encoder_bidirectional,\n",
    "        sentence_encoder_attention_dim = sentence_encoder_hidden_dim,\n",
    "        rnn_dropout = hierarchial_model_rnn_dropout,\n",
    "        fc_dropout = hierarchial_model_fc_dropout,\n",
    "        train_device = device_cpu\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_name,map_location=device_cpu))\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    for i in range(len(test_data)):\n",
    "        ans = model(test_data[i],[test_lengths[i]],sentence_lengths[i])\n",
    "        ans = torch.round(ans)\n",
    "        if ans[0][0] == test_labels[i]:\n",
    "            count+=1\n",
    "    \n",
    "    print(\"Accuracy = \" + str((count/len(test_data)*100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/depressedcoder/anaconda3/envs/pytorchenv/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_word_embeddings = [] \n",
    "test_n_sents = []\n",
    "test_sentence_lengths = []\n",
    "test_dataset_labels = []  \n",
    "\n",
    "def getAssignmentTestData():\n",
    "    test_processed_text = []\n",
    "    with open(\"./E0334 Assignment2 Test Dataset.csv\",encoding='utf-8') as csvfile:\n",
    "        csvFile = csv.reader(csvfile)\n",
    "        next(csvFile)\n",
    "        for line in csvFile:\n",
    "            processed_text = preprocess_text(line[0])\n",
    "            label = 1.0 if line[1] == 'positive' else 0.0\n",
    "            test_dataset_labels.append(label)\n",
    "            test_processed_text.append(processed_text)\n",
    "\n",
    "    for i in range(len(test_processed_text)):\n",
    "        current_embeddings,current_sent_lengths,current_n_sent = review_to_embed(test_processed_text[i]) \n",
    "        test_word_embeddings.append(current_embeddings.clone().detach().unsqueeze(0))\n",
    "        test_n_sents.append(current_n_sent)\n",
    "        test_sentence_lengths.append([current_sent_lengths])\n",
    "\n",
    "getAssignmentTestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 89.54895489548956\n"
     ]
    }
   ],
   "source": [
    "test('./300_256_2_True_256_256_2_True_256_0.0_0.3.pth',test_word_embeddings,test_sentence_lengths,test_n_sents,test_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gymenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b0110cf1cb03549be737c5657a86ea4daeeb81469a7991ed915d907f3e629c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
