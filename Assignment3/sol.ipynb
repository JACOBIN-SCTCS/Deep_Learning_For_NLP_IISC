{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import spacy\n",
    "import jsonlines\n",
    "import json\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence,pack_padded_sequence,pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import fasttext\n",
    "from torch.utils.data import SubsetRandomSampler,DataLoader,Subset\n",
    "from torchtext.vocab import GloVe\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "\n",
    "\n",
    "\n",
    "#   Which Dataset to use\n",
    "#   1 = Assignment 1 Training Data\n",
    "#   2 = Assignment 2 Training Data\n",
    "\n",
    "# SENTENCE_SPLITTING_USED; whether to use the splitting of reviews into sentences.\n",
    "\n",
    "DATASET_TO_USE = 2\n",
    "SENTENCE_SPLITTING_USED = True\n",
    "\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 128\n",
    "NUM_FILTERS = 86\n",
    "FILTER_SIZES = [3,4,5,7]\n",
    "CNN_DIM = NUM_FILTERS*len(FILTER_SIZES)\n",
    "\n",
    "PATIENCE_PARAMETER = 7\n",
    "VALIDATION_LOSS_COMPUTE_STEP = 1\n",
    "\n",
    "device_cpu = torch.device('cpu')\n",
    "device_fast = torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "if torch.has_mps:\n",
    "    device_fast = torch.device('mps')\n",
    "elif torch.has_cuda:\n",
    "    device_fast = torch.device('cuda')\n",
    "\n",
    "#torch.manual_seed(0)\n",
    "#np.random.seed(0)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "glove = GloVe()\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):    \n",
    "    text = re.sub(r'<br /><br />',\".\",text)\n",
    "    text = BeautifulSoup(text,'lxml').get_text().strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = ' '.join(re.findall(r\"[\\w']+|[.,!;/\\\"]\", text))\n",
    "    \n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word == '':\n",
    "            continue\n",
    "        new_text.append(word)\n",
    "    \n",
    "    text = ' '.join(new_text)\n",
    "    words = nlp(text)\n",
    "    text =  \" \".join([token.text for token in words if not token.is_punct or token.text=='/' or token.text==\"\\\"\" or token.text==\".\"]).strip()\n",
    "    new_words = []\n",
    "    for word in text.split(\" \"):\n",
    "        if word == 'n\\'t':\n",
    "            if len(new_words) > 1:\n",
    "                new_words[-1] = new_words[-1] + word\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    \n",
    "    text = \" \".join(new_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# preprocess the training data which was given for Assignment 1\n",
    "def process_assignment1_training_data():\n",
    "    negative_reviews = io.open('./Train.neg',encoding='latin-1').readlines()\n",
    "    positive_reviews = io.open('./Train.pos',encoding='latin-1').readlines()\n",
    "    with jsonlines.open('train.jsonl',mode='w') as writer:\n",
    "\n",
    "        for review in positive_reviews:\n",
    "            processed_text = preprocess_text(review)\n",
    "            d = {'text': processed_text , 'label': 1.0}\n",
    "            writer.write(d)\n",
    "        for review in negative_reviews:\n",
    "            processed_text = preprocess_text(review)\n",
    "            d = {'text': processed_text , 'label': 0.0}\n",
    "            writer.write(d)\n",
    "\n",
    "# preprocess the training data which was given for Assignment 2\n",
    "def process_assignment2_training_data():\n",
    "    preprocessed_dataset = []\n",
    "    train_dataset_labels = []\n",
    "    with open(\"./Train dataset.csv\") as csvfile:\n",
    "        csvFile = csv.reader(csvfile)\n",
    "        next(csvFile)\n",
    "        json_writer = jsonlines.open('processed_dataset.jsonl','w')\n",
    "\n",
    "        for line in csvFile:\n",
    "            processed_text = preprocess_text(line[0])\n",
    "            label = 1.0 if line[1] == 'positive' else 0.0\n",
    "            train_dataset_labels.append(label)\n",
    "            json_writer.write({\"text\":processed_text,\"label\":label})\n",
    "            preprocessed_dataset.append({\"text\":processed_text,\"label\":label})\n",
    "    \n",
    "        json_writer.close()\n",
    "\n",
    "if DATASET_TO_USE == 1:\n",
    "    process_assignment1_training_data()\n",
    "else:\n",
    "    process_assignment2_training_data()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = []\n",
    "train_dataset_labels = []\n",
    "\n",
    "\n",
    "TRAIN_FILE_NAME = './train.jsonl' if DATASET_TO_USE==1 else './processed_dataset.jsonl'\n",
    "\n",
    "with open(TRAIN_FILE_NAME ,encoding='utf-8') as f:\n",
    "#with open('processed_dataset.jsonl',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sample = json.loads(line)\n",
    "        train_dataset_labels.append(sample['label'])\n",
    "        preprocessed_dataset.append(sample)\n",
    "      \n",
    "train_dataset_labels = np.array(train_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordEmbeddingforText(text,glove=glove):\n",
    "    length = 0\n",
    "    words = []\n",
    "    text = text.strip()\n",
    "    for word in text.split(' '):\n",
    "        length+=1\n",
    "        word_embedding = glove[word]\n",
    "        words.append(word_embedding)\n",
    "\n",
    "    # If the number of words in a sentence is below 3\n",
    "    if length < max(FILTER_SIZES):\n",
    "        words.append(torch.zeros(EMBED_DIM))\n",
    "        length+=1\n",
    "    \n",
    "    return torch.stack(words),length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_embed(review,glove=glove): \n",
    "    sentences = review.split(\".\")\n",
    "    sentence_lengths = []\n",
    "    review_embeddings = []\n",
    "    num_sentences = 0\n",
    "    for sentence in sentences:\n",
    "        if sentence == '':\n",
    "            continue\n",
    "        s= sentence.strip()\n",
    "        num_sentences += 1\n",
    "        sentence_word_embeddings,sentence_length = getWordEmbeddingforText(s,glove)\n",
    "        sentence_lengths.append(sentence_length)\n",
    "        review_embeddings.append(sentence_word_embeddings)\n",
    "\n",
    "    return torch.nn.utils.rnn.pad_sequence(review_embeddings,batch_first=True),sentence_lengths,num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self,reviews):\n",
    "        super().__init__()\n",
    "        self.reviews = reviews\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.reviews[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for review in preprocessed_dataset:\n",
    "    if SENTENCE_SPLITTING_USED:\n",
    "        embeddings, sent_length ,n_sents = review_to_embed(review['text'])\n",
    "        processed_dataset.append({'review': embeddings,'sent_lengths': sent_length,'length' : n_sents,'label' : review['label']})\n",
    "    else:\n",
    "        embedding,length = getWordEmbeddingforText(review['text'])\n",
    "        processed_dataset.append({'review': embedding,'length': length,'label' : review['label']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewDataSet(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_no_sentence_split(batch_data):\n",
    "    inputs = [b['review'] for b in batch_data]\n",
    "    lengths = [b['length'] for b in batch_data]\n",
    "    labels = torch.tensor([b['label'] for b in batch_data])\n",
    "\n",
    "    labels = labels.unsqueeze(1)\n",
    "    inputs = pad_sequence(inputs,batch_first=True)\n",
    "    return  {'input' : inputs , 'lengths': lengths , 'labels' : labels }\n",
    "\n",
    "\n",
    "\n",
    "def collate_function(batch_data):   \n",
    "    inputs = [b['review'] for b in batch_data]\n",
    "    sent_lengths = [ b['sent_lengths'] for b in batch_data ]\n",
    "    n_sentences = [ b['length'] for b in batch_data ]\n",
    "    \n",
    "    labels = torch.tensor([b['label'] for b in batch_data])\n",
    "\n",
    "    labels = labels.unsqueeze(1)\n",
    "    \n",
    "    max_n_sentences = max([i.shape[0] for i in inputs] )\n",
    "    max_n_words = max([i.shape[1] for i in inputs])\n",
    "\n",
    " \n",
    "    processed_inputs = []\n",
    "    for inp in inputs:\n",
    "\n",
    "        t1 = torch.permute(inp,(2,1,0))\n",
    "        t1 = torch.nn.functional.pad(t1,(0,max_n_sentences-inp.shape[0],0,max_n_words-inp.shape[1]))\n",
    "        t1 = torch.permute(t1,(2,1,0))\n",
    "        processed_inputs.append(t1)\n",
    "\n",
    "    final_inp = torch.stack(processed_inputs)\n",
    "    #inputs = pad_sequence(inputs,batch_first=True)\n",
    "    return  {'input' : final_inp , 'sent_lengths': sent_lengths , 'lengths' : n_sentences ,'labels' : labels }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx,valid_idx = train_test_split(np.arange(train_dataset_labels.shape[0]), \n",
    "    test_size=0.2,\n",
    "    shuffle= True,\n",
    "    stratify= train_dataset_labels,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "if SENTENCE_SPLITTING_USED:\n",
    "    train_dataloader = DataLoader(dataset,16,sampler=train_sampler,collate_fn=collate_function)\n",
    "    valid_dataloader = DataLoader(dataset,16,sampler=valid_sampler,collate_fn=collate_function)\n",
    "else:\n",
    "    train_dataloader = DataLoader(dataset,64,sampler=train_sampler,collate_fn=collate_fn_no_sentence_split)\n",
    "    valid_dataloader = DataLoader(dataset,64,sampler=valid_sampler,collate_fn=collate_fn_no_sentence_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "    def __init__(self,embed_dim=EMBED_DIM,hidden_dim = HIDDEN_DIM, droput_prob = 0.3, train_device = device_cpu):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(self.embed_dim,self.hidden_dim)\n",
    "        #self.fc = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim,1)\n",
    "        self.sigmoid  = nn.Sigmoid()\n",
    "        self.word_dropout_prob = droput_prob\n",
    "\n",
    "        self.train_device = train_device\n",
    "\n",
    "\n",
    "    def forward(self,inp,inp_len):    \n",
    "        inp_mask = torch.ones((inp.shape[0],inp.shape[1]))\n",
    "        for i  in range(inp.shape[0]):\n",
    "            inp_mask[i,inp_len[i]:] = 0.0\n",
    "\n",
    "        inp_mask = inp_mask.to(self.train_device)\n",
    "        inp_lengths = torch.sum(inp_mask,-1,keepdim=True).int()\n",
    "        inp_lengths = inp_lengths.to(self.train_device)\n",
    "        total = torch.sum(inp*(inp_mask.unsqueeze(2)),axis=1)\n",
    "        vector_average = total / inp_lengths\n",
    "        ans = F.relu(self.fc1(vector_average))\n",
    "        #ans = F.relu(self.fc(ans))\n",
    "        ans = self.sigmoid(self.fc2(ans))\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "            embed_dim=EMBED_DIM,hidden_dim =HIDDEN_DIM,bidirectional=False,\n",
    "            rnn_type = 'gru',num_layers=1,rnn_dropout = 0.4,fc_dropout = 0.3):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = rnn_dropout\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "\n",
    "        self.rnn = None\n",
    "        \n",
    "        if rnn_type == 'gru':\n",
    "            self.rnn  = nn.GRU(input_size = self.embed_dim,\n",
    "                hidden_size = self.hidden_dim,\n",
    "                num_layers = self.num_layers,\n",
    "                batch_first = True,\n",
    "                dropout = self.dropout,\n",
    "                bidirectional = self.bidirectional\n",
    "            )\n",
    "        elif rnn_type == 'rnn':\n",
    "            self.rnn = nn.RNN(input_size = self.embed_dim,\n",
    "                hidden_size = self.hidden_dim,\n",
    "                num_layers = self.num_layers,\n",
    "                batch_first = True,\n",
    "                dropout = self.dropout,\n",
    "                bidirectional = self.bidirectional)\n",
    "        \n",
    "\n",
    "        self.bidirectional_factor = 2 if self.bidirectional else 1\n",
    "        self.dnn_input_size= self.bidirectional_factor * self.num_layers * self.hidden_dim\n",
    "        self.fc_list = []\n",
    "\n",
    "        log_base_2 = np.log2(self.dnn_input_size)\n",
    "        nearest_power_2 = int(log_base_2)\n",
    "\n",
    "        if(float(nearest_power_2) != log_base_2):\n",
    "            self.fc_list.append(nn.Linear(self.dnn_input_size,2**nearest_power_2))\n",
    "\n",
    "        while nearest_power_2 > 7 :\n",
    "            self.fc_list.append(nn.Linear(2**(nearest_power_2),2**(nearest_power_2-1)))\n",
    "            nearest_power_2-=1\n",
    "        \n",
    "        self.fc_list.append(nn.Linear(128,1))\n",
    "        self.fc = nn.ModuleList(self.fc_list)\n",
    "\n",
    "        #self.fc1 = nn.Linear(self.dnn_input_size,128)\n",
    "        #self.fc2 = nn.Linear(128,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x,x_len):\n",
    "\n",
    "        packed_input = pack_padded_sequence(x,x_len,batch_first=True,enforce_sorted=False)\n",
    "        packed_output,hidden = self.rnn(packed_input)\n",
    "        output,output_lengths = pad_packed_sequence(packed_output)\n",
    "        #hidden = hidden.squeeze()\n",
    "        hidden = torch.permute(hidden,(1,0,2))\n",
    "        hidden = hidden.contiguous().view((hidden.shape[0],-1))\n",
    "        out = hidden\n",
    "        for i,l in enumerate(self.fc):\n",
    "            if i!=(len(self.fc_list)-1):\n",
    "                out = self.fc_dropout(l(out))\n",
    "            \n",
    "        #out = self.fc_dropout(self.fc1(hidden))\n",
    "        ans = self.sigmoid(self.fc_list[len(self.fc_list)-1](out))\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD Dropout Term\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim=EMBED_DIM,hidden_dim = HIDDEN_DIM,filter_sizes = FILTER_SIZES, n_filters = NUM_FILTERS,dropout = 0.2):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = n_filters\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.modulelist = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=self.embed_dim,\n",
    "            out_channels=self.num_filters,\n",
    "            kernel_size= self.filter_sizes[i]\n",
    "            )\n",
    "    \n",
    "        for i in range(len(self.filter_sizes))])\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.num_filters*len(self.filter_sizes),hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x : torch.Tensor,xlen = None):\n",
    "        \n",
    "        permuted_x = x.permute(0,2,1)\n",
    "        x_conv_list = [F.relu(conv(permuted_x)) for conv in self.modulelist]\n",
    "        x_max_pool_list = [F.max_pool1d(x_conv,kernel_size=x_conv.shape[2]) for x_conv in x_conv_list]\n",
    "        \n",
    "        feature = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_max_pool_list],dim=1)\n",
    "        \n",
    "        out = self.dropout(F.relu(self.fc1(feature)))\n",
    "        out = self.fc2(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "\n",
    "cnnmodel = CNNModel(10,2,FILTER_SIZES,NUM_FILTERS)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'n_sent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vivitsu329\\Documents\\GitHub\\Deep_Learning_For_NLP_IISC\\Assignment3\\sol.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m batch_data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_dataloader))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m j \u001b[39m=\u001b[39m EnsembleModel(EMBED_DIM,CNN_DIM,HIDDEN_DIM)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X15sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m j(batch_data[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m],batch_data[\u001b[39m'\u001b[39;49m\u001b[39mn_sent\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'n_sent'"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class EnsembleModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,EMBED_DIM,CNN_DIM,HIDDEN_DIM):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_size = CNN_DIM,hidden_size = HIDDEN_DIM, batch_first = True)\n",
    "        self.cnn = nn.Conv1d(in_channels=EMBED_DIM,out_channels=CNN_DIM,kernel_size=3)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM,1)\n",
    "\n",
    "\n",
    "    def forward(self,inp : torch.Tensor,n_sents=None):\n",
    "\n",
    "        ## inp  = (batch_size,max_sent_length,max_word_length,embed_dim)\n",
    "\n",
    "        outputs = []\n",
    "      \n",
    "        \n",
    "        for i in range(inp.shape[1]):\n",
    "            current_inp = inp[:,i,:,:]\n",
    "            current_inp = torch.permute(current_inp,(0,2,1))\n",
    "            current_output = self.cnn(current_inp)\n",
    "            current_output = F.max_pool1d(current_output,kernel_size = current_output.shape[2]).squeeze(dim=2)\n",
    "            outputs.append(current_output)\n",
    "        \n",
    "        #print(len(outputs))\n",
    "        #print(outputs[0].shape)\n",
    "        lstm_in = torch.stack(outputs,dim=1)\n",
    "   \n",
    "        packed_input = pack_padded_sequence(lstm_in,n_sents,batch_first=True,enforce_sorted=False)\n",
    "        packed_output,hidden = self.rnn(packed_input)\n",
    "        output,output_lengths = pad_packed_sequence(packed_output,batch_first=True)\n",
    "\n",
    "        hidden = torch.permute(hidden,(1,0,2))\n",
    "        hidden = hidden.contiguous().view((hidden.shape[0],-1))\n",
    "\n",
    "        out = self.fc(hidden)\n",
    "        return nn.Sigmoid()(out)\n",
    "        #out = self.cnn(inp)\n",
    "        #return out\n",
    "\n",
    "batch_data = next(iter(train_dataloader))\n",
    "j = EnsembleModel(EMBED_DIM,CNN_DIM,HIDDEN_DIM)\n",
    "j(batch_data['input'],batch_data['n_sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class CNNLSTMAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,EMBED_DIM,CNN_DIM,HIDDEN_DIM,bidirectional_factor = 2,fc_dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        bidirectional = False\n",
    "\n",
    "        if bidirectional_factor==2:\n",
    "            bidirectional = True\n",
    "\n",
    "        self.rnn = nn.GRU(input_size = CNN_DIM,hidden_size = HIDDEN_DIM, bidirectional=bidirectional,batch_first = True)\n",
    "        \n",
    "        cnn_layers = [  \n",
    "            nn.Conv1d(in_channels=EMBED_DIM,out_channels=NUM_FILTERS,kernel_size=FILTER_SIZES[i]) for i in range(len(FILTER_SIZES))\n",
    "        ]\n",
    "        self.cnn_list = nn.ModuleList(cnn_layers)\n",
    "    \n",
    "        #self.cnn = nn.Conv1d(in_channels=EMBED_DIM,out_channels=CNN_DIM,kernel_size=3)\n",
    "        self.attention_layer = nn.Linear(bidirectional_factor* HIDDEN_DIM,1)\n",
    "        self.fc = nn.Linear(bidirectional_factor*HIDDEN_DIM,HIDDEN_DIM)\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
    "        self.out_fc = nn.Linear(HIDDEN_DIM,1)\n",
    "        self.batchnorm1d = nn.BatchNorm1d(CNN_DIM)\n",
    "\n",
    "    def forward(self,inp : torch.Tensor,n_sents=None):\n",
    "\n",
    "        ## inp  = (batch_size,max_sent_length,max_word_length,embed_dim)\n",
    "\n",
    "        outputs = []\n",
    "      \n",
    "        \n",
    "        for i in range(inp.shape[1]):\n",
    "            current_inp = inp[:,i,:,:]\n",
    "            current_inp = torch.permute(current_inp,(0,2,1))\n",
    "            \n",
    "            current_output = None\n",
    "            for cnn in self.cnn_list:\n",
    "                current_out = cnn(current_inp)\n",
    "                current_out = F.max_pool1d(current_out,kernel_size=current_out.shape[2]).squeeze(dim=2)\n",
    "\n",
    "                if current_output is None:\n",
    "                    current_output = current_out\n",
    "                else:\n",
    "                    current_output = torch.cat([current_output,current_out],dim=1)\n",
    "                #current_output = self.cnn(current_inp)\n",
    "                #current_output = F.max_pool1d(current_output,kernel_size = current_output.shape[2]).squeeze(dim=2)\n",
    "\n",
    "            outputs.append(current_output)\n",
    "        \n",
    "        #print(len(outputs))\n",
    "        #print(outputs[0].shape)\n",
    "        lstm_in = torch.stack(outputs,dim=2)\n",
    "        lstm_in = self.batchnorm1d(lstm_in)\n",
    "        lstm_in = torch.permute(lstm_in,(0,2,1))\n",
    "\n",
    "        packed_input = pack_padded_sequence(lstm_in,n_sents,batch_first=True,enforce_sorted=False)\n",
    "        packed_output,hidden = self.rnn(packed_input)\n",
    "        output,output_lengths = pad_packed_sequence(packed_output,batch_first=True)\n",
    "        attention_logs = self.attention_layer(output).squeeze(dim=2)\n",
    "        attention_score = F.softmax(attention_logs,dim=1).unsqueeze(2)\n",
    "\n",
    "        final_out = attention_score*output\n",
    "\n",
    "        averaged_vector = torch.sum(final_out,dim=1,keepdim=False)\n",
    "\n",
    "        #hidden = torch.permute(hidden,(1,0,2))\n",
    "        #hidden = hidden.contiguous().view((hidden.shape[0],-1))\n",
    "        out = self.fc_dropout(F.leaky_relu(self.fc(averaged_vector)))\n",
    "        out = self.out_fc(out)\n",
    "        return nn.Sigmoid()(out)\n",
    "        #out = self.cnn(inp)\n",
    "        #return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 58, 65, 300])\n",
      "[14, 30, 36, 29, 8, 11, 55, 14, 5, 10, 58, 13, 14, 8, 15, 20]\n"
     ]
    }
   ],
   "source": [
    "print(batch_data['input'].shape)\n",
    "print(batch_data['lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4688],\n",
       "        [0.4460],\n",
       "        [0.4683],\n",
       "        [0.4364],\n",
       "        [0.4712],\n",
       "        [0.4773],\n",
       "        [0.4318],\n",
       "        [0.4664],\n",
       "        [0.4781],\n",
       "        [0.4691],\n",
       "        [0.5031],\n",
       "        [0.4612],\n",
       "        [0.4626],\n",
       "        [0.4737],\n",
       "        [0.4584],\n",
       "        [0.4693]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cnn = CNNLSTMAttention(EMBED_DIM,CNN_DIM,HIDDEN_DIM)\n",
    "cnn(batch_data['input'],batch_data['lengths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import  datetime\n",
    "\n",
    "def train(model,train_dataloader,valid_dataloader,num_epochs,criterion,optimizer,\n",
    "    checkpoint_name='best_model.pt',\n",
    "    device_train = device_fast,use_rnn = False,log=True):\n",
    "\n",
    "    tensorboard_name='Ensemble'\n",
    "    if log == True:\n",
    "        current_datetime = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "        tensorboard_name = tensorboard_name + \"_\" + current_datetime\n",
    "        writer = SummaryWriter('runs/' + tensorboard_name)\n",
    "    \n",
    "    \n",
    "    model = model.to(device_train)\n",
    "    clip = 0\n",
    "    if use_rnn:\n",
    "        clip = 5\n",
    "\n",
    "    best_validation_loss = 1000.0\n",
    "    valdiation_loss_not_decreased_steps = 0\n",
    "    \n",
    "    model.train()\n",
    "    for e in range(num_epochs):\n",
    "        \n",
    "        training_set_size = 0\n",
    "        training_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for data in tqdm(train_dataloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            if SENTENCE_SPLITTING_USED:\n",
    "                input_reviews,sent_lengths,n_sents,output_labels = data['input'], data['sent_lengths'],data['lengths'],data['labels']\n",
    "            else:\n",
    "                input_reviews,n_sents,output_labels = data['input'],data['lengths'],data['labels']\n",
    "\n",
    "            input_reviews = input_reviews.to(device_train)\n",
    "            training_set_size += input_reviews.shape[0]\n",
    "            output = model(input_reviews,n_sents)\n",
    "            output = output.to(device_cpu)\n",
    "            loss = criterion(output,output_labels.float())\n",
    "            training_loss += loss.item()\n",
    "            loss.backward()\n",
    "            if use_rnn:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "            optimizer.step()\n",
    "        \n",
    "        current_training_loss = training_loss\n",
    "        if log==True:\n",
    "            print(\"Epoch \" + str(e) + \" Average Training Loss = \" +  str(current_training_loss))\n",
    "            writer.add_scalars(tensorboard_name + 'Training Loss vs Epoch',{'train' : current_training_loss},e)\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        if valid_dataloader is None:\n",
    "            continue\n",
    "        \n",
    "        validation_set_size  = 0 \n",
    "        if e% VALIDATION_LOSS_COMPUTE_STEP==0:\n",
    "            correct_count = 0\n",
    "            validation_loss = 0\n",
    "\n",
    "            for i,data in enumerate(valid_dataloader,0):\n",
    "                if SENTENCE_SPLITTING_USED:\n",
    "                    input_reviews,sent_lengths,n_sents,output_labels = data['input'], data['sent_lengths'],data['lengths'],data['labels']\n",
    "                else:\n",
    "                    input_reviews,n_sents,output_labels = data['input'],data['lengths'],data['labels']\n",
    "                \n",
    "                input_reviews = input_reviews.to(device_train)\n",
    "                validation_set_size += input_reviews.shape[0]\n",
    "                output = model(input_reviews,n_sents)\n",
    "                output = output.to(device_cpu)\n",
    "                loss = criterion(output,output_labels.float())\n",
    "                validation_loss += loss.item()\n",
    "                nearest_class = torch.round(output)\n",
    "\n",
    "                correct = (nearest_class == output_labels.float()).float()\n",
    "                correct_count += correct.sum()\n",
    "            correct_count = int(correct_count)\n",
    "            current_validation_accuracy = (correct_count/validation_set_size)*100\n",
    "            current_validation_loss = (1.0* validation_loss)\n",
    "            if log == True:\n",
    "                print(\"Epoch \" + str(e) + \" \" +  \"Validation Loss = \" + str(current_validation_loss) )\n",
    "                print(\"Validation Set Accuracy = \" + str((correct_count/validation_set_size)*100) )\n",
    "                writer.add_scalar(tensorboard_name + ' Validation Accuracy vs Epoch ',(correct_count/validation_set_size*100),e)\n",
    "                writer.add_scalars(tensorboard_name + 'Validation Loss vs Epoch',{'valid' : current_validation_loss},e)\n",
    "\n",
    "            \n",
    "            if log==True:\n",
    "                if current_validation_loss < best_validation_loss:\n",
    "                    valdiation_loss_not_decreased_steps = 0\n",
    "                    torch.save(model.state_dict(),checkpoint_name)\n",
    "                    best_validation_loss = current_validation_loss\n",
    "                else:\n",
    "                    valdiation_loss_not_decreased_steps +=1\n",
    "        if log == True:\n",
    "            if valdiation_loss_not_decreased_steps >= PATIENCE_PARAMETER:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [09:00<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Average Training Loss = 730.8767990171909\n",
      "Epoch 0 Validation Loss = 135.62008828297257\n",
      "Validation Set Accuracy = 89.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:35<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Average Training Loss = 480.47654472664\n",
      "Epoch 1 Validation Loss = 126.14620087854564\n",
      "Validation Set Accuracy = 89.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:41<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Average Training Loss = 402.75145566184074\n",
      "Epoch 2 Validation Loss = 125.95687280595303\n",
      "Validation Set Accuracy = 90.14999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:36<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Average Training Loss = 320.07007088838145\n",
      "Epoch 3 Validation Loss = 195.673126203008\n",
      "Validation Set Accuracy = 84.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 111/2000 [00:33<09:28,  3.33it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.64 GiB (GPU 0; 2.00 GiB total capacity; 157.35 MiB already allocated; 536.18 MiB free; 444.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vivitsu329\\Documents\\GitHub\\Deep_Learning_For_NLP_IISC\\Assignment3\\sol.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m optimizer\u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(net\u001b[39m.\u001b[39mparameters(),lr\u001b[39m=\u001b[39m\u001b[39m0.0054\u001b[39m,momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m,nesterov\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mCyclicLR(optimizer, base_lr\u001b[39m=\u001b[39m\u001b[39m0.0054\u001b[39m, max_lr\u001b[39m=\u001b[39m\u001b[39m0.0072\u001b[39m,step_size_up\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train(net,train_dataloader,valid_dataloader,\u001b[39m100\u001b[39;49m,nn\u001b[39m.\u001b[39;49mBCELoss(),optimizer,\u001b[39m'\u001b[39;49m\u001b[39mtest_cnn_rnn_att_adam_batch_nrom_cyclelr_bidir_0.0054.pt\u001b[39;49m\u001b[39m'\u001b[39;49m,device_fast,\u001b[39mTrue\u001b[39;49;00m,\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\Vivitsu329\\Documents\\GitHub\\Deep_Learning_For_NLP_IISC\\Assignment3\\sol.ipynb Cell 16\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, valid_dataloader, num_epochs, criterion, optimizer, checkpoint_name, device_train, use_rnn, log)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X21sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     input_reviews,n_sents,output_labels \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m],data[\u001b[39m'\u001b[39m\u001b[39mlengths\u001b[39m\u001b[39m'\u001b[39m],data[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m input_reviews \u001b[39m=\u001b[39m input_reviews\u001b[39m.\u001b[39;49mto(device_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m training_set_size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m input_reviews\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X21sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m output \u001b[39m=\u001b[39m model(input_reviews,n_sents)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.64 GiB (GPU 0; 2.00 GiB total capacity; 157.35 MiB already allocated; 536.18 MiB free; 444.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "net = CNNLSTMAttention(EMBED_DIM,CNN_DIM,HIDDEN_DIM)\n",
    "optimizer= optim.SGD(net.parameters(),lr=0.0054,momentum=0.9,nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0054, max_lr=0.0072,step_size_up=10000)\n",
    "train(net,train_dataloader,valid_dataloader,100,nn.BCELoss(),optimizer,'test_cnn_rnn_att_adam_batch_nrom_cyclelr_bidir_0.0054.pt',device_fast,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 2.00 GiB total capacity; 1.64 GiB already allocated; 0 bytes free; 1.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Vivitsu329\\Documents\\GitHub\\Deep_Learning_For_NLP_IISC\\Assignment3\\sol.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m net \u001b[39m=\u001b[39m EnsembleModel(EMBED_DIM,CNN_DIM,HIDDEN_DIM)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train(net,train_dataloader,valid_dataloader,\u001b[39m50\u001b[39;49m,nn\u001b[39m.\u001b[39;49mBCELoss(),optim\u001b[39m.\u001b[39;49mAdam(net\u001b[39m.\u001b[39;49mparameters(),\u001b[39m0.001\u001b[39;49m),\u001b[39m'\u001b[39;49m\u001b[39mcnn_rnn_dnn_adam.pt\u001b[39;49m\u001b[39m'\u001b[39;49m,device_fast,\u001b[39mTrue\u001b[39;49;00m,\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Users\\Vivitsu329\\Documents\\GitHub\\Deep_Learning_For_NLP_IISC\\Assignment3\\sol.ipynb Cell 17\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, valid_dataloader, num_epochs, criterion, optimizer, checkpoint_name, device_train, use_rnn, log)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X22sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X22sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m input_reviews,sent_lengths,n_sents,output_labels \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m], data[\u001b[39m'\u001b[39m\u001b[39msent_lengths\u001b[39m\u001b[39m'\u001b[39m],data[\u001b[39m'\u001b[39m\u001b[39mn_sent\u001b[39m\u001b[39m'\u001b[39m],data[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X22sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m input_reviews \u001b[39m=\u001b[39m input_reviews\u001b[39m.\u001b[39;49mto(device_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X22sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m training_set_size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m input_reviews\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Vivitsu329/Documents/GitHub/Deep_Learning_For_NLP_IISC/Assignment3/sol.ipynb#X22sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m output \u001b[39m=\u001b[39m model(input_reviews,n_sents)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 46.00 MiB (GPU 0; 2.00 GiB total capacity; 1.64 GiB already allocated; 0 bytes free; 1.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "net = EnsembleModel(EMBED_DIM,CNN_DIM,HIDDEN_DIM)\n",
    "train(net,train_dataloader,valid_dataloader,50,nn.BCELoss(),optim.Adam(net.parameters(),0.001),'cnn_rnn_dnn_adam.pt',device_fast,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_name,test_data,test_lengths,test_labels):\n",
    "    model = CNNLSTMAttention(EMBED_DIM,CNN_DIM,HIDDEN_DIM)\n",
    "    model.load_state_dict(torch.load(model_name,map_location=device_cpu))\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    for i in range(len(test_data)):\n",
    "        ans = model(test_data[i],[test_lengths[i]])\n",
    "        ans = torch.round(ans)\n",
    "        if ans[0][0] == test_labels[i]:\n",
    "            count+=1\n",
    "    \n",
    "    print(\"Accuracy = \" + str((count/len(test_data)*100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vivitsu329\\Documents\\environments\\nlpenv\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_word_embeddings = [] \n",
    "test_sentence_lengths = []\n",
    "test_dataset_labels = []  \n",
    "\n",
    "def getAssignment2TestData():\n",
    "    test_processed_text = []\n",
    "    with open(\"./E0334 Assignment2 Test Dataset.csv\",encoding='utf-8') as csvfile:\n",
    "        csvFile = csv.reader(csvfile)\n",
    "        next(csvFile)\n",
    "        for line in csvFile:\n",
    "            processed_text = preprocess_text(line[0])\n",
    "            label = 1.0 if line[1] == 'positive' else 0.0\n",
    "            test_dataset_labels.append(label)\n",
    "            test_processed_text.append(processed_text)\n",
    "\n",
    "    for i in range(len(test_processed_text)):\n",
    "        if SENTENCE_SPLITTING_USED:\n",
    "            current_embeddings,current_sent_lengths,current_n_sent = review_to_embed(test_processed_text[i]) \n",
    "        else:\n",
    "            current_embeddings,current_n_sent = getWordEmbeddingforText(test_processed_text[i])\n",
    "\n",
    "        test_word_embeddings.append(current_embeddings.clone().detach().unsqueeze(0))\n",
    "        test_sentence_lengths.append(current_n_sent)\n",
    "\n",
    "\n",
    "def getAssignment1TestData():\n",
    "    \n",
    "    correct_count = 0\n",
    "\n",
    "    reviews = open('./TestData','r',encoding='latin-1').readlines()\n",
    "    for i in range(len(reviews)):\n",
    "        r = reviews[i]\n",
    "        reviews[i] = preprocess_text(r)\n",
    "\n",
    "        if SENTENCE_SPLITTING_USED:\n",
    "            current_embeddings,current_sent_lengths,current_n_sent = review_to_embed(reviews[i]) \n",
    "        else:\n",
    "            current_embeddings,current_n_sent = getWordEmbeddingforText(reviews[i])\n",
    "      \n",
    "        if (i<331):\n",
    "            test_dataset_labels.append(1.0)\n",
    "        else:\n",
    "            test_dataset_labels.append(0.0)\n",
    "        test_word_embeddings.append(current_embeddings.clone().detach().unsqueeze(0))\n",
    "        test_sentence_lengths.append(current_n_sent)\n",
    "\n",
    "getAssignment2TestData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 89.6989698969897\n"
     ]
    }
   ],
   "source": [
    "test('./test_cnn_rnn_att_adam_batch_nrom_cyclelr_bidir_0.0054.pt',test_word_embeddings,test_sentence_lengths,test_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 80.76923076923077\n"
     ]
    }
   ],
   "source": [
    "test('./first_cnn_rnn_att_adam_batch_nrom_cyclelr_bidir_0.0054.pt',test_word_embeddings,test_sentence_lengths,test_dataset_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b0110cf1cb03549be737c5657a86ea4daeeb81469a7991ed915d907f3e629c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
