{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchtext.vocab import GloVe,FastText\n",
    "import spacy\n",
    "import io\n",
    "import jsonlines\n",
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import random_split\n",
    "import fasttext\n",
    "\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "if torch.has_mps:\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_LAYER_DIM = 100\n",
    "NUM_EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model('./crawl-300d-2M-subword/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_word_vector('asfkskfjkeglaekjghgidfg').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FastText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.__getitem__('it\\'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3640, -0.4614, -0.2887,  0.2210,  0.3395,  0.2251, -0.2298,  0.2102,\n",
       "        -0.1499, -1.2309,  0.1906, -0.6621,  0.2563,  0.1245, -0.0430, -0.4992,\n",
       "        -0.1759, -1.3224,  0.1590, -0.0765, -0.3088,  0.0463, -0.1286,  0.1315,\n",
       "        -0.1142, -0.4390,  0.5309, -0.2725,  0.1481, -0.3749,  0.2236, -0.4525,\n",
       "         0.8860, -0.0410, -0.4543, -0.0826,  0.4276, -0.3297,  0.2673,  0.3213,\n",
       "        -0.3717,  0.6772, -0.4830,  0.0100,  0.3571, -0.0447,  0.3538, -0.9765,\n",
       "        -0.1465,  0.0931, -0.4583,  0.0137, -0.0181,  0.0088,  0.7065, -0.4261,\n",
       "         0.1341, -0.5798,  0.2476, -0.1317, -0.1288, -0.6861, -0.5583, -0.0515,\n",
       "        -0.0277,  0.1584,  0.1007,  0.1715, -0.4610, -0.3673, -0.0540,  0.0867,\n",
       "        -0.2057, -0.0238,  0.0137,  0.1134,  0.1019,  0.1092, -0.6096,  0.0876,\n",
       "         0.1344, -0.0335, -0.6103,  0.5123,  0.6763,  0.3822,  0.7131, -1.2000,\n",
       "         0.1577,  0.2205, -1.1074, -0.0709, -0.3897,  0.6892,  0.1607,  0.8657,\n",
       "        -0.1863, -0.3489,  0.1519,  0.2079, -0.9941,  0.1911,  0.0782, -0.0688,\n",
       "         0.2491, -1.4513, -0.0272, -0.2934, -0.6701,  0.0698, -0.1871,  0.0432,\n",
       "        -0.3575,  0.1171,  0.4880,  0.4998,  0.2699,  0.8656,  0.1748, -0.4348,\n",
       "        -0.1576,  0.0833,  0.3471,  0.3694, -0.0382,  0.3246,  0.1456, -0.9241,\n",
       "        -0.2306, -0.0704, -0.3151, -0.1836,  0.0733,  0.2396,  0.3403, -0.4998,\n",
       "         0.0552, -0.1736,  0.1818, -0.3761,  0.2539, -0.2259,  0.0817,  0.0816,\n",
       "        -0.2275, -0.1332, -0.7522, -0.0848, -0.2559,  0.7770, -0.2029, -0.2159,\n",
       "        -0.2218, -0.3668, -0.0278,  0.5638,  0.7531, -0.6381,  0.1195, -0.4188,\n",
       "         0.0726, -0.5831,  0.2259,  0.4506,  0.7378, -0.4863,  0.4677,  0.4219,\n",
       "         0.3403, -0.1670, -0.2920,  0.1691,  0.5248, -0.1145,  0.1712,  0.3144,\n",
       "        -0.0554, -0.3451, -0.6779,  0.1259,  0.2232,  0.0229,  0.1137, -0.3753,\n",
       "        -0.0912,  0.4810,  0.5416, -0.5575, -0.2345,  0.2800,  0.2240, -0.1485,\n",
       "        -0.1576, -0.1888, -0.0284,  0.0538,  1.1169, -0.3025,  0.2678, -0.0966,\n",
       "         0.2324,  0.0516, -0.0399,  0.0104, -0.4377,  0.4408,  0.1138, -0.0909,\n",
       "         0.4458, -0.0149, -0.3249,  0.5883, -0.2882, -0.5268, -0.2276,  0.2081,\n",
       "         0.5026, -0.2490,  0.9149, -0.3857,  0.4516, -0.4330, -0.0678, -0.6485,\n",
       "        -0.1270, -0.0029, -0.7178,  0.0870, -0.2185, -0.2817, -0.3384,  0.1101,\n",
       "        -0.1833, -0.0994, -0.0587,  0.5411,  0.7301, -0.4389, -0.2971, -0.1298,\n",
       "         0.3462,  0.0211,  0.0875,  0.1614,  0.3451,  0.3280, -0.5718,  0.0171,\n",
       "         0.4960,  0.9344, -0.2416,  0.0296,  0.2732, -0.0344, -0.2691, -0.1247,\n",
       "         0.0978, -0.3535, -0.3696,  0.0803,  0.7278,  0.3016,  0.1179, -0.1525,\n",
       "         0.2821,  0.7534, -0.2328,  0.6830, -0.1915,  0.0181, -0.1945, -0.4377,\n",
       "        -0.2294,  0.1298,  0.1809,  0.1672, -0.5337,  0.0190,  1.1173, -0.0276,\n",
       "        -0.3667, -0.6857, -1.0948, -0.1862,  0.5569, -0.2998,  0.0334,  0.1541,\n",
       "        -0.4473, -0.7804,  0.2187,  0.0218,  0.4843, -0.4453,  0.0132, -0.0277,\n",
       "        -0.7685, -0.3735,  0.0163, -0.5095])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):    \n",
    "    words = nlp(text)\n",
    "    #sentence = [token.text_with_ws for token in words]\n",
    "    sentence  = \"\".join([token.text_with_ws for token in words]).strip()\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "def process_training_data():\n",
    "    negative_reviews = io.open('./Train.neg',encoding='latin-1').readlines()\n",
    "    positive_reviews = io.open('./Train.pos',encoding='latin-1').readlines()\n",
    "    with jsonlines.open('train.jsonl',mode='w') as writer:\n",
    "\n",
    "        for review in positive_reviews:\n",
    "            processed_text = preprocess_text(review)\n",
    "            d = {'text': processed_text , 'sentiment': 1}\n",
    "            writer.write(d)\n",
    "        for review in negative_reviews:\n",
    "            processed_text = preprocess_text(review)\n",
    "            d = {'text': processed_text , 'sentiment': 0}\n",
    "            writer.write(d)\n",
    "process_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "class ReviewDataSet(Dataset):\n",
    "\n",
    "    def __init__(self,file):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.file = file\n",
    "        self.data = []\n",
    "        with open(self.file) as f:\n",
    "            for line in f:\n",
    "                sample = json.loads(line)\n",
    "                self.data.append([sample['text'],sample['sentiment']])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    \n",
    "\n",
    "dataset = ReviewDataSet('train.jsonl') \n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "print(train_size)\n",
    "print(test_size)\n",
    "\n",
    "train_dataset,validation_dataset = random_split(dataset,[train_size,test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "val_dataloader = DataLoader(validation_dataset,batch_size=32,shuffle=True)\n",
    "glove = GloVe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordEmbeddings(batch_data,glove):\n",
    "    \n",
    "    \n",
    "    if(len(batch_data)==2):\n",
    "        reviews = batch_data[0]\n",
    "        sentiment = batch_data[1]\n",
    "    else:\n",
    "        reviews = batch_data[0]\n",
    "        sentiment = None\n",
    "    \n",
    "    reviews_tensor = []\n",
    "    lengths = []\n",
    "    for review in reviews:\n",
    "        words = review.split()\n",
    "        words_tensor = []\n",
    "        lengths.append(len(words))\n",
    "        for word in words:\n",
    "            #words_tensor.append(glove.__getitem__(word))\n",
    "            words_tensor.append(torch.tensor(glove.get_word_vector(word)))\n",
    "        reviews_tensor.append(torch.stack(words_tensor,dim=0))\n",
    "    \n",
    "    mask = torch.zeros((len(lengths),max(lengths)))\n",
    "    for i in range(len(lengths)):\n",
    "        mask[i,:lengths[i]] = 1.0\n",
    "    \n",
    "    return (pad_sequence(reviews_tensor,batch_first=True),mask,sentiment)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from gensim import models\\n\\nw = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin',binary=True)\\nw.get_vector('rat')\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from gensim import models\n",
    "\n",
    "w = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin',binary=True)\n",
    "w.get_vector('rat')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim=EMBED_DIM,hidden_dim = HIDDEN_LAYER_DIM):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(self.embed_dim,self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim,1)\n",
    "        self.sigmoid  = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,inp,inp_mask):\n",
    "        \n",
    "        inp_lengths = torch.sum(inp_mask,-1,keepdim=True)\n",
    "        total = torch.sum(inp*(inp_mask.unsqueeze(2)),axis=1)\n",
    "        vector_average = total / inp_lengths\n",
    "        ans = F.relu(self.fc1(vector_average))\n",
    "        ans = self.sigmoid(self.fc2(ans))\n",
    "        return ans\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inp = torch.randn((2,3,5))\\nmask = torch.tensor([[1.0,0.0,0.0],[1.0,0.0,1.0]])\\n\\nprint(inp)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''inp = torch.randn((2,3,5))\n",
    "mask = torch.tensor([[1.0,0.0,0.0],[1.0,0.0,1.0]])\n",
    "\n",
    "print(inp)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.580641510605812   tensor(0.7480)\n",
      "0.4952510052919388   tensor(0.7520)\n",
      "0.4796864761114121   tensor(0.7635)\n",
      "0.46994533884525297   tensor(0.7660)\n",
      "0.46831831336021423   tensor(0.7780)\n",
      "0.46359367173910143   tensor(0.7810)\n",
      "0.46076147639751436   tensor(0.7820)\n",
      "0.46139667761325837   tensor(0.7500)\n",
      "0.4522773082256317   tensor(0.7795)\n",
      "0.4508477953672409   tensor(0.7675)\n",
      "0.4544805326461792   tensor(0.7520)\n",
      "0.44381807452440264   tensor(0.7775)\n",
      "0.44355688804388044   tensor(0.7730)\n",
      "0.43802326601743696   tensor(0.7765)\n",
      "0.43508167409896853   tensor(0.7800)\n",
      "0.43086389738321307   tensor(0.7755)\n",
      "0.4289894720315933   tensor(0.7760)\n",
      "0.4263450213670731   tensor(0.7660)\n",
      "0.42774930596351624   tensor(0.7765)\n",
      "0.42039433801174164   tensor(0.7740)\n",
      "0.4186685197353363   tensor(0.7790)\n",
      "0.41415614742040635   tensor(0.7755)\n",
      "0.41782544887065887   tensor(0.7605)\n",
      "0.4121598373353481   tensor(0.7755)\n",
      "0.4091121007800102   tensor(0.7670)\n",
      "0.40661845916509626   tensor(0.7640)\n",
      "0.4075793016552925   tensor(0.7785)\n",
      "0.3951620810031891   tensor(0.7745)\n",
      "0.39343780821561813   tensor(0.7820)\n",
      "0.3920053432583809   tensor(0.7520)\n",
      "0.3937349447607994   tensor(0.7745)\n",
      "0.38558769911527635   tensor(0.7825)\n",
      "0.38259357696771623   tensor(0.7765)\n",
      "0.37855176931619644   tensor(0.7470)\n",
      "0.3755425108075142   tensor(0.7650)\n",
      "0.367810556769371   tensor(0.7765)\n",
      "0.3672333595752716   tensor(0.7720)\n",
      "0.3653207640647888   tensor(0.7705)\n",
      "0.3534228476881981   tensor(0.7740)\n",
      "0.35450366258621213   tensor(0.7560)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "dan = DAN(EMBED_DIM,HIDDEN_LAYER_DIM)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(dan.parameters(),lr=0.01)\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    training_loss = 0.0\n",
    "    size = 0\n",
    "    dan.train()\n",
    "\n",
    "    for i,data in enumerate(train_dataloader,0):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_reviews , input_mask , output_labels = getWordEmbeddings(data,model)\n",
    "\n",
    "        output = dan(input_reviews,input_mask).squeeze()\n",
    "        \n",
    "     \n",
    "\n",
    "        loss = criterion(output,output_labels.float())\n",
    "       \n",
    "        \n",
    "        \n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        size = max(size,i+1)\n",
    "\n",
    "    dan.eval()\n",
    "    validation_loss = 0\n",
    "\n",
    "    val_size = 0\n",
    "    for i,data in enumerate(val_dataloader,0):\n",
    "        input_reviews,input_mask,output_labels = getWordEmbeddings(data,model)\n",
    "        output = dan(input_reviews,input_mask).squeeze()\n",
    "        nearest_class = torch.round(output)\n",
    "        correct = (nearest_class == output_labels.float()).float()\n",
    "        validation_loss += correct.sum()\n",
    "  \n",
    "    print(str(training_loss/size )+ \"   \" + str(validation_loss/len(validation_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000],\n",
      "        [1.0000],\n",
      "        [1.0000]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def test(filename):\n",
    "    reviews = open(filename,'r').readlines()\n",
    "    for i in range(len(reviews)):\n",
    "        r = reviews[i]\n",
    "        reviews[i] = preprocess_text(r)\n",
    "    \n",
    "    reviews,reviews_mask,labels = getWordEmbeddings([reviews],glove)\n",
    "    dan.eval()\n",
    "\n",
    "    output = dan(reviews,reviews_mask)\n",
    "    print(output)\n",
    "\n",
    "\n",
    "test('test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1b0110cf1cb03549be737c5657a86ea4daeeb81469a7991ed915d907f3e629c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
