{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchtext.vocab import GloVe\n",
    "import spacy\n",
    "import io\n",
    "import jsonlines\n",
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import random_split\n",
    "import fasttext\n",
    "import re\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "from gensim import models\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "\n",
    "\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "\n",
    "use_word2vec = True\n",
    "use_glove = True\n",
    "use_fasttext = True\n",
    "use_word_dropout = False\n",
    "\n",
    "word_vector_types_count = 0\n",
    "if use_word2vec:\n",
    "    word_vector_types_count += 1\n",
    "if use_glove:\n",
    "    word_vector_types_count += 1\n",
    "if use_fasttext:\n",
    "    word_vector_types_count += 1\n",
    "\n",
    "\n",
    "if torch.has_mps:\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_LAYER_DIM = 128\n",
    "NUM_EPOCHS = 90\n",
    "BATCH_SIZE = 16\n",
    "EXPAND_CONTRACTIONS = True\n",
    "PATIENCE_PARAMETER = 3\n",
    "VALIDATION_LOSS_STEP = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "glove = None\n",
    "fasttext_model = None\n",
    "word2vec_model = None\n",
    "\n",
    "if use_glove:\n",
    "    glove = GloVe()\n",
    "if use_fasttext:\n",
    "    fasttext_model = fasttext.load_model('./crawl-300d-2M-subword/crawl-300d-2M-subword.bin')\n",
    "if use_word2vec:\n",
    "    word2vec_model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions_text(text):    \n",
    "    flags = re.IGNORECASE | re.MULTILINE\n",
    "    text = re.sub(r'`', \"'\", text, flags = flags)\n",
    "    ## starts / ends with '\n",
    "    text = re.sub(\n",
    "        r\"(\\s|^)'(aight|cause)(\\s|$)\",\n",
    "        '\\g<1>\\g<2>\\g<3>',\n",
    "        text, flags = flags\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"(\\s|^)'t(was|is)(\\s|$)\", r'\\g<1>it \\g<2>\\g<3>',\n",
    "        text,\n",
    "        flags = flags\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"(\\s|^)ol'(\\s|$)\",\n",
    "        '\\g<1>old\\g<2>',\n",
    "        text, flags = flags\n",
    "    )\n",
    "        \n",
    "    text = re.sub(r\"\\b(aight)\\b\", 'alright', text, flags = flags)\n",
    "    text = re.sub(r'\\bcause\\b', 'because', text, flags = flags)\n",
    "    text = re.sub(r'\\b(finna|gonna)\\b', 'going to', text, flags = flags)\n",
    "    text = re.sub(r'\\bgimme\\b', 'give me', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgive'n\\b\", 'given', text, flags = flags)\n",
    "    text = re.sub(r\"\\bhowdy\\b\", 'how do you do', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgotta\\b\", 'got to', text, flags = flags)\n",
    "    text = re.sub(r\"\\binnit\\b\", 'is it not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(can)(not)\\b\", r'\\g<1> \\g<2>', text, flags = flags)\n",
    "    text = re.sub(r\"\\bwanna\\b\", 'want to', text, flags = flags)\n",
    "    text = re.sub(r\"\\bmethinks\\b\", 'me thinks', text, flags = flags)\n",
    "    text = re.sub(r\"\\bo'er\\b\", r'over', text, flags = flags)\n",
    "    text = re.sub(r\"\\bne'er\\b\", r'never', text, flags = flags)\n",
    "    text = re.sub(r\"\\bo'?clock\\b\", 'of the clock', text, flags = flags)\n",
    "    text = re.sub(r\"\\bma'am\\b\", 'madam', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgiv'n\\b\", 'given', text, flags = flags)\n",
    "    text = re.sub(r\"\\be'er\\b\", 'ever', text, flags = flags)\n",
    "    text = re.sub(r\"\\bd'ye\\b\", 'do you', text, flags = flags)\n",
    "    text = re.sub(r\"\\be'er\\b\", 'ever', text, flags = flags)\n",
    "    text = re.sub(r\"\\bd'ye\\b\", 'do you', text, flags = flags)\n",
    "    text = re.sub(r\"\\bg'?day\\b\", 'good day', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(ain|amn)'?t\\b\", 'am not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(are|can)'?t\\b\", r'\\g<1> not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(let)'?s\\b\", r'\\g<1> us', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all'dn't've'd\\b\", 'you all would not have had', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all're\\b\", 'you all are', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all'd've\\b\", 'you all would have', text, flags = flags)\n",
    "    text = re.sub(r\"(\\s)y'all(\\s)\", r'\\g<1>you all\\g<2>', text, flags = flags)  \n",
    "    text = re.sub(r\"\\b(won)'?t\\b\", 'will not', text, flags = flags)\n",
    "    text = re.sub(r\"\\bhe'd\\b\", 'he had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I|we|who)'?d'?ve\\b\", r'\\g<1> would have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(could|would|must|should|would)n'?t'?ve\\b\", r'\\g<1> not have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(he)'?dn'?t'?ve'?d\\b\", r'\\g<1> would not have had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(daren|daresn|dasn)'?t\", 'dare not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(he|how|i|it|she|that|there|these|they|we|what|where|which|who|you)'?ll\\b\", r'\\g<1> will', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(everybody|everyone|he|how|it|she|somebody|someone|something|that|there|this|what|when|where|which|who|why)'?s\\b\", r'\\g<1> is', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m'a\\b\", r'\\g<1> am about to', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m'o\\b\", r'\\g<1> am going to', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m\\b\", r'\\g<1> am', text, flags = flags)\n",
    "    text = re.sub(r\"\\bshan't\\b\", 'shall not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(are|could|did|does|do|go|had|has|have|is|may|might|must|need|ought|shall|should|was|were|would)n'?t\\b\", r'\\g<1> not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(could|had|he|i|may|might|must|should|these|they|those|to|we|what|where|which|who|would|you)'?ve\\b\", r'\\g<1> have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(how|so|that|there|these|they|those|we|what|where|which|who|why|you)'?re\\b\", r'\\g<1> are', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I|it|she|that|there|they|we|which|you)'?d\\b\", r'\\g<1> had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(how|what|where|who|why)'?d\\b\", r'\\g<1> did', text, flags = flags)    \n",
    "    return text\n",
    "\n",
    "\n",
    "class ExpandContractionsClass:\n",
    "    def __init__(self, nlp: Language):\n",
    "        self.nlp = nlp\n",
    "    \n",
    "    def __call__(self,doc: Doc):\n",
    "        text = doc.text\n",
    "        return self.nlp.make_doc(expand_contractions_text(text))\n",
    "    \n",
    "@Language.factory(\"expand_contractions_component\")\n",
    "def create_expand_contractions_component(nlp : Language, name: str):\n",
    "    return ExpandContractionsClass(nlp)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "if EXPAND_CONTRACTIONS:\n",
    "    nlp.add_pipe(\"expand_contractions_component\",before='tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_text(text):    \n",
    "    words = nlp(text)\n",
    "    #sentence = [token.text_with_ws for token in words if not token.is_punct]\n",
    "    sentence  = \" \".join([token.text for token in words if not token.is_punct ]).strip()\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "def process_training_data():\n",
    "    negative_reviews = io.open('./Train.neg',encoding='latin-1').readlines()\n",
    "    positive_reviews = io.open('./Train.pos',encoding='latin-1').readlines()\n",
    "    with jsonlines.open('train.jsonl',mode='w') as writer:\n",
    "\n",
    "        for review in positive_reviews:\n",
    "            processed_text = preprocess_text(review)\n",
    "            d = {'text': processed_text , 'sentiment': 1}\n",
    "            writer.write(d)\n",
    "        for review in negative_reviews:\n",
    "            processed_text = preprocess_text(review)\n",
    "            d = {'text': processed_text , 'sentiment': 0}\n",
    "            writer.write(d)\n",
    "process_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "class ReviewDataSet(Dataset):\n",
    "\n",
    "    def __init__(self,file):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.file = file\n",
    "        self.data = []\n",
    "        with open(self.file) as f:\n",
    "            for line in f:\n",
    "                sample = json.loads(line)\n",
    "                self.data.append([sample['text'],sample['sentiment']])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    \n",
    "\n",
    "dataset = ReviewDataSet('train.jsonl') \n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset,validation_dataset = random_split(dataset,[train_size,test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "val_dataloader = DataLoader(validation_dataset,batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordEmbeddings(batch_data,dicts):\n",
    "    \n",
    "    word2vec,glove,fast_text = dicts\n",
    "\n",
    "    if(len(batch_data)==2):\n",
    "        reviews = batch_data[0]\n",
    "        sentiment = batch_data[1]\n",
    "    else:\n",
    "        reviews = batch_data[0]\n",
    "        sentiment = None\n",
    "    \n",
    "    reviews_tensor = []\n",
    "    lengths = []\n",
    "    for review in reviews:\n",
    "        \n",
    "        preprocessed_review_list = []\n",
    "        i = 0\n",
    "        review_doc = nlp(review)\n",
    "        for token in review_doc:\n",
    "            if token.text == '\\'s':\n",
    "                if i>0 and review_doc[i-1].pos_!='PROPN':\n",
    "                    preprocessed_review_list[i-1] = preprocessed_review_list[i-1] + '\\'s'\n",
    "            elif token.text == '\\'nt':\n",
    "                if i>0:\n",
    "                    preprocessed_review_list[i-1] = preprocessed_review_list[i-1] + '\\'nt'\n",
    "            else:\n",
    "                preprocessed_review_list.append(token.text) \n",
    "                i+=1\n",
    "\n",
    "        new_review = \" \".join(preprocessed_review_list)\n",
    "        \n",
    "        words = new_review.split()\n",
    "        words_tensor = []\n",
    "        lengths.append(len(words))\n",
    "        for word in words:\n",
    "            #words_tensor.append(glove.__getitem__(word))\n",
    "            word_embeddings = []\n",
    "            if use_word2vec:\n",
    "                try:\n",
    "                    word_embeddings.append(torch.tensor(word2vec.get_vector(word)))\n",
    "                except:\n",
    "                    word_embeddings.append(torch.zeros(EMBED_DIM,))\n",
    "            if use_glove:\n",
    "                word_embeddings.append(glove.__getitem__(word))\n",
    "            if use_fasttext:\n",
    "                word_embeddings.append(torch.tensor(fast_text.get_word_vector(word)))\n",
    "            word_tensor = torch.stack(word_embeddings)\n",
    "            word_tensor = word_tensor.squeeze()\n",
    "            words_tensor.append(word_tensor)            \n",
    "        reviews_tensor.append(torch.stack(words_tensor,dim=0))\n",
    "    \n",
    "    mask = torch.zeros((len(lengths),max(lengths)))\n",
    "    for i in range(len(lengths)):\n",
    "        mask[i,:lengths[i]] = 1.0\n",
    "    \n",
    "    return (pad_sequence(reviews_tensor,batch_first=True),mask,sentiment)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim=EMBED_DIM,hidden_dim = HIDDEN_LAYER_DIM, droput_prob = 0.3):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(self.embed_dim,self.hidden_dim)\n",
    "        #self.fc = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim,1)\n",
    "        self.sigmoid  = nn.Sigmoid()\n",
    "        self.word_dropout_prob = droput_prob\n",
    "\n",
    "        if(word_vector_types_count > 1) :\n",
    "           self.importance_weights = nn.Linear(word_vector_types_count*EMBED_DIM,word_vector_types_count)\n",
    "        \n",
    "\n",
    "    def forward(self,inp,inp_mask):\n",
    "        \n",
    "        \n",
    "        inp_lengths = torch.sum(inp_mask,-1,keepdim=True).int()\n",
    "        if use_word_dropout:\n",
    "            for i in range(inp_lengths.shape[0]):\n",
    "                dist = Bernoulli(self.word_dropout_prob).sample((int(inp_lengths[i]),))\n",
    "                while torch.count_nonzero(dist) == 0:\n",
    "                    dist = Bernoulli(self.word_dropout_prob).sample((int(inp_lengths[i]),))\n",
    "                inp_mask[i,:int(inp_lengths[i])]  = dist\n",
    "                inp_lengths[i,0] = int(torch.sum(inp_mask[i]))\n",
    "                #print(inp_mask)\n",
    "                #inp_lengths = torch.sum(inp_mask,-1,keepdim=True)\n",
    "        if word_vector_types_count > 1:\n",
    "            input = inp.view((inp.shape[0],inp.shape[1],-1))\n",
    "            out = self.importance_weights(input)\n",
    "            weights = F.softmax(out,dim=2).unsqueeze(3)\n",
    "            weights_multiplied_vector = weights * inp\n",
    "            inp = torch.sum(weights_multiplied_vector,dim=2)\n",
    "\n",
    "        total = torch.sum(inp*(inp_mask.unsqueeze(2)),axis=1)\n",
    "        vector_average = total / inp_lengths\n",
    "        ans = F.relu(self.fc1(vector_average))\n",
    "        #ans = F.relu(self.fc(ans))\n",
    "        ans = self.sigmoid(self.fc2(ans))\n",
    "        return ans\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inp = torch.randn((2,3,5))\\nmask = torch.tensor([[1.0,0.0,0.0],[1.0,0.0,1.0]])\\n\\nprint(inp)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''inp = torch.randn((2,3,5))\n",
    "mask = torch.tensor([[1.0,0.0,0.0],[1.0,0.0,1.0]])\n",
    "\n",
    "print(inp)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss = 0.029047386437654494 Validation Set Accuracy = 0.7845\n",
      "Epoch 0; Training Loss (averaged) = 0.0314750400390476\n",
      "Epoch 1; Training Loss (averaged) = 0.029074758902192115\n",
      "Epoch 2; Training Loss (averaged) = 0.028462086379528045\n",
      "Epoch 3; Training Loss (averaged) = 0.02802375228330493\n",
      "Epoch 4; Training Loss (averaged) = 0.027705910433083774\n",
      "Validation Loss = 0.027612757965922354 Validation Set Accuracy = 0.792\n",
      "Epoch 5; Training Loss (averaged) = 0.027421869499608873\n",
      "Epoch 6; Training Loss (averaged) = 0.027177873196080327\n",
      "Epoch 7; Training Loss (averaged) = 0.026956752879545093\n",
      "Epoch 8; Training Loss (averaged) = 0.026745970483869314\n",
      "Epoch 9; Training Loss (averaged) = 0.026572395687922835\n",
      "Validation Loss = 0.027196997702121736 Validation Set Accuracy = 0.7975\n",
      "Epoch 10; Training Loss (averaged) = 0.026394562305882573\n",
      "Epoch 11; Training Loss (averaged) = 0.026256768800318242\n",
      "Epoch 12; Training Loss (averaged) = 0.026087416104972363\n",
      "Epoch 13; Training Loss (averaged) = 0.02594664218276739\n",
      "Epoch 14; Training Loss (averaged) = 0.02581951099075377\n",
      "Validation Loss = 0.02700753567367792 Validation Set Accuracy = 0.794\n",
      "Epoch 15; Training Loss (averaged) = 0.025675143780186774\n",
      "Epoch 16; Training Loss (averaged) = 0.02555803606007248\n",
      "Epoch 17; Training Loss (averaged) = 0.025429834733717144\n",
      "Epoch 18; Training Loss (averaged) = 0.02532920128107071\n",
      "Epoch 19; Training Loss (averaged) = 0.02520849796757102\n",
      "Validation Loss = 0.026877163641154767 Validation Set Accuracy = 0.8005\n",
      "Epoch 20; Training Loss (averaged) = 0.02511956065520644\n",
      "Epoch 21; Training Loss (averaged) = 0.02498895507492125\n",
      "Epoch 22; Training Loss (averaged) = 0.02490316893346608\n",
      "Epoch 23; Training Loss (averaged) = 0.024805723955854772\n",
      "Epoch 24; Training Loss (averaged) = 0.024731186930090188\n",
      "Validation Loss = 0.026798722207546234 Validation Set Accuracy = 0.8005\n",
      "Epoch 25; Training Loss (averaged) = 0.024594529146328568\n",
      "Epoch 26; Training Loss (averaged) = 0.02453623971529305\n",
      "Epoch 27; Training Loss (averaged) = 0.02444765131548047\n",
      "Epoch 28; Training Loss (averaged) = 0.02436224244721234\n",
      "Epoch 29; Training Loss (averaged) = 0.02428007492236793\n",
      "Validation Loss = 0.02690238116681576 Validation Set Accuracy = 0.793\n",
      "Epoch 30; Training Loss (averaged) = 0.024208975144661964\n",
      "Epoch 31; Training Loss (averaged) = 0.02415048420988023\n",
      "Epoch 32; Training Loss (averaged) = 0.02406288994383067\n",
      "Epoch 33; Training Loss (averaged) = 0.023970692877657712\n",
      "Epoch 34; Training Loss (averaged) = 0.023915275081060828\n",
      "Validation Loss = 0.02670489226281643 Validation Set Accuracy = 0.802\n",
      "Epoch 35; Training Loss (averaged) = 0.023845025513321162\n",
      "Epoch 36; Training Loss (averaged) = 0.02376367319189012\n",
      "Epoch 37; Training Loss (averaged) = 0.0236984338182956\n",
      "Epoch 38; Training Loss (averaged) = 0.02363842614740133\n",
      "Epoch 39; Training Loss (averaged) = 0.02356963272485882\n",
      "Validation Loss = 0.02664776713401079 Validation Set Accuracy = 0.803\n",
      "Epoch 40; Training Loss (averaged) = 0.0234977201661095\n",
      "Epoch 41; Training Loss (averaged) = 0.02341595809813589\n",
      "Epoch 42; Training Loss (averaged) = 0.023378520662896335\n",
      "Epoch 43; Training Loss (averaged) = 0.02329448099154979\n",
      "Epoch 44; Training Loss (averaged) = 0.023259224966168404\n",
      "Validation Loss = 0.026647434420883655 Validation Set Accuracy = 0.8045\n",
      "Epoch 45; Training Loss (averaged) = 0.023195139897987247\n",
      "Epoch 46; Training Loss (averaged) = 0.023124703004024923\n",
      "Epoch 47; Training Loss (averaged) = 0.02306658823788166\n",
      "Epoch 48; Training Loss (averaged) = 0.023019612985663115\n",
      "Epoch 49; Training Loss (averaged) = 0.022934265980497002\n",
      "Validation Loss = 0.026690651029348374 Validation Set Accuracy = 0.7995\n",
      "Epoch 50; Training Loss (averaged) = 0.022904804217629134\n",
      "Epoch 51; Training Loss (averaged) = 0.022835316145792605\n",
      "Epoch 52; Training Loss (averaged) = 0.02278992365114391\n",
      "Epoch 53; Training Loss (averaged) = 0.02273411050066352\n",
      "Epoch 54; Training Loss (averaged) = 0.02267746895365417\n",
      "Validation Loss = 0.026640580795705318 Validation Set Accuracy = 0.8035\n",
      "Epoch 55; Training Loss (averaged) = 0.02261129775084555\n",
      "Epoch 56; Training Loss (averaged) = 0.0225697817876935\n",
      "Epoch 57; Training Loss (averaged) = 0.022514213992282747\n",
      "Epoch 58; Training Loss (averaged) = 0.022463506983593107\n",
      "Epoch 59; Training Loss (averaged) = 0.02239280054345727\n",
      "Validation Loss = 0.026692010045051576 Validation Set Accuracy = 0.8025\n",
      "Epoch 60; Training Loss (averaged) = 0.022361264856532215\n",
      "Epoch 61; Training Loss (averaged) = 0.02231523306760937\n",
      "Epoch 62; Training Loss (averaged) = 0.022269894066266716\n",
      "Epoch 63; Training Loss (averaged) = 0.022220889301970602\n",
      "Epoch 64; Training Loss (averaged) = 0.022165252110920845\n",
      "Validation Loss = 0.02669615934416652 Validation Set Accuracy = 0.805\n",
      "Epoch 65; Training Loss (averaged) = 0.022117051327601077\n",
      "Epoch 66; Training Loss (averaged) = 0.022073064208962025\n",
      "Epoch 67; Training Loss (averaged) = 0.022033598998561502\n",
      "Epoch 68; Training Loss (averaged) = 0.021979125002399087\n",
      "Epoch 69; Training Loss (averaged) = 0.02193269583489746\n",
      "Validation Loss = 0.026730890169739722 Validation Set Accuracy = 0.8035\n",
      "Epoch 70; Training Loss (averaged) = 0.02188993367459625\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "dan = DAN(EMBED_DIM,HIDDEN_LAYER_DIM)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adagrad(dan.parameters(),lr=0.01)\n",
    "#optimizer = optim.Adam(dan.parameters(),lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "validation_losses = []\n",
    "validation_accuracy = []\n",
    "training_losses = []\n",
    "\n",
    "best_validation_loss = 1000.0\n",
    "validation_loss_not_decreased_steps = 0\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    if validation_loss_not_decreased_steps >= PATIENCE_PARAMETER:\n",
    "        break\n",
    "\n",
    "    training_loss = 0.0\n",
    "    size = 0\n",
    "    dan.train()\n",
    "\n",
    "    for i,data in enumerate(train_dataloader,0):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_reviews , input_mask , output_labels = getWordEmbeddings(data,(word2vec_model,glove,fasttext_model))\n",
    "        #print(input_mask)\n",
    "        output = dan(input_reviews,input_mask).squeeze()\n",
    "        #print(output)\n",
    "        loss = criterion(output,output_labels.float())\n",
    "\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "     \n",
    "    \n",
    "    if e%VALIDATION_LOSS_STEP==0:\n",
    "        dan.eval()\n",
    "        correct_count = 0\n",
    "        validation_loss = 0\n",
    "\n",
    "        val_size = 0\n",
    "        for i,data in enumerate(val_dataloader,0):\n",
    "            input_reviews,input_mask,output_labels = getWordEmbeddings(data,(word2vec_model,glove,fasttext_model))\n",
    "            output = dan(input_reviews,input_mask).squeeze()\n",
    "            loss = criterion(output,output_labels.float())\n",
    "            validation_loss += loss.item()\n",
    "            nearest_class = torch.round(output)\n",
    "            correct = (nearest_class == output_labels.float()).float()\n",
    "            \n",
    "            correct_count += correct.sum()\n",
    "        correct_count = int(correct_count)\n",
    "        \n",
    "        current_validation_loss = validation_loss/len(validation_dataset)\n",
    "\n",
    "        if current_validation_loss <= best_validation_loss:\n",
    "            validation_loss_not_decreased_steps = 0\n",
    "            torch.save(dan.state_dict(),'best_model.pth')\n",
    "            best_validation_loss = current_validation_loss\n",
    "\n",
    "\n",
    "        else:\n",
    "            validation_loss_not_decreased_steps+=1\n",
    "\n",
    "        validation_losses.append(validation_loss/len(validation_dataset))\n",
    "        validation_accuracy.append(100*(correct_count/len(validation_dataset)))\n",
    "\n",
    "\n",
    "        print(\"Validation Loss = \" + str(validation_loss/len(validation_dataset)) + \" Validation Set Accuracy = \" + str(100*(correct_count/len(validation_dataset))))\n",
    "    training_losses.append(training_loss/len(train_dataset))\n",
    "    print(\"Epoch \" + str(e) + \"; Training Loss (averaged) = \" + str(training_loss/len(train_dataset) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.linspace(1,len(training_losses),len(training_losses))\n",
    "plt.plot(epochs,training_losses) \n",
    "epochs = 5*np.linspace(1,len(validation_accuracy),len(validation_accuracy))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (Averaged)')\n",
    "plt.plot(epochs,validation_losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5*np.linspace(1,len(validation_accuracy),len(validation_accuracy))\n",
    "plt.plot(epochs,validation_accuracy)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.9829e-07],\n",
      "        [9.9987e-01],\n",
      "        [9.7376e-01],\n",
      "        [5.3653e-01]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dan_model = DAN()\n",
    "dan_model.load_state_dict(torch.load('best_model.pth'))\n",
    "dan_model.eval()\n",
    "\n",
    "def test(filename):\n",
    "    reviews = open(filename,'r').readlines()\n",
    "    for i in range(len(reviews)):\n",
    "        r = reviews[i]\n",
    "        reviews[i] = preprocess_text(r)\n",
    "    \n",
    "    reviews,reviews_mask,labels = getWordEmbeddings([reviews],(word2vec_model,glove,fasttext_model))\n",
    "\n",
    "\n",
    "    output = dan_model(reviews,reviews_mask)\n",
    "    print(output)\n",
    "\n",
    "\n",
    "test('test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_data():\n",
    "  \n",
    "    negative_reviews = io.open('./Test.neg',encoding='latin-1').readlines()\n",
    "    positive_reviews = io.open('./Test.pos',encoding='latin-1').readlines()\n",
    "    with jsonlines.open('test.jsonl',mode='w') as writer:\n",
    "\n",
    "        for review in positive_reviews:\n",
    "            processed_text = preprocess_text(review)\n",
    "            d = {'text': processed_text , 'sentiment': 1}\n",
    "            writer.write(d)\n",
    "        for review in negative_reviews:\n",
    "            processed_text = preprocess_text(review)\n",
    "            d = {'text': processed_text , 'sentiment': 0}\n",
    "            writer.write(d)\n",
    "process_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ReviewDataSet('test.jsonl')\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dan_model = DAN()\n",
    "dan_model.load_state_dict(torch.load('best_model.pth'))\n",
    "dan_model.eval()\n",
    "correct_count = 0\n",
    "for i,data in enumerate(test_dataloader,0):\n",
    "            input_reviews,input_mask,output_labels = getWordEmbeddings(data,(word2vec_model,glove,fasttext_model))\n",
    "            output = dan_model(input_reviews,input_mask).squeeze()\n",
    "            nearest_class = torch.round(output)\n",
    "            correct = (nearest_class == output_labels.float()).float()\n",
    "            \n",
    "            correct_count += correct.sum()\n",
    "\n",
    "print(\"Test Set Accuracy = \"  + str(((1.0*correct_count)/len(test_dataset))*100))     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1b0110cf1cb03549be737c5657a86ea4daeeb81469a7991ed915d907f3e629c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
