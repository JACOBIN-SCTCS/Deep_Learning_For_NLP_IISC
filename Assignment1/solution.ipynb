{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchtext.vocab import GloVe,FastText\n",
    "import spacy\n",
    "import io\n",
    "import jsonlines\n",
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import random_split\n",
    "import fasttext\n",
    "import re\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "if torch.has_mps:\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_LAYER_DIM = 100\n",
    "NUM_EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model('./crawl-300d-2M-subword/crawl-300d-2M-subword.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_word_vector('21st')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FastText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.__getitem__('21st')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GloVe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.__getitem__('21st')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"expandcontractions\")\n",
    "def expand_contractions(text):\n",
    "    \n",
    "    \n",
    "    flags = re.IGNORECASE | re.MULTILINE\n",
    "    text = re.sub(r'`', \"'\", text, flags = flags)\n",
    "    \n",
    "    ## starts / ends with '\n",
    "    text = re.sub(\n",
    "        r\"(\\s|^)'(aight|cause)(\\s|$)\",\n",
    "        '\\g<1>\\g<2>\\g<3>',\n",
    "        text, flags = flags\n",
    "    )\n",
    "    \n",
    "    text = re.sub(\n",
    "        r\"(\\s|^)'t(was|is)(\\s|$)\", r'\\g<1>it \\g<2>\\g<3>',\n",
    "        text,\n",
    "        flags = flags\n",
    "    )\n",
    "    \n",
    "    text = re.sub(\n",
    "        r\"(\\s|^)ol'(\\s|$)\",\n",
    "        '\\g<1>old\\g<2>',\n",
    "        text, flags = flags\n",
    "    )\n",
    "    \n",
    "    text = re.sub(r\"\\b(aight)\\b\", 'alright', text, flags = flags)\n",
    "    text = re.sub(r'\\bcause\\b', 'because', text, flags = flags)\n",
    "    text = re.sub(r'\\b(finna|gonna)\\b', 'going to', text, flags = flags)\n",
    "    text = re.sub(r'\\bgimme\\b', 'give me', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgive'n\\b\", 'given', text, flags = flags)\n",
    "    text = re.sub(r\"\\bhowdy\\b\", 'how do you do', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgotta\\b\", 'got to', text, flags = flags)\n",
    "    text = re.sub(r\"\\binnit\\b\", 'is it not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(can)(not)\\b\", r'\\g<1> \\g<2>', text, flags = flags)\n",
    "    text = re.sub(r\"\\bwanna\\b\", 'want to', text, flags = flags)\n",
    "    text = re.sub(r\"\\bmethinks\\b\", 'me thinks', text, flags = flags)\n",
    "    text = re.sub(r\"\\bo'er\\b\", r'over', text, flags = flags)\n",
    "    text = re.sub(r\"\\bne'er\\b\", r'never', text, flags = flags)\n",
    "    text = re.sub(r\"\\bo'?clock\\b\", 'of the clock', text, flags = flags)\n",
    "    text = re.sub(r\"\\bma'am\\b\", 'madam', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgiv'n\\b\", 'given', text, flags = flags)\n",
    "    text = re.sub(r\"\\be'er\\b\", 'ever', text, flags = flags)\n",
    "    text = re.sub(r\"\\bd'ye\\b\", 'do you', text, flags = flags)\n",
    "    text = re.sub(r\"\\be'er\\b\", 'ever', text, flags = flags)\n",
    "    text = re.sub(r\"\\bd'ye\\b\", 'do you', text, flags = flags)\n",
    "    text = re.sub(r\"\\bg'?day\\b\", 'good day', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(ain|amn)'?t\\b\", 'am not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(are|can)'?t\\b\", r'\\g<1> not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(let)'?s\\b\", r'\\g<1> us', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all'dn't've'd\\b\", 'you all would not have had', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all're\\b\", 'you all are', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all'd've\\b\", 'you all would have', text, flags = flags)\n",
    "    text = re.sub(r\"(\\s)y'all(\\s)\", r'\\g<1>you all\\g<2>', text, flags = flags)  \n",
    "    text = re.sub(r\"\\b(won)'?t\\b\", 'will not', text, flags = flags)\n",
    "    text = re.sub(r\"\\bhe'd\\b\", 'he had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I|we|who)'?d'?ve\\b\", r'\\g<1> would have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(could|would|must|should|would)n'?t'?ve\\b\", r'\\g<1> not have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(he)'?dn'?t'?ve'?d\\b\", r'\\g<1> would not have had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(daren|daresn|dasn)'?t\", 'dare not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(he|how|i|it|she|that|there|these|they|we|what|where|which|who|you)'?ll\\b\", r'\\g<1> will', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(everybody|everyone|he|how|it|she|somebody|someone|something|that|there|this|what|when|where|which|who|why)'?s\\b\", r'\\g<1> is', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m'a\\b\", r'\\g<1> am about to', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m'o\\b\", r'\\g<1> am going to', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m\\b\", r'\\g<1> am', text, flags = flags)\n",
    "    text = re.sub(r\"\\bshan't\\b\", 'shall not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(are|could|did|does|do|go|had|has|have|is|may|might|must|need|ought|shall|should|was|were|would)n'?t\\b\", r'\\g<1> not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(could|had|he|i|may|might|must|should|these|they|those|to|we|what|where|which|who|would|you)'?ve\\b\", r'\\g<1> have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(how|so|that|there|these|they|those|we|what|where|which|who|why|you)'?re\\b\", r'\\g<1> are', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I|it|she|that|there|they|we|which|you)'?d\\b\", r'\\g<1> had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(how|what|where|who|why)'?d\\b\", r'\\g<1> did', text, flags = flags)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x436614220>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x111feb760>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x4364c1f20>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x15770d980>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x15775f700>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x436600040>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_text(text):    \n",
    "    words = nlp(text)\n",
    "    #sentence = [token.text_with_ws for token in words if not token.is_punct]\n",
    "    sentence  = \"\".join([token.text_with_ws for token in words if not token.is_punct ]).strip()\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "def process_training_data():\n",
    "    negative_reviews = io.open('./Train.neg',encoding='latin-1').readlines()\n",
    "    positive_reviews = io.open('./Train.pos',encoding='latin-1').readlines()\n",
    "    with jsonlines.open('train.jsonl',mode='w') as writer:\n",
    "\n",
    "        for review in positive_reviews:\n",
    "            processed_text = preprocess_text(review)\n",
    "            d = {'text': processed_text , 'sentiment': 1}\n",
    "            writer.write(d)\n",
    "        for review in negative_reviews:\n",
    "            processed_text = preprocess_text(review)\n",
    "            d = {'text': processed_text , 'sentiment': 0}\n",
    "            writer.write(d)\n",
    "process_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "\n",
    "class ReviewDataSet(Dataset):\n",
    "\n",
    "    def __init__(self,file):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.file = file\n",
    "        self.data = []\n",
    "        with open(self.file) as f:\n",
    "            for line in f:\n",
    "                sample = json.loads(line)\n",
    "                self.data.append([sample['text'],sample['sentiment']])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    \n",
    "\n",
    "dataset = ReviewDataSet('train.jsonl') \n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "print(train_size)\n",
    "print(test_size)\n",
    "\n",
    "train_dataset,validation_dataset = random_split(dataset,[train_size,test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=32,shuffle=True)\n",
    "val_dataloader = DataLoader(validation_dataset,batch_size=32,shuffle=True)\n",
    "glove = GloVe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordEmbeddings(batch_data,glove):\n",
    "    \n",
    "    \n",
    "    if(len(batch_data)==2):\n",
    "        reviews = batch_data[0]\n",
    "        sentiment = batch_data[1]\n",
    "    else:\n",
    "        reviews = batch_data[0]\n",
    "        sentiment = None\n",
    "    \n",
    "    reviews_tensor = []\n",
    "    lengths = []\n",
    "    for review in reviews:\n",
    "        words = review.split()\n",
    "        words_tensor = []\n",
    "        lengths.append(len(words))\n",
    "        for word in words:\n",
    "            #words_tensor.append(glove.__getitem__(word))\n",
    "            words_tensor.append(torch.tensor(glove.get_word_vector(word)))\n",
    "        reviews_tensor.append(torch.stack(words_tensor,dim=0))\n",
    "    \n",
    "    mask = torch.zeros((len(lengths),max(lengths)))\n",
    "    for i in range(len(lengths)):\n",
    "        mask[i,:lengths[i]] = 1.0\n",
    "    \n",
    "    return (pad_sequence(reviews_tensor,batch_first=True),mask,sentiment)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from gensim import models\\n\\nw = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin',binary=True)\\nw.get_vector('rat')\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from gensim import models\n",
    "\n",
    "w = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin',binary=True)\n",
    "w.get_vector('rat')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim=EMBED_DIM,hidden_dim = HIDDEN_LAYER_DIM):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(self.embed_dim,self.hidden_dim)\n",
    "        self.fc = nn.Linear(self.hidden_dim,self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim,1)\n",
    "        self.sigmoid  = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,inp,inp_mask):\n",
    "        \n",
    "        inp_lengths = torch.sum(inp_mask,-1,keepdim=True)\n",
    "        total = torch.sum(inp*(inp_mask.unsqueeze(2)),axis=1)\n",
    "        vector_average = total / inp_lengths\n",
    "        ans = F.relu(self.fc1(vector_average))\n",
    "        ans = F.relu(self.fc(ans))\n",
    "        ans = self.sigmoid(self.fc2(ans))\n",
    "        return ans\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inp = torch.randn((2,3,5))\\nmask = torch.tensor([[1.0,0.0,0.0],[1.0,0.0,1.0]])\\n\\nprint(inp)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''inp = torch.randn((2,3,5))\n",
    "mask = torch.tensor([[1.0,0.0,0.0],[1.0,0.0,1.0]])\n",
    "\n",
    "print(inp)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5609703376293182   tensor(0.7395)\n",
      "0.4992580144405365   tensor(0.7590)\n",
      "0.47869559985399246   tensor(0.7540)\n",
      "0.46793403470516204   tensor(0.7595)\n",
      "0.46379221087694167   tensor(0.7725)\n",
      "0.4575304556488991   tensor(0.6840)\n",
      "0.45060817152261734   tensor(0.7760)\n",
      "0.4531295042037964   tensor(0.7455)\n",
      "0.4385815027952194   tensor(0.7705)\n",
      "0.43537878841161726   tensor(0.7735)\n",
      "0.431089332818985   tensor(0.7395)\n",
      "0.42689629596471784   tensor(0.7650)\n",
      "0.42459876781702044   tensor(0.7745)\n",
      "0.41615037870407107   tensor(0.7715)\n",
      "0.4116773464083672   tensor(0.7755)\n",
      "0.40364035159349443   tensor(0.7755)\n",
      "0.4008920320868492   tensor(0.7740)\n",
      "0.39158280968666076   tensor(0.7570)\n",
      "0.392457572221756   tensor(0.7555)\n",
      "0.3937276052236557   tensor(0.7600)\n",
      "0.3827943657040596   tensor(0.7610)\n",
      "0.3743901337981224   tensor(0.7635)\n",
      "0.3682677587270737   tensor(0.7595)\n",
      "0.3656094669103622   tensor(0.7555)\n",
      "0.3640474045276642   tensor(0.7675)\n",
      "0.3562387942671776   tensor(0.7670)\n",
      "0.3492117664217949   tensor(0.7660)\n",
      "0.3492134701013565   tensor(0.7605)\n",
      "0.343189827978611   tensor(0.7650)\n",
      "0.33573369663953784   tensor(0.7605)\n",
      "0.34051595586538314   tensor(0.7500)\n",
      "0.3298198561668396   tensor(0.7560)\n",
      "0.324541156232357   tensor(0.7470)\n",
      "0.3141990906000137   tensor(0.7560)\n",
      "0.3169832136332989   tensor(0.7620)\n",
      "0.30745970037579534   tensor(0.7450)\n",
      "0.3185756089091301   tensor(0.7490)\n",
      "0.3016155277490616   tensor(0.7525)\n",
      "0.2941811137795448   tensor(0.7545)\n",
      "0.2933887705206871   tensor(0.7595)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "dan = DAN(EMBED_DIM,HIDDEN_LAYER_DIM)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(dan.parameters(),lr=0.01)\n",
    "\n",
    "for e in range(NUM_EPOCHS):\n",
    "    training_loss = 0.0\n",
    "    size = 0\n",
    "    dan.train()\n",
    "\n",
    "    for i,data in enumerate(train_dataloader,0):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_reviews , input_mask , output_labels = getWordEmbeddings(data,model)\n",
    "\n",
    "        output = dan(input_reviews,input_mask).squeeze()\n",
    "        \n",
    "     \n",
    "\n",
    "        loss = criterion(output,output_labels.float())\n",
    "       \n",
    "        \n",
    "        \n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        size = max(size,i+1)\n",
    "\n",
    "    dan.eval()\n",
    "    validation_loss = 0\n",
    "\n",
    "    val_size = 0\n",
    "    for i,data in enumerate(val_dataloader,0):\n",
    "        input_reviews,input_mask,output_labels = getWordEmbeddings(data,model)\n",
    "        output = dan(input_reviews,input_mask).squeeze()\n",
    "        nearest_class = torch.round(output)\n",
    "        correct = (nearest_class == output_labels.float()).float()\n",
    "        validation_loss += correct.sum()\n",
    "  \n",
    "    print(str(training_loss/size )+ \"   \" + str(validation_loss/len(validation_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(filename):\n",
    "    reviews = open(filename,'r').readlines()\n",
    "    for i in range(len(reviews)):\n",
    "        r = reviews[i]\n",
    "        reviews[i] = preprocess_text(r)\n",
    "    \n",
    "    reviews,reviews_mask,labels = getWordEmbeddings([reviews],model)\n",
    "    dan.eval()\n",
    "\n",
    "    output = dan(reviews,reviews_mask)\n",
    "    print(output)\n",
    "\n",
    "\n",
    "test('test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1b0110cf1cb03549be737c5657a86ea4daeeb81469a7991ed915d907f3e629c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
