{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import spacy\n",
    "import jsonlines\n",
    "import json\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence,pack_padded_sequence,pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import SubsetRandomSampler,DataLoader,Subset\n",
    "from torchtext.vocab import GloVe\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# SENTENCE_SPLITTING_USED; whether to use the splitting of reviews into sentences.\n",
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "ATTENTION_DIM = 256\n",
    "\n",
    "PATIENCE_PARAMETER = 7\n",
    "VALIDATION_LOSS_COMPUTE_STEP = 1\n",
    "EXPAND_CONTRACTIONS = True\n",
    "\n",
    "\n",
    "device_cpu = torch.device('cpu')\n",
    "device_fast = torch.device('cpu')\n",
    "\n",
    "USE_PRETRAINED_MODEL= False\n",
    "\n",
    "if torch.has_mps:\n",
    "    device_fast = torch.device('mps')\n",
    "elif torch.has_cuda:\n",
    "    device_fast = torch.device('cuda')\n",
    "\n",
    "#torch.manual_seed(0)\n",
    "#np.random.seed(0)\n",
    "glove = GloVe()\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions_text(text):    \n",
    "    flags = re.IGNORECASE | re.MULTILINE\n",
    "    text = re.sub(r'`', \"'\", text, flags = flags)\n",
    "    ## starts / ends with '\n",
    "    text = re.sub(\n",
    "        r\"(\\s|^)'(aight|cause)(\\s|$)\",\n",
    "        '\\g<1>\\g<2>\\g<3>',\n",
    "        text, flags = flags\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"(\\s|^)'t(was|is)(\\s|$)\", r'\\g<1>it \\g<2>\\g<3>',\n",
    "        text,\n",
    "        flags = flags\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"(\\s|^)ol'(\\s|$)\",\n",
    "        '\\g<1>old\\g<2>',\n",
    "        text, flags = flags\n",
    "    )\n",
    "        \n",
    "    text = re.sub(r\"\\b(aight)\\b\", 'alright', text, flags = flags)\n",
    "    text = re.sub(r'\\bcause\\b', 'because', text, flags = flags)\n",
    "    text = re.sub(r'\\b(finna|gonna)\\b', 'going to', text, flags = flags)\n",
    "    text = re.sub(r'\\bgimme\\b', 'give me', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgive'n\\b\", 'given', text, flags = flags)\n",
    "    text = re.sub(r\"\\bhowdy\\b\", 'how do you do', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgotta\\b\", 'got to', text, flags = flags)\n",
    "    text = re.sub(r\"\\binnit\\b\", 'is it not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(can)(not)\\b\", r'\\g<1> \\g<2>', text, flags = flags)\n",
    "    text = re.sub(r\"\\bwanna\\b\", 'want to', text, flags = flags)\n",
    "    text = re.sub(r\"\\bmethinks\\b\", 'me thinks', text, flags = flags)\n",
    "    text = re.sub(r\"\\bo'er\\b\", r'over', text, flags = flags)\n",
    "    text = re.sub(r\"\\bne'er\\b\", r'never', text, flags = flags)\n",
    "    text = re.sub(r\"\\bo'?clock\\b\", 'of the clock', text, flags = flags)\n",
    "    text = re.sub(r\"\\bma'am\\b\", 'madam', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgiv'n\\b\", 'given', text, flags = flags)\n",
    "    text = re.sub(r\"\\be'er\\b\", 'ever', text, flags = flags)\n",
    "    text = re.sub(r\"\\bd'ye\\b\", 'do you', text, flags = flags)\n",
    "    text = re.sub(r\"\\be'er\\b\", 'ever', text, flags = flags)\n",
    "    text = re.sub(r\"\\bd'ye\\b\", 'do you', text, flags = flags)\n",
    "    text = re.sub(r\"\\bg'?day\\b\", 'good day', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(ain|amn)'?t\\b\", 'am not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(are|can)'?t\\b\", r'\\g<1> not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(let)'?s\\b\", r'\\g<1> us', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all'dn't've'd\\b\", 'you all would not have had', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all're\\b\", 'you all are', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all'd've\\b\", 'you all would have', text, flags = flags)\n",
    "    text = re.sub(r\"(\\s)y'all(\\s)\", r'\\g<1>you all\\g<2>', text, flags = flags)  \n",
    "    text = re.sub(r\"\\b(won)'?t\\b\", 'will not', text, flags = flags)\n",
    "    text = re.sub(r\"\\bhe'd\\b\", 'he had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I|we|who)'?d'?ve\\b\", r'\\g<1> would have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(could|would|must|should|would)n'?t'?ve\\b\", r'\\g<1> not have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(he)'?dn'?t'?ve'?d\\b\", r'\\g<1> would not have had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(daren|daresn|dasn)'?t\", 'dare not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(he|how|i|it|she|that|there|these|they|we|what|where|which|who|you)'?ll\\b\", r'\\g<1> will', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(everybody|everyone|he|how|it|she|somebody|someone|something|that|there|this|what|when|where|which|who|why)'?s\\b\", r'\\g<1> is', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m'a\\b\", r'\\g<1> am about to', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m'o\\b\", r'\\g<1> am going to', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m\\b\", r'\\g<1> am', text, flags = flags)\n",
    "    text = re.sub(r\"\\bshan't\\b\", 'shall not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(are|could|did|does|do|go|had|has|have|is|may|might|must|need|ought|shall|should|was|were|would)n'?t\\b\", r'\\g<1> not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(could|had|he|i|may|might|must|should|these|they|those|to|we|what|where|which|who|would|you)'?ve\\b\", r'\\g<1> have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(how|so|that|there|these|they|those|we|what|where|which|who|why|you)'?re\\b\", r'\\g<1> are', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I|it|she|that|there|they|we|which|you)'?d\\b\", r'\\g<1> had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(how|what|where|who|why)'?d\\b\", r'\\g<1> did', text, flags = flags)    \n",
    "    return text\n",
    "\n",
    "\n",
    "class ExpandContractionsClass:\n",
    "    def __init__(self, nlp: Language):\n",
    "        self.nlp = nlp\n",
    "    \n",
    "    def __call__(self,doc: Doc):\n",
    "        text = doc.text\n",
    "        return self.nlp.make_doc(expand_contractions_text(text))\n",
    "    \n",
    "@Language.factory(\"expand_contractions_component\")\n",
    "def create_expand_contractions_component(nlp : Language, name: str):\n",
    "    return ExpandContractionsClass(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "if EXPAND_CONTRACTIONS:\n",
    "    nlp.add_pipe(\"expand_contractions_component\",before='tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):    \n",
    "    \n",
    "    text = re.sub(r'<br /><br />',\"$$\",text)\n",
    "    text = BeautifulSoup(text,'lxml').get_text().strip()\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = ' '.join(re.findall(r\"[\\w']+|[.,!;/\\\"]\", text))\n",
    "    \n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word == '':\n",
    "            continue\n",
    "        new_text.append(word)\n",
    "    \n",
    "    text = ' '.join(new_text)\n",
    "    words = nlp(text)\n",
    "    text =  \" \".join([token.text for token in words if not token.is_punct or token.text=='/' or token.text==\"\\\"\" or token.text==\".\"]).strip()\n",
    "    \n",
    "    new_sents = []\n",
    "    for sent in text.split(\".\"):\n",
    "        sent = sent.strip()\n",
    "        if sent!='' and len(sent)>1:\n",
    "        \n",
    "            new_sents.append(sent)\n",
    "\n",
    "    text = ' . '.join(new_sents)\n",
    "\n",
    "    new_words = []\n",
    "    for word in text.split(\" \"):\n",
    "        #if word == 'n\\'t':\n",
    "        #    if len(new_words) > 1:\n",
    "        #        new_words[-1] = new_words[-1] + word\n",
    "        #    else:\n",
    "        #        new_words.append(word)\n",
    "        if word == '\\'s':\n",
    "            if len(new_words) > 1:\n",
    "                new_words[-1] = new_words[-1] + word\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "            \n",
    "    text = \" \".join(new_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the training data which was given for Assignment 2\n",
    "def process_assignment2_training_data():\n",
    "    preprocessed_dataset = []\n",
    "    train_dataset_labels = []\n",
    "    with open(\"./Train dataset.csv\") as csvfile:\n",
    "        csvFile = csv.reader(csvfile)\n",
    "        next(csvFile)\n",
    "        json_writer = jsonlines.open('processed_dataset.jsonl','w')\n",
    "\n",
    "        for line in csvFile:\n",
    "            processed_text = preprocess_text(line[0])\n",
    "            label = 1.0 if line[1] == 'positive' else 0.0\n",
    "            train_dataset_labels.append(label)\n",
    "            json_writer.write({\"text\":processed_text,\"label\":label})\n",
    "            preprocessed_dataset.append({\"text\":processed_text,\"label\":label})\n",
    "    \n",
    "        json_writer.close()\n",
    "\n",
    "\n",
    "#process_assignment2_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = []\n",
    "train_dataset_labels = []\n",
    "\n",
    "TRAIN_FILE_NAME = './processed_dataset.jsonl'\n",
    "\n",
    "with open(TRAIN_FILE_NAME ,encoding='utf-8') as f:\n",
    "#with open('processed_dataset.jsonl',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        sample = json.loads(line)\n",
    "        train_dataset_labels.append(sample['label'])\n",
    "        preprocessed_dataset.append(sample)\n",
    "      \n",
    "train_dataset_labels = np.array(train_dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "\n",
    "for review in preprocessed_dataset:\n",
    "    #embedding,length = getWordEmbeddingforText(review['text'])\n",
    "    #processed_dataset.append({'text': embedding,'length': length,'label' : review['label']})\n",
    "    processed_dataset.append({'text' : review['text'], 'label' : review['label']})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataSet(Dataset):\n",
    "    def __init__(self,data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "dataset = ReviewDataSet(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validation split and an equal distriubition of classes\n",
    "\n",
    "berttokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_idx,valid_idx = train_test_split(np.arange(train_dataset_labels.shape[0]), \n",
    "    test_size=0.2,\n",
    "    shuffle= True,\n",
    "    stratify= train_dataset_labels,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "'''def collate_function(batch_data):\n",
    "    inputs = [b['text'] for b in batch_data]\n",
    "    lengths = [b['length'] for b in batch_data]\n",
    "    labels = torch.tensor([b['label'] for b in batch_data])\n",
    "\n",
    "    labels = labels.unsqueeze(1)\n",
    "    inputs = pad_sequence(inputs,batch_first=True)\n",
    "    return  {'input' : inputs , 'lengths': lengths , 'labels' : labels }'''\n",
    "\n",
    "def collate_function(batch_data):\n",
    "\n",
    "    inputs = [b['text'] for b in batch_data]\n",
    "\n",
    "    if USE_PRETRAINED_MODEL:\n",
    "        ids = berttokenizer(inputs,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "    else:\n",
    "        ids = berttokenizer(inputs,padding=True,truncation=False,return_tensors=\"pt\")\n",
    "\n",
    "    lengths = torch.sum(ids['attention_mask'],dim=1).tolist()\n",
    "    labels = torch.tensor([b['label'] for b in batch_data])\n",
    "    return {'input' : ids['input_ids'] ,'lengths' : lengths , 'attention_mask' : ids['attention_mask'] ,'labels' : labels }\n",
    "    \n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_dataloader = DataLoader(dataset,64,sampler=train_sampler,collate_fn=collate_function)\n",
    "valid_dataloader = DataLoader(dataset,64,sampler=valid_sampler,collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data['input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bertmodel =  DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/depressedcoder/DLNLP/Assignment5/parta/solution.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/depressedcoder/DLNLP/Assignment5/parta/solution.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenized_input \u001b[39m=\u001b[39m  berttokenizer(batch_data[\u001b[39m'\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m'\u001b[39;49m],padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/environments/gymenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2484\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2482\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2483\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2484\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2485\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/environments/gymenv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2542\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2539\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2545\u001b[0m     )\n\u001b[1;32m   2547\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2548\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2549\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2550\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2551\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "tokenized_input =  berttokenizer(batch_data['input'],padding=True,truncation=True,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = bertmodel(input_ids=batch_data['input'],attention_mask=batch_data['attention_mask'],return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedModel(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout = 0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        self.fc1 = nn.Linear(768,256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def freeze_weights(self):\n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "\n",
    "    def forward(self,inp,attention_mask):\n",
    "\n",
    "        output = self.bert_model(inp,attention_mask,return_dict=False)[0]\n",
    "        \n",
    "        out = output[:,0,:]\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.sigmoid(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model = PretrainedModel()\n",
    "pre_model.freeze_weights()\n",
    "\n",
    "inp = berttokenizer(batch_data['input'],padding=True,truncation=True,return_tensors='pt')\n",
    "out = pre_model(inp['input_ids'],inp['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout = 0.3, max_seq_length = 5000):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros((max_seq_length,d_model))\n",
    "\n",
    "        for i in range(max_seq_length):\n",
    "            for j in range(d_model//2):\n",
    "                pe[i,2*j] = math.sin(i/(10000.0**(2*j/d_model)))\n",
    "                pe[i,2*j+1] = math.cos(i/(10000.0**(2*j/d_model)))\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self,x,x_mask):\n",
    "        \n",
    "        for i in range(len(x_mask)):\n",
    "            x[i,:x_mask[i],:] = x[i,:x_mask[i],:] + self.pe[:x_mask[i],:] \n",
    "\n",
    "        return self.dropout(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self,n_tokens=30522, d_model = 512, nhead=8, dim_feed_forward = 2048, nlayers=6, dropout = 0.3,fc_dropout_p=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model,nhead,dim_feed_forward,batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers,nlayers)\n",
    "       \n",
    "        self.embedding_layer = nn.Embedding(n_tokens,d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(512,256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,1)\n",
    "\n",
    "        self.fc_dropout = nn.Dropout(fc_dropout_p)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "     \n",
    "    def create_src_mask(self,padding_mask):\n",
    "        \n",
    "        attention_mask = torch.zeros((len(padding_mask),max(padding_mask)),dtype=torch.bool)\n",
    "        for i in range(len(padding_mask)):\n",
    "            attention_mask[i,padding_mask[i]:] = True\n",
    "        return attention_mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding_layer.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "\n",
    "    def forward(self, inp, inp_mask):\n",
    "        \n",
    "        # inp = [ batch_size, max_seq_length]\n",
    "        # inp_mask =  ( Length = batch_size ) \n",
    "\n",
    "        embeddings = self.embedding_layer(inp) * math.sqrt(self.d_model)  # [batch_size, max_seq_length, embed_dim]\n",
    "        pos_encoded_embeddings = self.pos_encoder(embeddings,inp_mask)\n",
    "        src_attention_mask = self.create_src_mask(inp_mask)\n",
    "        output = self.transformer_encoder(pos_encoded_embeddings,src_key_padding_mask=src_attention_mask)\n",
    "        cls_vector = output[:,0,:]\n",
    "        out = F.relu(self.fc_dropout(self.fc1(cls_vector)))\n",
    "        out = F.relu(self.fc_dropout(self.fc2(out)))\n",
    "        out = self.sigmoid(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TransformerEncoderModel()\n",
    "inp = torch.randint(0,30522,(2,7))\n",
    "inp_mask = [4,7]\n",
    "\n",
    "out = t(inp,inp_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posencoding = PositionalEncoding(4)\n",
    "\n",
    "a = torch.randn((2,3,4))\n",
    "b = [2,1]\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posencoding(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import  datetime\n",
    "\n",
    "def train(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        train_dataloader,valid_dataloader,\n",
    "        num_epochs,\n",
    "        criterion,optimizer,\n",
    "        check_point_name,tensorboard_name,\n",
    "        device_train = device_fast,use_rnn = False,log=True\n",
    "    ):\n",
    "    \n",
    "    if log == True:\n",
    "        current_datetime = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "        tensorboard_name = tensorboard_name + \"_\" + current_datetime\n",
    "        writer = SummaryWriter('runs/' + tensorboard_name)\n",
    "    \n",
    "\n",
    "\n",
    "    model = model.to(device_train)\n",
    "\n",
    "    \n",
    "\n",
    "    best_validation_loss = 1000.0\n",
    "    valdiation_loss_not_decreased_steps = 0\n",
    "    \n",
    "    model.train()\n",
    "    for e in range(num_epochs):\n",
    "        \n",
    "        training_set_size = 0\n",
    "        training_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for data in tqdm(train_dataloader):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            #input_reviews,inp_lengths,output_labels = data['input'], data['lengths'],data['labels']\n",
    "            input_reviews, input_lengths , attention_mask ,output_labels = data['input'], data['lengths'], data['attention_mask'], data['labels']\n",
    "            \n",
    "            \n",
    "            #inp = tokenizer(input_review,padding=True,truncation=True,return_tensors='pt')\n",
    "\n",
    "            #input_reviews = inp['input_ids']\n",
    "            #attention_mask = inp['attention_mask']\n",
    "\n",
    "            input_reviews = input_reviews.to(device_train)\n",
    "            attention_mask = attention_mask.to(device_train)\n",
    "\n",
    "            training_set_size += input_reviews.shape[0]\n",
    "            \n",
    "            #output = model(input_reviews,inp_lengths)\n",
    "\n",
    "            if USE_PRETRAINED_MODEL:\n",
    "                output = model(input_reviews,attention_mask)\n",
    "            else:\n",
    "                output = model(input_reviews,input_lengths)\n",
    "\n",
    "            output = output.to(device_cpu)\n",
    "            loss = criterion(output,output_labels.float())\n",
    "            training_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        current_training_loss = training_loss / training_set_size\n",
    "        if log==True:\n",
    "            print(\"Epoch \" + str(e) + \" Average Training Loss = \" +  str(current_training_loss))\n",
    "            writer.add_scalars('Loss vs Epoch',{'train' : current_training_loss},e)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        if valid_dataloader is None:\n",
    "            continue\n",
    "        \n",
    "        validation_set_size  = 0 \n",
    "        if e% VALIDATION_LOSS_COMPUTE_STEP==0:\n",
    "            correct_count = 0\n",
    "            validation_loss = 0\n",
    "\n",
    "            for i,data in enumerate(valid_dataloader,0):\n",
    "                #input_reviews,inp_lengths,output_labels = data['input'], data['lengths'],data['labels']\n",
    "                \n",
    "                #input_review, output_labels = data['input'] , data['labels']\n",
    "                input_reviews, input_lengths , attention_mask ,output_labels = data['input'], data['lengths'], data['attention_mask'], data['labels']\n",
    "\n",
    "                #inp = tokenizer(input_review,padding=True,truncation=True,return_tensors='pt')\n",
    "\n",
    "\n",
    "                #input_reviews = inp['input_ids']\n",
    "                #attention_mask = inp['attention_mask']\n",
    "\n",
    "                input_reviews = input_reviews.to(device_train)\n",
    "                attention_mask = attention_mask.to(device_train)\n",
    "\n",
    "                #input_reviews = input_reviews.to(device_train)\n",
    "                validation_set_size += input_reviews.shape[0]\n",
    "                #output = model(input_reviews,attention_mask)\n",
    "                if USE_PRETRAINED_MODEL:\n",
    "                    output = model(input_reviews,attention_mask)\n",
    "                else:\n",
    "                    output = model(input_reviews,input_lengths)\n",
    "\n",
    "                #output = model(input_reviews,inp_lengths)\n",
    "                output = output.to(device_cpu)\n",
    "                loss = criterion(output,output_labels.float())\n",
    "                validation_loss += loss.item()\n",
    "                nearest_class = torch.round(output)\n",
    "\n",
    "                correct = (nearest_class == output_labels.float()).float()\n",
    "                correct_count += correct.sum()\n",
    "            correct_count = int(correct_count)\n",
    "            current_validation_accuracy = (correct_count/validation_set_size)*100\n",
    "            current_validation_loss = (1.0* validation_loss)/validation_set_size\n",
    "            if log == True:\n",
    "                print(\"Epoch \" + str(e) + \" \" +  \"Validation Loss = \" + str(current_validation_loss) )\n",
    "                print(\"Validation Set Accuracy = \" + str((correct_count/validation_set_size)*100) )\n",
    "\n",
    "                writer.add_scalar(' Validation Accuracy vs Epoch ',int((correct_count/validation_set_size)*100),e)\n",
    "                writer.add_scalars('Loss vs Epoch',{'valid' : current_validation_loss},e)\n",
    "            \n",
    "            if log==True:\n",
    "                if current_validation_loss < best_validation_loss:\n",
    "                    valdiation_loss_not_decreased_steps = 0\n",
    "                    torch.save(model.state_dict(),check_point_name)\n",
    "                    best_validation_loss = current_validation_loss\n",
    "                else:\n",
    "                    valdiation_loss_not_decreased_steps +=1\n",
    "         \n",
    "        if log == True:\n",
    "            if valdiation_loss_not_decreased_steps >= PATIENCE_PARAMETER:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "model = PretrainedModel()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "train(model,tokenizer,train_dataloader,valid_dataloader,EPOCHS,nn.BCELoss(),optimizer,'test','PretrainedModel',device_fast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(model,test_data,sentence_lengths,attention_mask,test_labels,device_test):\n",
    "   \n",
    "    #model.load_state_dict(torch.load(model_name))\n",
    "    model.eval()\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    test_labels = test_labels.to(device_test)\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        model = model.to(device_test)\n",
    "        data_point = test_data[i].to(device_test)\n",
    "        ans = model(data_point,[test_lengths[i]],sentence_lengths[i])\n",
    "        ans = torch.round(ans)\n",
    "        if ans[0][0] == test_labels[i]:\n",
    "            count+=1\n",
    "    \n",
    "    print(\"Accuracy = \" + str((count/len(test_data)*100)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word_embeddings = [] \n",
    "test_n_sents = []\n",
    "test_sentence_lengths = []\n",
    "test_dataset_labels = []  \n",
    "\n",
    "def getAssignmentTestData(load_from_trained=True):\n",
    "    test_processed_text = []\n",
    "    if not load_from_trained:\n",
    "        with open(\"./E0334 Assignment2 Test Dataset.csv\",encoding='utf-8') as csvfile:\n",
    "            csvFile = csv.reader(csvfile)\n",
    "            json_writer = jsonlines.open('test.jsonl','w')\n",
    "            next(csvFile)\n",
    "            for line in csvFile:\n",
    "                processed_text = preprocess_text(line[0])\n",
    "                label = 1.0 if line[1] == 'positive' else 0.0\n",
    "                json_writer.write({\"text\":processed_text,\"label\":label})\n",
    "                test_dataset_labels.append(label)\n",
    "                test_processed_text.append(processed_text)\n",
    "            json_writer.close()\n",
    "    else:\n",
    "        with open('./test.jsonl' ,encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                sample = json.loads(line)\n",
    "                test_dataset_labels.append(sample['label'])\n",
    "                test_processed_text.append(sample['text'])\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(test_processed_text)):\n",
    "        current_embeddings,current_sent_lengths,current_n_sent = review_to_embed(test_processed_text[i]) \n",
    "        test_word_embeddings.append(current_embeddings.clone().detach().unsqueeze(0))\n",
    "        test_n_sents.append(current_n_sent)\n",
    "        test_sentence_lengths.append([current_sent_lengths])\n",
    "\n",
    "getAssignmentTestData(load_from_trained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test('ValAccHierarchialSelfAttention_real_Key_0.99_300_256_2_True_256_256_2_True_256_0.0_0.3_0.3_0.3.pth',test_word_embeddings,test_sentence_lengths,test_n_sents,test_dataset_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1b0110cf1cb03549be737c5657a86ea4daeeb81469a7991ed915d907f3e629c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
