{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Question</th>\n",
       "      <th>Equation</th>\n",
       "      <th>Input Numbers</th>\n",
       "      <th>Output</th>\n",
       "      <th>algebraic_symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>when relatives visit haley and her family she ...</td>\n",
       "      <td>how many will each receive if she gives everyo...</td>\n",
       "      <td>/ number0 number1</td>\n",
       "      <td>48 6</td>\n",
       "      <td>8</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>at the fair amy started with number0 tickets a...</td>\n",
       "      <td>how many tickets did amy have total ?</td>\n",
       "      <td>+ number0 number1</td>\n",
       "      <td>33 21</td>\n",
       "      <td>54</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>an architect was building a hotel downtown . h...</td>\n",
       "      <td>how many rooms does the hotel have totaled ?</td>\n",
       "      <td>* number0 number1</td>\n",
       "      <td>3 8</td>\n",
       "      <td>24</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>after going down the falls he then went to roc...</td>\n",
       "      <td>how far is the rocky mist from the city ?</td>\n",
       "      <td>* number0 number1</td>\n",
       "      <td>50 8</td>\n",
       "      <td>400</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>zachary did number0 push-ups in gym class toda...</td>\n",
       "      <td>how many push-ups did david do ?</td>\n",
       "      <td>+ number0 number1</td>\n",
       "      <td>47 15</td>\n",
       "      <td>62</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>matthew had number0 crackers . if matthew gave...</td>\n",
       "      <td>how many crackers did each person eat ?</td>\n",
       "      <td>/ number0 number1</td>\n",
       "      <td>27 9</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>if carl has a total of number0 stamps and kevi...</td>\n",
       "      <td>how many more stamps does carl have more than ...</td>\n",
       "      <td>- number0 number1</td>\n",
       "      <td>89 57</td>\n",
       "      <td>32</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>there were number0 soccer balls in the bag . c...</td>\n",
       "      <td>how many soccer balls did he pick up ?</td>\n",
       "      <td>- number1 number0</td>\n",
       "      <td>6 24</td>\n",
       "      <td>18</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>for gwen 's birthday she received number0 doll...</td>\n",
       "      <td>how much money did she still have ?</td>\n",
       "      <td>- number0 number1</td>\n",
       "      <td>5 3</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>paul had number0 strawberries in his basket . ...</td>\n",
       "      <td>how many strawberries did he have then ?</td>\n",
       "      <td>+ number0 number1</td>\n",
       "      <td>28 35</td>\n",
       "      <td>63</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>881 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Description  \\\n",
       "272  when relatives visit haley and her family she ...   \n",
       "872  at the fair amy started with number0 tickets a...   \n",
       "718  an architect was building a hotel downtown . h...   \n",
       "233  after going down the falls he then went to roc...   \n",
       "491  zachary did number0 push-ups in gym class toda...   \n",
       "..                                                 ...   \n",
       "955  matthew had number0 crackers . if matthew gave...   \n",
       "166  if carl has a total of number0 stamps and kevi...   \n",
       "591  there were number0 soccer balls in the bag . c...   \n",
       "651  for gwen 's birthday she received number0 doll...   \n",
       "303  paul had number0 strawberries in his basket . ...   \n",
       "\n",
       "                                              Question           Equation  \\\n",
       "272  how many will each receive if she gives everyo...  / number0 number1   \n",
       "872              how many tickets did amy have total ?  + number0 number1   \n",
       "718       how many rooms does the hotel have totaled ?  * number0 number1   \n",
       "233          how far is the rocky mist from the city ?  * number0 number1   \n",
       "491                   how many push-ups did david do ?  + number0 number1   \n",
       "..                                                 ...                ...   \n",
       "955            how many crackers did each person eat ?  / number0 number1   \n",
       "166  how many more stamps does carl have more than ...  - number0 number1   \n",
       "591             how many soccer balls did he pick up ?  - number1 number0   \n",
       "651                how much money did she still have ?  - number0 number1   \n",
       "303           how many strawberries did he have then ?  + number0 number1   \n",
       "\n",
       "    Input Numbers Output algebraic_symbols  \n",
       "272          48 6      8   [0, 0, 0, 1, 0]  \n",
       "872         33 21     54   [1, 0, 0, 0, 0]  \n",
       "718           3 8     24   [0, 0, 1, 0, 0]  \n",
       "233          50 8    400   [0, 0, 1, 0, 0]  \n",
       "491         47 15     62   [1, 0, 0, 0, 0]  \n",
       "..            ...    ...               ...  \n",
       "955          27 9      3   [0, 0, 0, 1, 0]  \n",
       "166         89 57     32   [0, 1, 0, 0, 0]  \n",
       "591          6 24     18   [0, 1, 0, 0, 0]  \n",
       "651           5 3      2   [0, 1, 0, 0, 0]  \n",
       "303         28 35     63   [1, 0, 0, 0, 0]  \n",
       "\n",
       "[881 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration , BertModel , BertTokenizer\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "train_dataset_name = 'ArithOpsTrain.xlsx'\n",
    "df = pd.read_excel(train_dataset_name)\n",
    "df = df.drop('Table 1',axis=1)\n",
    "df = df.rename(columns=df.iloc[0]).loc[1:]\n",
    "\n",
    "device_cpu = torch.device('cpu')\n",
    "device_fast = torch.device('cpu')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_fast = torch.device('cuda')\n",
    "\n",
    "\n",
    "counters = {\"[PAD]\":1,\"<SOS>\":2,\"<EOS>\" : 3 , \"+\" : 4, \"-\" :5 , \"*\" : 6 , \"/\" : 7 }\n",
    "for i in range(10):\n",
    "    counters[\"number\"+str(i)] = i + 8\n",
    "\n",
    "algebraic_symbols = []\n",
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]['Equation']\n",
    "    current_algebraic_symbol = [0 for i in range(5)]\n",
    "    for sym in row.split(' '):\n",
    "        if sym in ['+','-','*','/','%']:\n",
    "            current_algebraic_symbol[counters[sym]-4]+=1\n",
    "    algebraic_symbols.append(str(current_algebraic_symbol))\n",
    "\n",
    "df['algebraic_symbols'] = algebraic_symbols\n",
    "output_vocabulary = vocab(counters,)\n",
    "\n",
    "train_df , valid_df = train_test_split(df,test_size=0.1,random_state=0,stratify=df['algebraic_symbols'])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Question</th>\n",
       "      <th>Equation</th>\n",
       "      <th>Input Numbers</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>mrs. hilt is baking bread . she needs number0 ...</td>\n",
       "      <td>how much flour will she need to make number2 l...</td>\n",
       "      <td>/ number0 number1</td>\n",
       "      <td>5 2 1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>robin had number0 songs on her number1 player ...</td>\n",
       "      <td>how many songs does she have on her number4 pl...</td>\n",
       "      <td>+ - number0 number2 number3</td>\n",
       "      <td>30 3 8 10 3</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>there are number0 more sections that are undev...</td>\n",
       "      <td>what is the total area of the undeveloped land ?</td>\n",
       "      <td>* number0 number1</td>\n",
       "      <td>3 2435</td>\n",
       "      <td>7305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>mom made number0 chocolate chip cookies . it t...</td>\n",
       "      <td>how many cookies were left ?</td>\n",
       "      <td>- number0 number3</td>\n",
       "      <td>32 24 16 9</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>number0 children are taking a bus to the zoo ....</td>\n",
       "      <td>how many seats will the children need in all ?</td>\n",
       "      <td>/ number0 number1</td>\n",
       "      <td>58 2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Description  \\\n",
       "373  mrs. hilt is baking bread . she needs number0 ...   \n",
       "901  robin had number0 songs on her number1 player ...   \n",
       "254  there are number0 more sections that are undev...   \n",
       "468  mom made number0 chocolate chip cookies . it t...   \n",
       "333  number0 children are taking a bus to the zoo ....   \n",
       "\n",
       "                                              Question  \\\n",
       "373  how much flour will she need to make number2 l...   \n",
       "901  how many songs does she have on her number4 pl...   \n",
       "254   what is the total area of the undeveloped land ?   \n",
       "468                       how many cookies were left ?   \n",
       "333     how many seats will the children need in all ?   \n",
       "\n",
       "                        Equation Input Numbers Output  \n",
       "373            / number0 number1         5 2 1    2.5  \n",
       "901  + - number0 number2 number3   30 3 8 10 3     32  \n",
       "254            * number0 number1        3 2435   7305  \n",
       "468            - number0 number3    32 24 16 9     23  \n",
       "333            / number0 number1          58 2     29  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class T5Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data  : pd.DataFrame,\n",
    "        tokenizer : T5Tokenizer,\n",
    "        text_max_token_length = 512,\n",
    "        output_max_token_length = 128\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data \n",
    "        self.text_max_token_length = text_max_token_length\n",
    "        self.output_max_token_length = output_max_token_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        input_text = data_row[\"Description\"]\n",
    "        input_question = data_row[\"Question\"]\n",
    "\n",
    "        in_text = input_text + \" [SEP] \" + input_question\n",
    "        \n",
    "        input_text_encoding = self.tokenizer(\n",
    "            in_text,\n",
    "            max_length=self.text_max_token_length,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        \n",
    "        output_text = data_row[\"Equation\"]        \n",
    "        output_text = \"<SOS> \" + output_text + \" <EOS>\"\n",
    "        output_tokens = output_text.split()\n",
    "\n",
    "        output_tokens_id_full = torch.zeros((self.output_max_token_length,),dtype=torch.int64)\n",
    "        output_tokens_id = torch.tensor(output_vocabulary.forward(output_tokens),dtype=torch.int64)\n",
    "        \n",
    "        output_tokens_id_full[:len(output_tokens)] = output_tokens_id\n",
    "\n",
    "        output_attention_mask = torch.zeros((self.output_max_token_length,))\n",
    "        output_attention_mask[:len(output_tokens_id)] = 1\n",
    "        \n",
    "        output_text_encoding = self.tokenizer(\n",
    "            output_text,\n",
    "            max_length=self.output_max_token_length,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "\n",
    "        return dict(\n",
    "            input_text = input_text,\n",
    "            output_text = output_text,\n",
    "            input_text_ids = input_text_encoding['input_ids'].flatten(),\n",
    "            input_attention_mask = input_text_encoding['attention_mask'].flatten(),\n",
    "            output_text_ids = output_text_encoding['input_ids'].flatten(),\n",
    "            output_attention_mask = output_text_encoding['attention_mask'].flatten(),\n",
    "            output_text_ids_custom_tokenizer = output_tokens_id_full,\n",
    "            output_attention_mask_custom_tokenizer = output_attention_mask,\n",
    "        )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\",model_max_length=512)\n",
    "special_tokens_dict = {'additional_special_tokens' : ['[SEP]']}\n",
    "num_added_tokens = t5_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "t5_model =T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_dataset = T5Dataset(train_df,t5_tokenizer)\n",
    "#valid_dataset = T5Dataset(valid_df,t5_tokenizer)\n",
    "train_dataset = T5Dataset(train_df,bert_tokenizer)\n",
    "valid_dataset = T5Dataset(valid_df,bert_tokenizer)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,32,True)\n",
    "valid_dataloader = DataLoader(valid_dataset,32,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4., 7.,  ..., 0., 0., 0.],\n",
       "        [1., 4., 7.,  ..., 0., 0., 0.],\n",
       "        [1., 3., 5.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 4., 7.,  ..., 0., 0., 0.],\n",
       "        [1., 4., 8.,  ..., 0., 0., 0.],\n",
       "        [1., 5., 7.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data['output_text_ids_custom_tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postfix_evaluation(batch_data,input_values):\n",
    "\n",
    "    arith_symbols = set(['+','-','*','/','%'])\n",
    "    output_values = []\n",
    "    \n",
    "    for i in range(len(batch_data)):\n",
    "        flag = True\n",
    "        current_input = batch_data[i].split(' ')\n",
    "        current_input.reverse()\n",
    "        input_value = input_values[i]\n",
    "\n",
    "        stack = []\n",
    "        for symbol in current_input:\n",
    "            if symbol in arith_symbols:\n",
    "                if len(stack)<2:\n",
    "                    flag = False\n",
    "                    break\n",
    "                in1 = stack.pop(-1)\n",
    "                in2 = stack.pop(-1)\n",
    "\n",
    "                res = 0\n",
    "                if symbol=='+':\n",
    "                    res = in1+in2\n",
    "                elif symbol=='-':\n",
    "                    res = in1 - in2 \n",
    "                elif symbol == '*':\n",
    "                    res = in1 * in2\n",
    "                elif symbol=='/':\n",
    "                    res = in1/in2\n",
    "                else:\n",
    "                    res = in1 % in2\n",
    "                stack.append(res)\n",
    "\n",
    "\n",
    "            else:\n",
    "                if \"number\" in symbol:\n",
    "                    index = int(symbol[6])\n",
    "                    stack.append(input_value[index])\n",
    "\n",
    "        if flag==False or len(stack)!=1:\n",
    "            output_values.append(0)\n",
    "        else:\n",
    "            output_values.append(stack.pop(-1))\n",
    "\n",
    "    ans = torch.tensor(output_values)\n",
    "    return ans\n",
    "\n",
    "ans = postfix_evaluation([\"+ - number0 number1 number2\",\"+ / - number0 number2 number1 number3\"],[[1,4,6],[5,6,7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,dim_model,dropout_p,max_len) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout =  nn.Dropout(dropout_p)\n",
    "\n",
    "        pos_encoding = torch.zeros(max_len,dim_model)\n",
    "        \n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) \n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) \n",
    "        \n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        #pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(1), :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens_input,\n",
    "        num_tokens_output,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model,\n",
    "            dropout_p= dropout_p,\n",
    "            max_len=5000\n",
    "        )\n",
    "\n",
    "        self.src_embedding = nn.Embedding.from_pretrained(bert_model.embeddings.word_embeddings.weight,freeze=False)\n",
    "        #self.src_embedding = nn.Embedding(num_tokens_input,dim_model)\n",
    "        self.trg_embedding = nn.Embedding(num_tokens_output,dim_model)\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout= dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(self.dim_model,num_tokens_output)\n",
    "\n",
    "    \n",
    "    def forward(self, src, trg, src_padding_mask=None,target_mask=None, target_padding_mask=None):\n",
    "\n",
    "        src = self.src_embedding(src) * math.sqrt(self.dim_model)\n",
    "        target = self.trg_embedding(trg) * math.sqrt(self.dim_model)\n",
    "        #print(target.shape)\n",
    "        src = self.positional_encoder(src)\n",
    "        target = self.positional_encoder(target)\n",
    "        \n",
    "        transformer_out = self.transformer(\n",
    "            src=  src,tgt = target,tgt_mask=target_mask,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=target_padding_mask\n",
    "        )\n",
    "        out = self.out(transformer_out)\n",
    "        return out\n",
    "    \n",
    "        \n",
    "    def get_tgt_mask(self,size):\n",
    "        \n",
    "        mask = torch.tril(torch.ones(size,size) == 1)\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask==0,float('-inf'))\n",
    "        mask = mask.masked_fill(mask==1,float(0.0))\n",
    "        mask = mask.to(device_fast)\n",
    "        return mask\n",
    "\n",
    "    def get_padding_mask(self,matrix,pad_token):\n",
    "        return (matrix==pad_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "class TransformerTranslator(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens_input,\n",
    "        num_tokens_output,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.transformer = TransformerModel(\n",
    "                num_tokens_input=num_tokens_input,\n",
    "                num_tokens_output=num_tokens_output,\n",
    "                dim_model=dim_model,\n",
    "                num_heads=num_heads,\n",
    "                num_encoder_layers=num_encoder_layers,\n",
    "                num_decoder_layers=num_decoder_layers,\n",
    "                dim_feedforward= dim_feedforward,\n",
    "                dropout_p=dropout_p\n",
    "            )\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, src, trg, src_padding_mask=None,target_mask=None, target_padding_mask=None):\n",
    "\n",
    "        return self.transformer(src,trg,src_padding_mask,target_mask,target_padding_mask)\n",
    "        \n",
    "\n",
    "    def training_step(self, batch_data,batch_idx):\n",
    "\n",
    "        input_text_ids = batch_data['input_text_ids']\n",
    "        input_attention_mask = batch_data['input_attention_mask']\n",
    "        #output_text_ids = batch_data['output_text_ids']\n",
    "        #output_attention_mask = batch_data['output_attention_mask']\n",
    "\n",
    "        output_text_ids = batch_data['output_text_ids_custom_tokenizer']\n",
    "        output_attention_mask = batch_data['output_attention_mask_custom_tokenizer']\n",
    "        \n",
    "        output_in = output_text_ids[:,:-1]\n",
    "        output_expected = output_text_ids[:,1:]\n",
    "\n",
    "        \n",
    "        target_mask = self.transformer.get_tgt_mask(output_expected.shape[1])\n",
    "\n",
    "        src_padding_mask = self.transformer.get_padding_mask(input_attention_mask,0)\n",
    "        \n",
    "        tgt_padding_mask = self.transformer.get_padding_mask(output_attention_mask[:,:-1],0)\n",
    "\n",
    "\n",
    "        predictions = self(input_text_ids,output_in,src_padding_mask,target_mask,tgt_padding_mask)\n",
    "\n",
    "        loss_value = None\n",
    "        \n",
    "        for i in range(predictions.shape[0]):\n",
    "            if loss_value == None:\n",
    "                loss_value = self.loss_fn(predictions[i],output_expected[i])\n",
    "            else:\n",
    "                loss_value += self.loss_fn(predictions[i],output_expected[i])\n",
    "\n",
    "        train_loss = loss_value*(1.0/predictions.shape[0])\n",
    "\n",
    "\n",
    "        #train_loss = self.loss_fn(predictions,output_expected)\n",
    "        \n",
    "        self.log(\"train_loss\" , train_loss, prog_bar=True,logger=True)\n",
    "  \n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch_data,batch_idx):\n",
    "        \n",
    "        input_text_ids = batch_data['input_text_ids']\n",
    "        input_attention_mask = batch_data['input_attention_mask']\n",
    "        #output_text_ids = batch_data['output_text_ids']\n",
    "        #output_attention_mask = batch_data['output_attention_mask']\n",
    "\n",
    "        output_text_ids = batch_data['output_text_ids_custom_tokenizer']\n",
    "        output_attention_mask = batch_data['output_attention_mask_custom_tokenizer']\n",
    "        \n",
    "        output_in = output_text_ids[:,:-1]\n",
    "        output_expected = output_text_ids[:,1:]\n",
    "\n",
    "        \n",
    "        target_mask = self.transformer.get_tgt_mask(output_expected.shape[1])\n",
    "\n",
    "        src_padding_mask = self.transformer.get_padding_mask(input_attention_mask,0)\n",
    "        \n",
    "        tgt_padding_mask = self.transformer.get_padding_mask(output_attention_mask[:,:-1],0)\n",
    "\n",
    "\n",
    "        predictions = self(input_text_ids,output_in,src_padding_mask,target_mask,tgt_padding_mask)\n",
    "\n",
    "\n",
    "        loss_value = None\n",
    "        \n",
    "        for i in range(predictions.shape[0]):\n",
    "            if loss_value == None:\n",
    "                loss_value = self.loss_fn(predictions[i],output_expected[i])\n",
    "            else:\n",
    "                loss_value += self.loss_fn(predictions[i],output_expected[i])\n",
    "\n",
    "        valid_loss = loss_value*(1.0/predictions.shape[0])\n",
    "        #valid_loss = self.loss_fn(predictions,output_expected)\n",
    "        \n",
    "        self.log(\"valid_loss\" , valid_loss, prog_bar=True,logger=True)\n",
    "  \n",
    "        return valid_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(),lr = 0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class T5ArithTranslator(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, input_attention_mask, decoder_attention_mask, labels):\n",
    "\n",
    "        outs = self.t5_model(input_ids=input_ids,attention_mask = input_attention_mask,labels = labels)        \n",
    "        return outs.loss ,  outs.logits\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx) :\n",
    "        \n",
    "        input_text_ids = batch[\"input_text_ids\"]\n",
    "        input_attention_mask = batch[\"input_attention_mask\"]\n",
    "        output_text_ids = batch[\"output_text_ids\"]\n",
    "        output_attention_mask = batch[\"output_attention_mask\"]\n",
    "\n",
    "        loss, outs = self(\n",
    "            input_text_ids,\n",
    "            input_attention_mask,\n",
    "            output_attention_mask,\n",
    "            output_text_ids\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\" , loss, prog_bar=True,logger=True)\n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        input_text_ids = batch[\"input_text_ids\"]\n",
    "        input_attention_mask = batch[\"input_attention_mask\"]\n",
    "        output_text_ids = batch[\"output_text_ids\"]\n",
    "        output_attention_mask = batch[\"output_attention_mask\"]\n",
    "\n",
    "        loss, outs = self(\n",
    "            input_text_ids,\n",
    "            input_attention_mask,\n",
    "            output_attention_mask,\n",
    "            output_text_ids\n",
    "        )\n",
    "\n",
    "        self.log(\"valid_loss\" , loss, prog_bar=True,logger=True)\n",
    "        return loss \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(),lr = 0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = \"checkpoints\",\n",
    "    filename=\"transformer-scratch-best-checkpoint\",\n",
    "    save_top_k = 1,\n",
    "    verbose = True,\n",
    "    monitor=\"valid_loss\",\n",
    "    mode = \"min\"\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"transformer_scratch_logs\",name=\"transformertranslator\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger = logger,\n",
    "    callbacks =  checkpoint_callback,\n",
    "    max_epochs=N_EPOCHS,\n",
    "    log_every_n_steps=5,\n",
    "    gpus=1,\n",
    "    accelerator='gpu'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_tokens_input=30522\n",
    "Num_tokens_output=len(output_vocabulary)\n",
    "Dim_model=768\n",
    "Num_heads=8\n",
    "Num_encoder_layers=6\n",
    "Num_decoder_layers=6\n",
    "Dim_feedforward= 2048\n",
    "Dropout_p=0.1\n",
    "\n",
    "model = TransformerTranslator(\n",
    "    Num_tokens_input,\n",
    "    Num_tokens_output,\n",
    "    Dim_model,\n",
    "    Num_heads,\n",
    "    Num_encoder_layers,\n",
    "    Num_decoder_layers,\n",
    "    Dim_feedforward,\n",
    "    Dropout_p\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model,train_dataloader,valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model = T5ArithTranslator.load_from_checkpoint(\n",
    "#    '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt'\n",
    "#)\n",
    "'''\n",
    "test_model =  TransformerTranslator.load_from_checkpoint(\n",
    "    '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt'\n",
    ")\n",
    "test_model.freeze()'''\n",
    "\n",
    "Num_tokens_input=30522\n",
    "Num_tokens_output=len(output_vocabulary)\n",
    "Dim_model=768\n",
    "Num_heads=8\n",
    "Num_encoder_layers=6\n",
    "Num_decoder_layers=6\n",
    "Dim_feedforward= 2048\n",
    "Dropout_p=0.1\n",
    "\n",
    "\n",
    "test_model = TransformerTranslator(\n",
    "    Num_tokens_input,\n",
    "    Num_tokens_output,\n",
    "    Dim_model,\n",
    "    Num_heads,\n",
    "    Num_encoder_layers,\n",
    "    Num_decoder_layers,\n",
    "    Dim_feedforward,\n",
    "    Dropout_p\n",
    ")\n",
    "\n",
    "test_model.load_state_dict(torch.load('./checkpoints/transformer-scratch-best-checkpoint.ckpt',map_location=device_fast)[\"state_dict\"])\n",
    "test_model.eval()\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "special_tokens_dict = {'additional_special_tokens' : ['[SEP]']}\n",
    "num_added_tokens = t5_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model, input_sequence, max_length=128, SOS_token=1, EOS_token=2):\n",
    "    \"\"\"\n",
    "    Method from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_sequence = input_sequence.to(device_fast)\n",
    "    \n",
    "\n",
    "    y_input = torch.tensor([[1]], dtype=torch.long, device=device_fast)\n",
    "    num_tokens = len(input_sequence[0])\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Get source mask\n",
    "        tgt_mask = model.transformer.get_tgt_mask(y_input.size(1)).to(device_fast)\n",
    "        \n",
    "        pred = model(input_sequence, y_input, target_mask=tgt_mask)\n",
    "        \n",
    "        next_item = pred.topk(1)[1].view(-1)[-1].item() # num with highest probability\n",
    "        next_item = torch.tensor([[next_item]], device=device_fast)\n",
    "\n",
    "        # Concatenate previous input with predicted best word\n",
    "        y_input = torch.cat((y_input, next_item), dim=1)\n",
    "\n",
    "        # Stop if model predicts end of sentence\n",
    "        if next_item.view(-1).item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.view(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = bert_tokenizer(\"last stop in their field trip was the aquarium . penny identified number0 species of sharks number1 species of eels and number2 different species of whales . [SEP] how many species was penny able to identify ?\",return_tensors='pt').input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = test_model.to(device_fast)\n",
    "ans = predict(test_model,test_input_ids.to(device_fast))\n",
    "print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = test_model.t5_model.generate(test_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#print(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gymenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b0110cf1cb03549be737c5657a86ea4daeeb81469a7991ed915d907f3e629c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
