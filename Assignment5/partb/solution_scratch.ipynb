{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "train_dataset_name = 'ArithOpsTrain.xlsx'\n",
    "df = pd.read_excel(train_dataset_name)\n",
    "df = df.drop('Table 1',axis=1)\n",
    "df = df.rename(columns=df.iloc[0]).loc[1:]\n",
    "\n",
    "train_df , valid_df = train_test_split(df,test_size=0.1,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Question</th>\n",
       "      <th>Equation</th>\n",
       "      <th>Input Numbers</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>mrs. hilt is baking bread . she needs number0 ...</td>\n",
       "      <td>how much flour will she need to make number2 l...</td>\n",
       "      <td>/ number0 number1</td>\n",
       "      <td>5 2 1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>robin had number0 songs on her number1 player ...</td>\n",
       "      <td>how many songs does she have on her number4 pl...</td>\n",
       "      <td>+ - number0 number2 number3</td>\n",
       "      <td>30 3 8 10 3</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>there are number0 more sections that are undev...</td>\n",
       "      <td>what is the total area of the undeveloped land ?</td>\n",
       "      <td>* number0 number1</td>\n",
       "      <td>3 2435</td>\n",
       "      <td>7305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>mom made number0 chocolate chip cookies . it t...</td>\n",
       "      <td>how many cookies were left ?</td>\n",
       "      <td>- number0 number3</td>\n",
       "      <td>32 24 16 9</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>number0 children are taking a bus to the zoo ....</td>\n",
       "      <td>how many seats will the children need in all ?</td>\n",
       "      <td>/ number0 number1</td>\n",
       "      <td>58 2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Description  \\\n",
       "373  mrs. hilt is baking bread . she needs number0 ...   \n",
       "901  robin had number0 songs on her number1 player ...   \n",
       "254  there are number0 more sections that are undev...   \n",
       "468  mom made number0 chocolate chip cookies . it t...   \n",
       "333  number0 children are taking a bus to the zoo ....   \n",
       "\n",
       "                                              Question  \\\n",
       "373  how much flour will she need to make number2 l...   \n",
       "901  how many songs does she have on her number4 pl...   \n",
       "254   what is the total area of the undeveloped land ?   \n",
       "468                       how many cookies were left ?   \n",
       "333     how many seats will the children need in all ?   \n",
       "\n",
       "                        Equation Input Numbers Output  \n",
       "373            / number0 number1         5 2 1    2.5  \n",
       "901  + - number0 number2 number3   30 3 8 10 3     32  \n",
       "254            * number0 number1        3 2435   7305  \n",
       "468            - number0 number3    32 24 16 9     23  \n",
       "333            / number0 number1          58 2     29  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data  : pd.DataFrame,\n",
    "        tokenizer : T5Tokenizer,\n",
    "        text_max_token_length = 512,\n",
    "        output_max_token_length = 128\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data \n",
    "        self.text_max_token_length = text_max_token_length,\n",
    "        self.output_max_token_length = output_max_token_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        input_text = data_row[\"Description\"]\n",
    "        input_question = data_row[\"Question\"]\n",
    "\n",
    "        in_text = input_text + \" [SEP] \" + input_question\n",
    "        \n",
    "        input_text_encoding = self.tokenizer(\n",
    "            in_text,\n",
    "            max_length=100,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        output_text = data_row[\"Equation\"]        \n",
    "        output_text_encoding = self.tokenizer(\n",
    "            output_text,\n",
    "            max_length=100,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            input_text = input_text,\n",
    "            output_text = output_text,\n",
    "            input_text_ids = input_text_encoding['input_ids'].flatten(),\n",
    "            input_attention_mask = input_text_encoding['attention_mask'].flatten(),\n",
    "            output_text_ids = output_text_encoding['input_ids'].flatten(),\n",
    "            output_attention_mask = output_text_encoding['attention_mask'].flatten()\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/depressedcoder/environments/gymenv/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "special_tokens_dict = {'additional_special_tokens' : ['[SEP]']}\n",
    "num_added_tokens = t5_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "\n",
    "\n",
    "t5_model =T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "dataset = T5Dataset(train_df,t5_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [32100, 37, 4016, 30278, 1681, 1], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_tokenizer(\"[SEP] The greatest integer function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output  = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class T5DataSetModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df : pd.DataFrame,\n",
    "        valid_df : pd.DataFrame,\n",
    "        tokenizer : T5Tokenizer,\n",
    "        batch_size = 32,\n",
    "        text_max_token_length = 512,\n",
    "        output_max_token_length = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.tokenizer = tokenizer,\n",
    "        self.batch_size = batch_size,\n",
    "        self.text_max_token_length = text_max_token_length\n",
    "        self.output_max_token_length = output_max_token_length\n",
    "\n",
    "    \n",
    "    def setup(self, stage = None):\n",
    "        \n",
    "        self.train_dataset = T5Dataset(self.train_df,self.tokenizer,self.text_max_token_length,self.output_max_token_length)\n",
    "        self.valid_dataset = T5Dataset(self.valid_df,self.tokenizer,self.text_max_token_length,self.output_max_token_length)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "#data_module = T5DataSetModule(train_df,valid_df,t5_tokenizer,BATCH_SIZE)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postfix_evaluation(batch_data,input_values):\n",
    "\n",
    "    arith_symbols = set(['+','-','*','/','%'])\n",
    "    output_values = []\n",
    "    \n",
    "    for i in range(len(batch_data)):\n",
    "        flag = True\n",
    "        current_input = batch_data[i].split(' ')\n",
    "        current_input.reverse()\n",
    "        input_value = input_values[i]\n",
    "\n",
    "        stack = []\n",
    "        for symbol in current_input:\n",
    "            if symbol in arith_symbols:\n",
    "                if len(stack)<2:\n",
    "                    flag = False\n",
    "                    break\n",
    "                in1 = stack.pop(-1)\n",
    "                in2 = stack.pop(-1)\n",
    "\n",
    "                res = 0\n",
    "                if symbol=='+':\n",
    "                    res = in1+in2\n",
    "                elif symbol=='-':\n",
    "                    res = in1 - in2 \n",
    "                elif symbol == '*':\n",
    "                    res = in1 * in2\n",
    "                elif symbol=='/':\n",
    "                    res = in1/in2\n",
    "                else:\n",
    "                    res = in1 % in2\n",
    "                stack.append(res)\n",
    "\n",
    "\n",
    "            else:\n",
    "                if \"number\" in symbol:\n",
    "                    index = int(symbol[6])\n",
    "                    stack.append(input_value[index])\n",
    "\n",
    "        if flag==False or len(stack)!=1:\n",
    "            output_values.append(0)\n",
    "        else:\n",
    "            output_values.append(stack.pop(-1))\n",
    "    \n",
    "    return output_values\n",
    "\n",
    "ans = postfix_evaluation([\"+ - number0 number1 number2\",\"+ / - number0 number2 number1 number3\"],[[1,4,6],[5,6,7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,dim_model,dropout_p,max_len) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout =  nn.Dropout(dropout_p)\n",
    "\n",
    "        pos_encoding = torch.zeros(max_len,dim_model)\n",
    "        \n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) \n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) \n",
    "        \n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model,\n",
    "            dropout_p= dropout_p,\n",
    "            max_len=5000\n",
    "        )\n",
    "\n",
    "        self.embedding = nn.Embedding(num_tokens,dim_model)\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout= dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(self.dim_model,num_tokens)\n",
    "\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "\n",
    "        src = self.embedding(src) * math.sqrt(self.dim_model)\n",
    "        #target = self.embedding(trg) * math.sqrt(self.dim_model)\n",
    "\n",
    "        src = self.positional_encoder(src)\n",
    "        #trg = self.positional_encoder(trg)\n",
    "        \n",
    "        print(src.shape)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "transformermodel = TransformerModel(30,20,2,1,1,512,0.01)\n",
    "r = torch.tensor([[1,5,6],[6,1,2]])\n",
    "transformermodel(r,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "class T5ArithTranslator(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, input_attention_mask, decoder_attention_mask, labels):\n",
    "\n",
    "        outs = self.t5_model(input_ids=input_ids,attention_mask = input_attention_mask,labels = labels)        \n",
    "        return outs.loss ,  outs.logits\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx) :\n",
    "        \n",
    "        input_text_ids = batch[\"input_text_ids\"]\n",
    "        input_attention_mask = batch[\"input_attention_mask\"]\n",
    "        output_text_ids = batch[\"output_text_ids\"]\n",
    "        output_attention_mask = batch[\"output_attention_mask\"]\n",
    "\n",
    "        loss, outs = self(\n",
    "            input_text_ids,\n",
    "            input_attention_mask,\n",
    "            output_attention_mask,\n",
    "            output_text_ids\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\" , loss, prog_bar=True,logger=True)\n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        input_text_ids = batch[\"input_text_ids\"]\n",
    "        input_attention_mask = batch[\"input_attention_mask\"]\n",
    "        output_text_ids = batch[\"output_text_ids\"]\n",
    "        output_attention_mask = batch[\"output_attention_mask\"]\n",
    "\n",
    "        loss, outs = self(\n",
    "            input_text_ids,\n",
    "            input_attention_mask,\n",
    "            output_attention_mask,\n",
    "            output_text_ids\n",
    "        )\n",
    "\n",
    "        self.log(\"valid_loss\" , loss, prog_bar=True,logger=True)\n",
    "        return loss \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(),lr = 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/depressedcoder/environments/gymenv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:447: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/depressedcoder/environments/gymenv/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1789: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = T5ArithTranslator()\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = \"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k = 1,\n",
    "    verbose = True,\n",
    "    monitor=\"valid_loss\",\n",
    "    mode = \"min\"\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\",name=\"translator\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger = logger,\n",
    "    callbacks =  checkpoint_callback,\n",
    "    max_epochs=N_EPOCHS,\n",
    "    log_every_n_steps=5,\n",
    "    gpus=1,\n",
    "    accelerator='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = T5Dataset(train_df,t5_tokenizer)\n",
    "valid_dataset = T5Dataset(valid_df,t5_tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,32,True)\n",
    "valid_dataloader = DataLoader(valid_dataset,32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/depressedcoder/environments/gymenv/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name     | Type                       | Params\n",
      "--------------------------------------------------------\n",
      "0 | t5_model | T5ForConditionalGeneration | 60.5 M\n",
      "--------------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/depressedcoder/environments/gymenv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/depressedcoder/environments/gymenv/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 32/32 [02:08<00:00,  4.02s/it, loss=1.98, v_num=1, train_loss=0.938, valid_loss=0.483]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 28: 'valid_loss' reached 0.48270 (best 0.48270), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 32/32 [02:09<00:00,  4.03s/it, loss=0.521, v_num=1, train_loss=0.402, valid_loss=0.373]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 56: 'valid_loss' reached 0.37264 (best 0.37264), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 32/32 [02:09<00:00,  4.06s/it, loss=0.371, v_num=1, train_loss=0.322, valid_loss=0.301]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 84: 'valid_loss' reached 0.30099 (best 0.30099), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 32/32 [02:10<00:00,  4.06s/it, loss=0.311, v_num=1, train_loss=0.248, valid_loss=0.245]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 112: 'valid_loss' reached 0.24466 (best 0.24466), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 32/32 [02:10<00:00,  4.08s/it, loss=0.265, v_num=1, train_loss=0.245, valid_loss=0.203]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 140: 'valid_loss' reached 0.20330 (best 0.20330), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 32/32 [02:10<00:00,  4.09s/it, loss=0.213, v_num=1, train_loss=0.186, valid_loss=0.156]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 168: 'valid_loss' reached 0.15589 (best 0.15589), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 32/32 [02:11<00:00,  4.11s/it, loss=0.162, v_num=1, train_loss=0.145, valid_loss=0.102]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 196: 'valid_loss' reached 0.10170 (best 0.10170), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 32/32 [02:11<00:00,  4.10s/it, loss=0.116, v_num=1, train_loss=0.114, valid_loss=0.0677]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 224: 'valid_loss' reached 0.06766 (best 0.06766), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 32/32 [02:12<00:00,  4.14s/it, loss=0.0933, v_num=1, train_loss=0.0722, valid_loss=0.0579]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 252: 'valid_loss' reached 0.05789 (best 0.05789), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 32/32 [02:11<00:00,  4.12s/it, loss=0.0729, v_num=1, train_loss=0.086, valid_loss=0.0509] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 280: 'valid_loss' reached 0.05091 (best 0.05091), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 32/32 [02:11<00:00,  4.10s/it, loss=0.0645, v_num=1, train_loss=0.069, valid_loss=0.0437] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 308: 'valid_loss' reached 0.04367 (best 0.04367), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 32/32 [02:11<00:00,  4.11s/it, loss=0.0575, v_num=1, train_loss=0.0447, valid_loss=0.0369]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 336: 'valid_loss' reached 0.03694 (best 0.03694), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 32/32 [02:11<00:00,  4.10s/it, loss=0.0492, v_num=1, train_loss=0.0479, valid_loss=0.0336]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 364: 'valid_loss' reached 0.03358 (best 0.03358), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 32/32 [02:07<00:00,  3.99s/it, loss=0.0461, v_num=1, train_loss=0.0419, valid_loss=0.0336]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 392: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 32/32 [02:11<00:00,  4.11s/it, loss=0.0423, v_num=1, train_loss=0.0432, valid_loss=0.0331]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 420: 'valid_loss' reached 0.03313 (best 0.03313), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 32/32 [02:12<00:00,  4.13s/it, loss=0.0406, v_num=1, train_loss=0.037, valid_loss=0.0321] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 448: 'valid_loss' reached 0.03214 (best 0.03214), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 32/32 [02:09<00:00,  4.04s/it, loss=0.0409, v_num=1, train_loss=0.0514, valid_loss=0.0311]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 476: 'valid_loss' reached 0.03107 (best 0.03107), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 32/32 [03:02<00:00,  5.72s/it, loss=0.0388, v_num=1, train_loss=0.0369, valid_loss=0.0306]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 504: 'valid_loss' reached 0.03060 (best 0.03060), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 32/32 [02:11<00:00,  4.11s/it, loss=0.0374, v_num=1, train_loss=0.0336, valid_loss=0.030] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 532: 'valid_loss' reached 0.02997 (best 0.02997), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 32/32 [02:10<00:00,  4.07s/it, loss=0.0359, v_num=1, train_loss=0.0397, valid_loss=0.0295]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 560: 'valid_loss' reached 0.02954 (best 0.02954), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 32/32 [02:12<00:00,  4.15s/it, loss=0.0359, v_num=1, train_loss=0.0376, valid_loss=0.0288]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 588: 'valid_loss' reached 0.02883 (best 0.02883), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 32/32 [02:11<00:00,  4.11s/it, loss=0.0365, v_num=1, train_loss=0.0345, valid_loss=0.0287]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 616: 'valid_loss' reached 0.02870 (best 0.02870), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 32/32 [02:10<00:00,  4.06s/it, loss=0.0331, v_num=1, train_loss=0.0397, valid_loss=0.0285]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 644: 'valid_loss' reached 0.02855 (best 0.02855), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 32/32 [02:13<00:00,  4.17s/it, loss=0.0332, v_num=1, train_loss=0.0356, valid_loss=0.0274]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 672: 'valid_loss' reached 0.02744 (best 0.02744), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 32/32 [02:13<00:00,  4.18s/it, loss=0.0316, v_num=1, train_loss=0.0253, valid_loss=0.0272]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 700: 'valid_loss' reached 0.02719 (best 0.02719), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 32/32 [02:13<00:00,  4.17s/it, loss=0.0324, v_num=1, train_loss=0.029, valid_loss=0.0272] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 728: 'valid_loss' reached 0.02718 (best 0.02718), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 32/32 [02:10<00:00,  4.09s/it, loss=0.0326, v_num=1, train_loss=0.0284, valid_loss=0.0269]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 756: 'valid_loss' reached 0.02689 (best 0.02689), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 32/32 [02:11<00:00,  4.11s/it, loss=0.032, v_num=1, train_loss=0.0295, valid_loss=0.0264] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 784: 'valid_loss' reached 0.02643 (best 0.02643), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 32/32 [02:15<00:00,  4.23s/it, loss=0.0314, v_num=1, train_loss=0.0349, valid_loss=0.0261]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 812: 'valid_loss' reached 0.02613 (best 0.02613), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 32/32 [02:11<00:00,  4.10s/it, loss=0.0317, v_num=1, train_loss=0.0335, valid_loss=0.0258]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 840: 'valid_loss' reached 0.02581 (best 0.02581), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 32/32 [02:37<00:00,  4.92s/it, loss=0.0296, v_num=1, train_loss=0.0315, valid_loss=0.0252]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30, global step 868: 'valid_loss' reached 0.02522 (best 0.02522), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 32/32 [02:12<00:00,  4.15s/it, loss=0.0296, v_num=1, train_loss=0.0382, valid_loss=0.0242]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31, global step 896: 'valid_loss' reached 0.02421 (best 0.02421), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 32/32 [02:17<00:00,  4.30s/it, loss=0.0293, v_num=1, train_loss=0.0289, valid_loss=0.0239]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32, global step 924: 'valid_loss' reached 0.02388 (best 0.02388), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 32/32 [02:10<00:00,  4.09s/it, loss=0.0306, v_num=1, train_loss=0.026, valid_loss=0.0237] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33, global step 952: 'valid_loss' reached 0.02372 (best 0.02372), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 32/32 [02:11<00:00,  4.10s/it, loss=0.0268, v_num=1, train_loss=0.0264, valid_loss=0.0233]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34, global step 980: 'valid_loss' reached 0.02331 (best 0.02331), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 32/32 [02:11<00:00,  4.10s/it, loss=0.0287, v_num=1, train_loss=0.0231, valid_loss=0.0228]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35, global step 1008: 'valid_loss' reached 0.02280 (best 0.02280), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 32/32 [02:14<00:00,  4.20s/it, loss=0.0267, v_num=1, train_loss=0.0239, valid_loss=0.0219]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36, global step 1036: 'valid_loss' reached 0.02188 (best 0.02188), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 32/32 [02:04<00:00,  3.88s/it, loss=0.0263, v_num=1, train_loss=0.0224, valid_loss=0.0219]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37, global step 1064: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 32/32 [02:16<00:00,  4.26s/it, loss=0.0259, v_num=1, train_loss=0.0335, valid_loss=0.0216]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38, global step 1092: 'valid_loss' reached 0.02156 (best 0.02156), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 32/32 [02:14<00:00,  4.21s/it, loss=0.0256, v_num=1, train_loss=0.0168, valid_loss=0.021] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39, global step 1120: 'valid_loss' reached 0.02095 (best 0.02095), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 32/32 [02:06<00:00,  3.96s/it, loss=0.0255, v_num=1, train_loss=0.0315, valid_loss=0.0207]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40, global step 1148: 'valid_loss' reached 0.02072 (best 0.02072), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 32/32 [02:10<00:00,  4.09s/it, loss=0.0248, v_num=1, train_loss=0.0281, valid_loss=0.0203]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41, global step 1176: 'valid_loss' reached 0.02029 (best 0.02029), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 32/32 [02:07<00:00,  4.00s/it, loss=0.0252, v_num=1, train_loss=0.0272, valid_loss=0.0204]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42, global step 1204: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 32/32 [02:11<00:00,  4.12s/it, loss=0.0236, v_num=1, train_loss=0.0217, valid_loss=0.0199]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43, global step 1232: 'valid_loss' reached 0.01995 (best 0.01995), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 32/32 [02:03<00:00,  3.86s/it, loss=0.0241, v_num=1, train_loss=0.0213, valid_loss=0.0195]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44, global step 1260: 'valid_loss' reached 0.01953 (best 0.01953), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 32/32 [02:12<00:00,  4.14s/it, loss=0.0233, v_num=1, train_loss=0.0211, valid_loss=0.020] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45, global step 1288: 'valid_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 32/32 [02:12<00:00,  4.15s/it, loss=0.0229, v_num=1, train_loss=0.0201, valid_loss=0.0195]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46, global step 1316: 'valid_loss' reached 0.01952 (best 0.01952), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|██████████| 32/32 [02:12<00:00,  4.15s/it, loss=0.023, v_num=1, train_loss=0.0284, valid_loss=0.0192] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47, global step 1344: 'valid_loss' reached 0.01919 (best 0.01919), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 32/32 [02:14<00:00,  4.19s/it, loss=0.0227, v_num=1, train_loss=0.0216, valid_loss=0.0191]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48, global step 1372: 'valid_loss' reached 0.01909 (best 0.01909), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 32/32 [02:12<00:00,  4.13s/it, loss=0.0214, v_num=1, train_loss=0.0213, valid_loss=0.019] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49, global step 1400: 'valid_loss' reached 0.01902 (best 0.01902), saving model to '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 32/32 [02:12<00:00,  4.15s/it, loss=0.0214, v_num=1, train_loss=0.0213, valid_loss=0.019]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,train_dataloader,valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = T5ArithTranslator.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path\n",
    ")\n",
    "test_model.freeze()\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "special_tokens_dict = {'additional_special_tokens' : ['[SEP]']}\n",
    "num_added_tokens = t5_tokenizer.add_special_tokens(special_tokens_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = t5_tokenizer(\"last stop in their field trip was the aquarium . penny identified number0 species of sharks number1 species of eels and number2 different species of whales . [SEP] how many species was penny able to identify ?\",return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ + number0 number1 number2\n"
     ]
    }
   ],
   "source": [
    "outputs = test_model.t5_model.generate(test_input_ids)\n",
    "print(t5_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gymenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b0110cf1cb03549be737c5657a86ea4daeeb81469a7991ed915d907f3e629c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
