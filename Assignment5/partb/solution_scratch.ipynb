{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset_name = 'ArithOpsTrain.xlsx'\n",
    "df = pd.read_excel(train_dataset_name)\n",
    "df = df.drop('Table 1',axis=1)\n",
    "df = df.rename(columns=df.iloc[0]).loc[1:]\n",
    "\n",
    "train_df , valid_df = train_test_split(df,test_size=0.1,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Question</th>\n",
       "      <th>Equation</th>\n",
       "      <th>Input Numbers</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>mrs. hilt is baking bread . she needs number0 ...</td>\n",
       "      <td>how much flour will she need to make number2 l...</td>\n",
       "      <td>/ number0 number1</td>\n",
       "      <td>5 2 1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>robin had number0 songs on her number1 player ...</td>\n",
       "      <td>how many songs does she have on her number4 pl...</td>\n",
       "      <td>+ - number0 number2 number3</td>\n",
       "      <td>30 3 8 10 3</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>there are number0 more sections that are undev...</td>\n",
       "      <td>what is the total area of the undeveloped land ?</td>\n",
       "      <td>* number0 number1</td>\n",
       "      <td>3 2435</td>\n",
       "      <td>7305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>mom made number0 chocolate chip cookies . it t...</td>\n",
       "      <td>how many cookies were left ?</td>\n",
       "      <td>- number0 number3</td>\n",
       "      <td>32 24 16 9</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>number0 children are taking a bus to the zoo ....</td>\n",
       "      <td>how many seats will the children need in all ?</td>\n",
       "      <td>/ number0 number1</td>\n",
       "      <td>58 2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Description  \\\n",
       "373  mrs. hilt is baking bread . she needs number0 ...   \n",
       "901  robin had number0 songs on her number1 player ...   \n",
       "254  there are number0 more sections that are undev...   \n",
       "468  mom made number0 chocolate chip cookies . it t...   \n",
       "333  number0 children are taking a bus to the zoo ....   \n",
       "\n",
       "                                              Question  \\\n",
       "373  how much flour will she need to make number2 l...   \n",
       "901  how many songs does she have on her number4 pl...   \n",
       "254   what is the total area of the undeveloped land ?   \n",
       "468                       how many cookies were left ?   \n",
       "333     how many seats will the children need in all ?   \n",
       "\n",
       "                        Equation Input Numbers Output  \n",
       "373            / number0 number1         5 2 1    2.5  \n",
       "901  + - number0 number2 number3   30 3 8 10 3     32  \n",
       "254            * number0 number1        3 2435   7305  \n",
       "468            - number0 number3    32 24 16 9     23  \n",
       "333            / number0 number1          58 2     29  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data  : pd.DataFrame,\n",
    "        tokenizer : T5Tokenizer,\n",
    "        text_max_token_length = 512,\n",
    "        output_max_token_length = 128\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data \n",
    "        self.text_max_token_length = text_max_token_length,\n",
    "        self.output_max_token_length = output_max_token_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        input_text = data_row[\"Description\"]\n",
    "        input_question = data_row[\"Question\"]\n",
    "\n",
    "        in_text = input_text + \" [SEP] \" + input_question\n",
    "        \n",
    "        input_text_encoding = self.tokenizer(\n",
    "            in_text,\n",
    "            max_length=self.text_max_token_length,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        output_text = data_row[\"Equation\"]        \n",
    "        output_text_encoding = self.tokenizer(\n",
    "            output_text,\n",
    "            max_length=self.output_max_token_length,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return dict(\n",
    "            input_text = input_text,\n",
    "            output_text = output_text,\n",
    "            input_text_ids = input_text_encoding['input_ids'].flatten(),\n",
    "            input_attention_mask = input_text_encoding['attention_mask'].flatten(),\n",
    "            output_text_ids = output_text_encoding['input_ids'].flatten(),\n",
    "            output_attention_mask = output_text_encoding['attention_mask'].flatten()\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "special_tokens_dict = {'additional_special_tokens' : ['[SEP]']}\n",
    "num_added_tokens = t5_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "t5_model =T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "dataset = T5Dataset(train_df,t5_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = T5Dataset(train_df,t5_tokenizer)\n",
    "valid_dataset = T5Dataset(valid_df,t5_tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,32,True)\n",
    "valid_dataloader = DataLoader(valid_dataset,32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class T5DataSetModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df : pd.DataFrame,\n",
    "        valid_df : pd.DataFrame,\n",
    "        tokenizer : T5Tokenizer,\n",
    "        batch_size = 32,\n",
    "        text_max_token_length = 512,\n",
    "        output_max_token_length = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.tokenizer = tokenizer,\n",
    "        self.batch_size = batch_size,\n",
    "        self.text_max_token_length = text_max_token_length\n",
    "        self.output_max_token_length = output_max_token_length\n",
    "\n",
    "    \n",
    "    def setup(self, stage = None):\n",
    "        \n",
    "        self.train_dataset = T5Dataset(self.train_df,self.tokenizer,self.text_max_token_length,self.output_max_token_length)\n",
    "        self.valid_dataset = T5Dataset(self.valid_df,self.tokenizer,self.text_max_token_length,self.output_max_token_length)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "#data_module = T5DataSetModule(train_df,valid_df,t5_tokenizer,BATCH_SIZE)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postfix_evaluation(batch_data,input_values):\n",
    "\n",
    "    arith_symbols = set(['+','-','*','/','%'])\n",
    "    output_values = []\n",
    "    \n",
    "    for i in range(len(batch_data)):\n",
    "        flag = True\n",
    "        current_input = batch_data[i].split(' ')\n",
    "        current_input.reverse()\n",
    "        input_value = input_values[i]\n",
    "\n",
    "        stack = []\n",
    "        for symbol in current_input:\n",
    "            if symbol in arith_symbols:\n",
    "                if len(stack)<2:\n",
    "                    flag = False\n",
    "                    break\n",
    "                in1 = stack.pop(-1)\n",
    "                in2 = stack.pop(-1)\n",
    "\n",
    "                res = 0\n",
    "                if symbol=='+':\n",
    "                    res = in1+in2\n",
    "                elif symbol=='-':\n",
    "                    res = in1 - in2 \n",
    "                elif symbol == '*':\n",
    "                    res = in1 * in2\n",
    "                elif symbol=='/':\n",
    "                    res = in1/in2\n",
    "                else:\n",
    "                    res = in1 % in2\n",
    "                stack.append(res)\n",
    "\n",
    "\n",
    "            else:\n",
    "                if \"number\" in symbol:\n",
    "                    index = int(symbol[6])\n",
    "                    stack.append(input_value[index])\n",
    "\n",
    "        if flag==False or len(stack)!=1:\n",
    "            output_values.append(0)\n",
    "        else:\n",
    "            output_values.append(stack.pop(-1))\n",
    "\n",
    "    ans = torch.tensor(output_values)\n",
    "    return ans\n",
    "\n",
    "ans = postfix_evaluation([\"+ - number0 number1 number2\",\"+ / - number0 number2 number1 number3\"],[[1,4,6],[5,6,7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self,dim_model,dropout_p,max_len) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout =  nn.Dropout(dropout_p)\n",
    "\n",
    "        pos_encoding = torch.zeros(max_len,dim_model)\n",
    "        \n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) \n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) \n",
    "        \n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens_input,\n",
    "        num_tokens_output,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model,\n",
    "            dropout_p= dropout_p,\n",
    "            max_len=5000\n",
    "        )\n",
    "\n",
    "        self.src_embedding = nn.Embedding(num_tokens_input,dim_model)\n",
    "        self.trg_embedding = nn.Embedding(num_tokens_output,dim_model)\n",
    "\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout= dropout_p,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(self.dim_model,num_tokens_output)\n",
    "\n",
    "    \n",
    "    def forward(self, src, trg, src_padding_mask=None,target_mask=None, target_padding_mask=None):\n",
    "\n",
    "        src = self.src_embedding(src) * math.sqrt(self.dim_model)\n",
    "        target = self.trg_embedding(trg) * math.sqrt(self.dim_model)\n",
    "\n",
    "        src = self.positional_encoder(src)\n",
    "        trg = self.positional_encoder(trg)\n",
    "        \n",
    "\n",
    "        transformer_out = self.transformer(\n",
    "            src,trg,tgt_mask=target_mask,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            trg_key_padding_mask=target_padding_mask\n",
    "        )\n",
    "        out = self.out(transformer_out)\n",
    "        return out\n",
    "    \n",
    "        \n",
    "    def get_tgt_mask(self,size):\n",
    "        \n",
    "        mask = torch.tril(torch.ones(size,size) == 1)\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask==0,float('-inf'))\n",
    "        mask = mask.masked_fill(mask==1,float(0.0))\n",
    "\n",
    "    def get_padding_mask(self,matrix,pad_token):\n",
    "        return (matrix==pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "class TranformerTranslator(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens_input,\n",
    "        num_tokens_output,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.transformer = TransformerModel(\n",
    "                num_tokens_input=num_tokens_input,\n",
    "                num_tokens_output=num_tokens_output,\n",
    "                dim_model=dim_model,\n",
    "                num_heads=num_heads,\n",
    "                num_encoder_layers=num_encoder_layers,\n",
    "                num_decoder_layers=num_decoder_layers,\n",
    "                dim_feedforward= dim_feedforward,\n",
    "                dropout_p=dropout_p\n",
    "            )\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, src, trg, src_padding_mask=None,target_mask=None, target_padding_mask=None):\n",
    "\n",
    "        return self.transformer(src,trg,src_padding_mask,target_mask,target_padding_mask)\n",
    "        \n",
    "\n",
    "    def training_step(self, batch_data):\n",
    "\n",
    "        input_text_ids = batch_data['input_text_ids']\n",
    "        input_attention_mask = batch_data['input_attention_mask']\n",
    "        output_text_ids = batch_data['output_text_ids']\n",
    "        output_attention_mask = batch_data['output_attention_mask']\n",
    "\n",
    "        \n",
    "        output_in = output_text_ids[:,:-1]\n",
    "        output_expected = output_text_ids[:,1:]\n",
    "        \n",
    "        target_mask = self.transformer.get_tgt_mask(output_expected.shape(1))\n",
    "\n",
    "        predictions = self(input_text_ids,output_in,target_mask=target_mask)\n",
    "\n",
    "        train_loss = self.loss_fn(predictions,output_expected)\n",
    "        \n",
    "        self.log(\"train_loss\" , train_loss, prog_bar=True,logger=True)\n",
    "\n",
    "        return train_loss\n",
    "        \n",
    "    def validation_step(self,batch_data):\n",
    "        \n",
    "        input_text_ids = batch_data['input_text_ids']\n",
    "        input_attention_mask = batch_data['input_attention_mask']\n",
    "        output_text_ids = batch_data['output_text_ids']\n",
    "        output_attention_mask = batch_data['output_attention_mask']\n",
    "\n",
    "        \n",
    "        output_in = output_text_ids[:,:-1]\n",
    "        output_expected = output_text_ids[:,1:]\n",
    "        \n",
    "        target_mask = self.transformer.get_tgt_mask(output_expected.shape(1))\n",
    "\n",
    "        predictions = self(input_text_ids,output_in,target_mask=target_mask)\n",
    "\n",
    "        valid_loss = self.loss_fn(predictions,output_expected)\n",
    "        \n",
    "        self.log(\"valid_loss\" , valid_loss, prog_bar=True,logger=True)\n",
    "\n",
    "        return valid_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(),lr = 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5ArithTranslator(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, input_attention_mask, decoder_attention_mask, labels):\n",
    "\n",
    "        outs = self.t5_model(input_ids=input_ids,attention_mask = input_attention_mask,labels = labels)        \n",
    "        return outs.loss ,  outs.logits\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx) :\n",
    "        \n",
    "        input_text_ids = batch[\"input_text_ids\"]\n",
    "        input_attention_mask = batch[\"input_attention_mask\"]\n",
    "        output_text_ids = batch[\"output_text_ids\"]\n",
    "        output_attention_mask = batch[\"output_attention_mask\"]\n",
    "\n",
    "        loss, outs = self(\n",
    "            input_text_ids,\n",
    "            input_attention_mask,\n",
    "            output_attention_mask,\n",
    "            output_text_ids\n",
    "        )\n",
    "\n",
    "        self.log(\"train_loss\" , loss, prog_bar=True,logger=True)\n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        input_text_ids = batch[\"input_text_ids\"]\n",
    "        input_attention_mask = batch[\"input_attention_mask\"]\n",
    "        output_text_ids = batch[\"output_text_ids\"]\n",
    "        output_attention_mask = batch[\"output_attention_mask\"]\n",
    "\n",
    "        loss, outs = self(\n",
    "            input_text_ids,\n",
    "            input_attention_mask,\n",
    "            output_attention_mask,\n",
    "            output_text_ids\n",
    "        )\n",
    "\n",
    "        self.log(\"valid_loss\" , loss, prog_bar=True,logger=True)\n",
    "        return loss \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(),lr = 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = T5ArithTranslator()\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = \"checkpoints\",\n",
    "    filename=\"transformer-scratch-best-checkpoint\",\n",
    "    save_top_k = 1,\n",
    "    verbose = True,\n",
    "    monitor=\"valid_loss\",\n",
    "    mode = \"min\"\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\",name=\"translator\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger = logger,\n",
    "    callbacks =  checkpoint_callback,\n",
    "    max_epochs=N_EPOCHS,\n",
    "    log_every_n_steps=5,\n",
    "    gpus=1,\n",
    "    accelerator='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model,train_dataloader,valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = T5ArithTranslator.load_from_checkpoint(\n",
    "    '/Users/depressedcoder/DLNLP/Assignment5/partb/checkpoints/best-checkpoint-v1.ckpt'\n",
    ")\n",
    "test_model.freeze()\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "special_tokens_dict = {'additional_special_tokens' : ['[SEP]']}\n",
    "num_added_tokens = t5_tokenizer.add_special_tokens(special_tokens_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = t5_tokenizer(\"last stop in their field trip was the aquarium . penny identified number0 species of sharks number1 species of eels and number2 different species of whales . [SEP] how many species was penny able to identify ?\",return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = test_model.t5_model.generate(test_input_ids)\n",
    "text = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gymenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b0110cf1cb03549be737c5657a86ea4daeeb81469a7991ed915d907f3e629c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
