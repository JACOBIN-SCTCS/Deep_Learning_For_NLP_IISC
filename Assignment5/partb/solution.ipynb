{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/depressedcoder/environments/gymenv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "train_dataset_name = 'ArithOpsTrain.xlsx'\n",
    "df = pd.read_excel(train_dataset_name)\n",
    "df = df.drop('Table 1',axis=1)\n",
    "df = df.rename(columns=df.iloc[0]).loc[1:]\n",
    "\n",
    "train_df , valid_df = train_test_split(df,test_size=0.1,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Question</th>\n",
       "      <th>Equation</th>\n",
       "      <th>Input Numbers</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>mrs. hilt is baking bread . she needs number0 ...</td>\n",
       "      <td>how much flour will she need to make number2 l...</td>\n",
       "      <td>/ number0 number1</td>\n",
       "      <td>5 2 1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>robin had number0 songs on her number1 player ...</td>\n",
       "      <td>how many songs does she have on her number4 pl...</td>\n",
       "      <td>+ - number0 number2 number3</td>\n",
       "      <td>30 3 8 10 3</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>there are number0 more sections that are undev...</td>\n",
       "      <td>what is the total area of the undeveloped land ?</td>\n",
       "      <td>* number0 number1</td>\n",
       "      <td>3 2435</td>\n",
       "      <td>7305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>mom made number0 chocolate chip cookies . it t...</td>\n",
       "      <td>how many cookies were left ?</td>\n",
       "      <td>- number0 number3</td>\n",
       "      <td>32 24 16 9</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>number0 children are taking a bus to the zoo ....</td>\n",
       "      <td>how many seats will the children need in all ?</td>\n",
       "      <td>/ number0 number1</td>\n",
       "      <td>58 2</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Description  \\\n",
       "373  mrs. hilt is baking bread . she needs number0 ...   \n",
       "901  robin had number0 songs on her number1 player ...   \n",
       "254  there are number0 more sections that are undev...   \n",
       "468  mom made number0 chocolate chip cookies . it t...   \n",
       "333  number0 children are taking a bus to the zoo ....   \n",
       "\n",
       "                                              Question  \\\n",
       "373  how much flour will she need to make number2 l...   \n",
       "901  how many songs does she have on her number4 pl...   \n",
       "254   what is the total area of the undeveloped land ?   \n",
       "468                       how many cookies were left ?   \n",
       "333     how many seats will the children need in all ?   \n",
       "\n",
       "                        Equation Input Numbers Output  \n",
       "373            / number0 number1         5 2 1    2.5  \n",
       "901  + - number0 number2 number3   30 3 8 10 3     32  \n",
       "254            * number0 number1        3 2435   7305  \n",
       "468            - number0 number3    32 24 16 9     23  \n",
       "333            / number0 number1          58 2     29  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data  : pd.DataFrame,\n",
    "        tokenizer : T5Tokenizer,\n",
    "        text_max_token_length = 512 ,\n",
    "        output_max_token_length = 128\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data \n",
    "        self.text_max_token_length = text_max_token_length\n",
    "        self.output_max_token_length = output_max_token_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data_row = self.data.iloc[index]\n",
    "\n",
    "        input_text = data_row[\"Description\"]\n",
    "        input_question = data_row[\"Question\"]\n",
    "\n",
    "        in_text = input_text + \" [SEP] \" + input_question\n",
    "        \n",
    "        input_text_encoding = self.tokenizer(\n",
    "            in_text,\n",
    "            max_length=self.text_max_token_length,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "        output_text = data_row[\"Equation\"]        \n",
    "        output_text_encoding = self.tokenizer(\n",
    "            output_text,\n",
    "            max_length=self.output_max_token_length,\n",
    "            padding = \"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_numbers_text = data_row[\"Input Numbers\"]\n",
    "        in_numbers = [float(x) for x in input_numbers_text.split(' ')]\n",
    "\n",
    "\n",
    "        final_result  = float(data_row[\"Output\"])\n",
    "\n",
    "        return dict(\n",
    "            input_text = input_text,\n",
    "            output_text = output_text,\n",
    "            input_text_ids = input_text_encoding['input_ids'].flatten(),\n",
    "            input_attention_mask = input_text_encoding['attention_mask'].flatten(),\n",
    "            output_text_ids = output_text_encoding['input_ids'].flatten(),\n",
    "            output_attention_mask = output_text_encoding['attention_mask'].flatten(),\n",
    "                \n",
    "            input_numbers  = in_numbers,\n",
    "            final_result = final_result\n",
    "\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/depressedcoder/environments/gymenv/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "special_tokens_dict = {'additional_special_tokens' : ['[SEP]']}\n",
    "num_added_tokens = t5_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "dataset = T5Dataset(train_df,t5_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class T5DataSetModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df : pd.DataFrame,\n",
    "        valid_df : pd.DataFrame,\n",
    "        tokenizer : T5Tokenizer,\n",
    "        batch_size = 32,\n",
    "        text_max_token_length = 512,\n",
    "        output_max_token_length = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.tokenizer = tokenizer,\n",
    "        self.batch_size = batch_size,\n",
    "        self.text_max_token_length = text_max_token_length\n",
    "        self.output_max_token_length = output_max_token_length\n",
    "\n",
    "    \n",
    "    def setup(self, stage = None):\n",
    "        \n",
    "        self.train_dataset = T5Dataset(self.train_df,self.tokenizer,self.text_max_token_length,self.output_max_token_length)\n",
    "        self.valid_dataset = T5Dataset(self.valid_df,self.tokenizer,self.text_max_token_length,self.output_max_token_length)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_dataset,\n",
    "            self.batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "#data_module = T5DataSetModule(train_df,valid_df,t5_tokenizer,BATCH_SIZE)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postfix_evaluation(batch_data,input_values):\n",
    "\n",
    "    arith_symbols = set(['+','-','*','/','%'])\n",
    "    output_values = []\n",
    "    \n",
    "    for i in range(len(batch_data)):\n",
    "        flag = True\n",
    "        current_input = batch_data[i].split(' ')\n",
    "        current_input.reverse()\n",
    "        input_value = input_values[i]\n",
    "\n",
    "        stack = []\n",
    "        for symbol in current_input:\n",
    "            if symbol in arith_symbols:\n",
    "                if len(stack)<2:\n",
    "                    flag = False\n",
    "                    break\n",
    "                in1 = stack.pop(-1)\n",
    "                in2 = stack.pop(-1)\n",
    "\n",
    "                res = 0\n",
    "                if symbol=='+':\n",
    "                    res = in1+in2\n",
    "                elif symbol=='-':\n",
    "                    res = in1 - in2 \n",
    "                elif symbol == '*':\n",
    "                    res = in1 * in2\n",
    "                elif symbol=='/':\n",
    "                    res = in1/in2\n",
    "                else:\n",
    "                    res = in1 % in2\n",
    "                stack.append(res)\n",
    "\n",
    "\n",
    "            else:\n",
    "                if \"number\" in symbol:\n",
    "                    index = int(symbol[6])\n",
    "                    stack.append(input_value[index])\n",
    "\n",
    "        if flag==False or len(stack)!=1:\n",
    "            output_values.append(0)\n",
    "        else:\n",
    "            output_values.append(stack.pop(-1))\n",
    "    \n",
    "\n",
    "    return torch.tensor(output_values)\n",
    "\n",
    "ans = postfix_evaluation([\"+ - number0 number1 number2\",\"+ / - number0 number2 number1 number3\"],[[1.0,4.0,6.0],[5.0,6.0,7.0,8.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0000, 7.6667])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "class T5ArithTranslator(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,t5_tokenizer,weighing_factor=0.001):\n",
    "        super().__init__()\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "        self.weighing_factor = weighing_factor\n",
    "        self.tokenizer = t5_tokenizer\n",
    "\n",
    "    def forward(self, input_ids, input_attention_mask, decoder_attention_mask, labels):\n",
    "\n",
    "        outs = self.t5_model(input_ids=input_ids,attention_mask = input_attention_mask,labels = labels)        \n",
    "        return outs.loss ,  outs.logits\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx) :\n",
    "        \n",
    "        input_text_ids = batch[\"input_text_ids\"]\n",
    "        input_attention_mask = batch[\"input_attention_mask\"]\n",
    "        output_text_ids = batch[\"output_text_ids\"]\n",
    "        output_attention_mask = batch[\"output_attention_mask\"]\n",
    "\n",
    "        loss, outs = self(\n",
    "            input_text_ids,\n",
    "            input_attention_mask,\n",
    "            output_attention_mask,\n",
    "            output_text_ids\n",
    "        )\n",
    "\n",
    "        preds = F.softmax(outs, dim=-1).argmax(dim=-1)\n",
    "        y = self.tokenizer.batch_decode(sequences=preds, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "        print(len(y))\n",
    "        self.log(\"train_loss\" , loss, prog_bar=True,logger=True)\n",
    "        return loss \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        input_text_ids = batch[\"input_text_ids\"]\n",
    "        input_attention_mask = batch[\"input_attention_mask\"]\n",
    "        output_text_ids = batch[\"output_text_ids\"]\n",
    "        output_attention_mask = batch[\"output_attention_mask\"]\n",
    "\n",
    "        loss, outs = self(\n",
    "            input_text_ids,\n",
    "            input_attention_mask,\n",
    "            output_attention_mask,\n",
    "            output_text_ids\n",
    "        )\n",
    "\n",
    "        self.log(\"valid_loss\" , loss, prog_bar=True,logger=True)\n",
    "        return loss \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(),lr = 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = T5ArithTranslator(t5_tokenizer)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath = \"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k = 1,\n",
    "    verbose = True,\n",
    "    monitor=\"valid_loss\",\n",
    "    mode = \"min\"\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\",name=\"translator\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger = logger,\n",
    "    callbacks =  checkpoint_callback,\n",
    "    max_epochs=N_EPOCHS,\n",
    "    log_every_n_steps=5,\n",
    "    gpus=1,\n",
    "    accelerator='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_function(batch_data):\n",
    "\n",
    "    input_text_ids_list = [  batch[\"input_text_ids\"] for batch in batch_data ] \n",
    "    input_attention_mask_list = [ batch[\"input_attention_mask\"] for batch in batch_data ] \n",
    "    output_text_ids_list = [ batch[\"output_text_ids\"] for batch in batch_data ] \n",
    "    output_attention_mask_list = [batch[\"output_attention_mask\"] for batch in batch_data]\n",
    "\n",
    "    input_numbers = [ batch[\"input_numbers\"] for batch in batch_data ]\n",
    "    result_list = [batch[\"final_result\"] for batch in batch_data]\n",
    "\n",
    "    \n",
    "    input_text_ids = torch.stack(input_text_ids_list)\n",
    "    input_attention_mask = torch.stack(input_attention_mask_list)\n",
    "    output_text_ids = torch.stack(output_text_ids_list)\n",
    "    output_attention_mask = torch.stack(output_attention_mask_list)\n",
    "\n",
    "    return {\n",
    "        \"input_text_ids\" : input_text_ids,\n",
    "        \"input_attention_mask\" : input_attention_mask,\n",
    "        \"output_text_ids\" : output_text_ids,\n",
    "        \"output_attention_mask\" : output_attention_mask,\n",
    "        \"input_numbers\" : input_numbers,\n",
    "        \"final_result\" : result_list\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "train_dataset = T5Dataset(train_df,t5_tokenizer)\n",
    "valid_dataset = T5Dataset(valid_df,t5_tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,32,True,collate_fn= collate_function)\n",
    "valid_dataloader = DataLoader(valid_dataset,32,shuffle=False,collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model,train_dataloader,valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = T5ArithTranslator.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path\n",
    ")\n",
    "test_model.freeze()\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "special_tokens_dict = {'additional_special_tokens' : ['[SEP]']}\n",
    "num_added_tokens = t5_tokenizer.add_special_tokens(special_tokens_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = t5_tokenizer(\"last stop in their field trip was the aquarium . penny identified number0 species of sharks number1 species of eels and number2 different species of whales . [SEP] how many species was penny able to identify ?\",return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = test_model.t5_model.generate(test_input_ids)\n",
    "print(t5_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gymenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1b0110cf1cb03549be737c5657a86ea4daeeb81469a7991ed915d907f3e629c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
